{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get CUDA device information\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "device = cuda.Device(0)\n",
    "print(\"Device Name:\", device.name())\n",
    "print(\"Total Memory:\", device.total_memory() / (1024 ** 2), \"MB\")\n",
    "print(\"Compute Capability:\", device.compute_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics \n",
    "\n",
    "from package.processtransformer import constants\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "if not os.path.exists(data_dir): \n",
    "  os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"bpi_2012\"\n",
    "data_processor = LogsDataProcessor(name=dataset_name, filepath=\"BPI_Challenge_2012.csv\",  \n",
    "                                    columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "                                    dir_path='datasets', datetime_format=\"ISO8601\", pool = 4)\n",
    "data_processor.process_logs(task=constants.Task.NEXT_ACTIVITY, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_ACTIVITY)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "    x_word_dict, y_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 1\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_activity_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "            x_word_dict, y_word_dict, max_case_length)   \n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\")\n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "k.append(i + 1)\n",
    "accuracies.append(np.mean(accuracy))\n",
    "fscores.append(np.mean(fscores))\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "\n",
    "accuracy = np.mean(accuracies)\n",
    "print('Average accuracy across all prefixes:', accuracy)\n",
    "print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "print('Average precision across all prefixes:', np.mean(precisions))\n",
    "print('Average recall across all prefixes:', np.mean(recalls))\n",
    "\n",
    "with open(f\"{dataset_name}_unweighted.txt\", 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, accuracies, fscores, precisions, recalls = [], [], [], [], []\n",
    "weighted_accuracies, weighted_fscores, weighted_precisions, weighted_recalls = [], [], [], []\n",
    "\n",
    "total_instances = 0\n",
    "\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_instances = len(test_data_subset)\n",
    "    \n",
    "    if num_instances > 0:\n",
    "        total_instances += num_instances\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(\n",
    "            test_data_subset, x_word_dict, y_word_dict, max_case_length\n",
    "        )\n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\"\n",
    "        )\n",
    "        \n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        weighted_accuracies.append(accuracy * num_instances)\n",
    "        weighted_fscores.append(fscore * num_instances)\n",
    "        weighted_precisions.append(precision * num_instances)\n",
    "        weighted_recalls.append(recall * num_instances)\n",
    "\n",
    "# Compute weighted averages\n",
    "average_accuracy = sum(weighted_accuracies) / total_instances\n",
    "average_fscore = sum(weighted_fscores) / total_instances\n",
    "average_precision = sum(weighted_precisions) / total_instances\n",
    "average_recall = sum(weighted_recalls) / total_instances\n",
    "\n",
    "print('Weighted average accuracy across all prefixes:', average_accuracy)\n",
    "print('Weighted average f-score across all prefixes:', average_fscore)\n",
    "print('Weighted average precision across all prefixes:', average_precision)\n",
    "print('Weighted average recall across all prefixes:', average_recall)\n",
    "\n",
    "with open(f\"{dataset_name}_weighted.txt\", 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(str(average_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"bpi_2012\"\n",
    "data_processor = LogsDataProcessor(name=dataset_name, filepath=\"BPI_Challenge_2012.csv\",  \n",
    "                                    columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "                                    dir_path='datasets', datetime_format=\"ISO8601\", pool = 4)\n",
    "data_processor.process_logs(task=constants.Task.NEXT_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_next_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# check the k-values #########################################\n",
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, maes, mses, rmses = [],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(np.mean(maes))\n",
    "mses.append(np.mean(mses))\n",
    "rmses.append(np.mean(rmses))  \n",
    "print('Average MAE across all prefixes:', np.mean(maes))\n",
    "print('Average MSE across all prefixes:', np.mean(mses))\n",
    "print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "#     \"mean_squared_error\":mses, \n",
    "#     \"root_mean_squared_error\":rmses})\n",
    "# results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, maes, mses, rmses = [],[],[],[]\n",
    "weights = []\n",
    "\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        num_samples = len(test_data_subset)\n",
    "        weights.append(num_samples)\n",
    "\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# Calculate weighted averages\n",
    "total_weight = np.sum(weights)\n",
    "weighted_mae = np.average(maes, weights=weights)\n",
    "weighted_mse = np.average(mses, weights=weights)\n",
    "weighted_rmse = np.average(rmses, weights=weights)\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(weighted_mae)\n",
    "mses.append(weighted_mse)\n",
    "rmses.append(weighted_rmse)\n",
    "\n",
    "print('Weighted MAE across all prefixes:', weighted_mae)\n",
    "print('Weighted MSE across all prefixes:', weighted_mse)\n",
    "print('Weighted RMSE across all prefixes:', weighted_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"bpi_2012\"\n",
    "data_processor = LogsDataProcessor(name=dataset_name, filepath=\"BPI_Challenge_2012.csv\",  \n",
    "                                    columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "                                    dir_path='datasets', datetime_format=\"ISO8601\", pool = 4)\n",
    "data_processor.process_logs(task=constants.Task.REMAINING_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.REMAINING_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_remaining_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_remaining_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# check the k-values #########################################\n",
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, maes, mses, rmses = [],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(np.mean(maes))\n",
    "mses.append(np.mean(mses))\n",
    "rmses.append(np.mean(rmses))  \n",
    "print('Average MAE across all prefixes:', np.mean(maes))\n",
    "print('Average MSE across all prefixes:', np.mean(mses))\n",
    "print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "#     \"mean_squared_error\":mses, \n",
    "#     \"root_mean_squared_error\":rmses})\n",
    "# results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, maes, mses, rmses = [],[],[],[]\n",
    "weights = []\n",
    "\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        num_samples = len(test_data_subset)\n",
    "        weights.append(num_samples)\n",
    "\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# Calculate weighted averages\n",
    "total_weight = np.sum(weights)\n",
    "weighted_mae = np.average(maes, weights=weights)\n",
    "weighted_mse = np.average(mses, weights=weights)\n",
    "weighted_rmse = np.average(rmses, weights=weights)\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(weighted_mae)\n",
    "mses.append(weighted_mse)\n",
    "rmses.append(weighted_rmse)\n",
    "\n",
    "print('Weighted MAE across all prefixes:', weighted_mae)\n",
    "print('Weighted MSE across all prefixes:', weighted_mse)\n",
    "print('Weighted RMSE across all prefixes:', weighted_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
