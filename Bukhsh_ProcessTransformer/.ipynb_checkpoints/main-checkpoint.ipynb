{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 01:12:30.775978: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-05 01:12:30.776022: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-05 01:12:30.776031: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-05 01:12:30.782626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device Name: NVIDIA RTX A6000\n",
      "Total Memory: 48669.75 MB\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get CUDA device information\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "device = cuda.Device(0)\n",
    "print(\"Device Name:\", device.name())\n",
    "print(\"Total Memory:\", device.total_memory() / (1024 ** 2), \"MB\")\n",
    "print(\"Compute Capability:\", device.compute_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics \n",
    "\n",
    "from package.processtransformer import constants\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "if not os.path.exists(data_dir): \n",
    "  os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 01:12:33.028505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46604 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-10-05 01:12:35.613497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f886c570b70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-05 01:12:35.613540: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-10-05 01:12:35.617477: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-05 01:12:35.632471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2024-10-05 01:12:35.696818: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121/1121 [==============================] - 9s 5ms/step - loss: 0.7917 - sparse_categorical_accuracy: 0.7521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8c142dd5b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"helpdesk\"\n",
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"sepsis.csv\",  \n",
    "#                                     columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S%z\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.NEXT_ACTIVITY, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_ACTIVITY)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "    x_word_dict, y_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_activity_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate over all the prefixes (k) and save the results\n",
    "# k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "# for i in range(max_case_length):\n",
    "#     test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "#     if len(test_data_subset) > 0:\n",
    "#         test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "#             x_word_dict, y_word_dict, max_case_length)   \n",
    "#         y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "#         accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "#         precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "#             test_token_y, y_pred, average=\"weighted\")\n",
    "#         k.append(i)\n",
    "#         accuracies.append(accuracy)\n",
    "#         fscores.append(fscore)\n",
    "#         precisions.append(precision)\n",
    "#         recalls.append(recall)\n",
    "\n",
    "# k.append(i + 1)\n",
    "# accuracies.append(np.mean(accuracy))\n",
    "# fscores.append(np.mean(fscores))\n",
    "# precisions.append(np.mean(precisions))\n",
    "# recalls.append(np.mean(recalls))\n",
    "\n",
    "# print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "# print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "# print('Average precision across all prefixes:', np.mean(precisions))\n",
    "# print('Average recall across all prefixes:', np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "                k    weight  accuracy    fscore  precision    recall\n",
      "0               0  0.275489  0.839520  0.793352   0.825832  0.839520\n",
      "1               1  0.275489  0.724891  0.632353   0.565523  0.724891\n",
      "2               2   0.26015  0.826590  0.775586   0.804216  0.826590\n",
      "3               3  0.107368  0.820728  0.760096   0.782483  0.820728\n",
      "4               4  0.042406  0.758865  0.674794   0.615296  0.758865\n",
      "5               5  0.020752  0.826087  0.756419   0.709124  0.826087\n",
      "6               6  0.009925  0.757576  0.680405   0.640332  0.757576\n",
      "7               7   0.00391  0.615385  0.512821   0.461538  0.615385\n",
      "8               8  0.002105  0.571429  0.485714   0.464286  0.571429\n",
      "9               9  0.001203  0.750000  0.650000   0.583333  0.750000\n",
      "10             10  0.000902  0.666667  0.666667   0.833333  0.666667\n",
      "11             11  0.000301  1.000000  1.000000   1.000000  1.000000\n",
      "12             12       0.0  0.000000  0.000000   0.000000  0.000000\n",
      "13             13       0.0  0.000000  0.000000   0.000000  0.000000\n",
      "14  Weighted Mean            0.796391  0.731922   0.728233  0.796391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics and counts\n",
    "k_list, accuracies, fscores, precisions, recalls = [], [], [], [], []\n",
    "weighted_accuracies, weighted_fscores, weighted_precisions, weighted_recalls = [], [], [], []\n",
    "num_instances_list = []\n",
    "total_instances = 0\n",
    "\n",
    "# Loop over each prefix length\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_instances = len(test_data_subset)\n",
    "    k_list.append(i)\n",
    "    num_instances_list.append(num_instances)\n",
    "    \n",
    "    if num_instances > 0:\n",
    "        total_instances += num_instances\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(\n",
    "            test_data_subset, x_word_dict, y_word_dict, max_case_length\n",
    "        )\n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\"\n",
    "        )\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        weighted_accuracies.append(accuracy * num_instances)\n",
    "        weighted_fscores.append(fscore * num_instances)\n",
    "        weighted_precisions.append(precision * num_instances)\n",
    "        weighted_recalls.append(recall * num_instances)\n",
    "    else:\n",
    "        # If there are no instances for this prefix, append zeros\n",
    "        accuracies.append(0)\n",
    "        fscores.append(0)\n",
    "        precisions.append(0)\n",
    "        recalls.append(0)\n",
    "        weighted_accuracies.append(0)\n",
    "        weighted_fscores.append(0)\n",
    "        weighted_precisions.append(0)\n",
    "        weighted_recalls.append(0)\n",
    "\n",
    "# Compute weights for each prefix\n",
    "weights = [n / total_instances if total_instances > 0 else 0 for n in num_instances_list]\n",
    "\n",
    "# Create a DataFrame with the collected metrics\n",
    "df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'weight': weights,\n",
    "    'accuracy': accuracies,\n",
    "    'fscore': fscores,\n",
    "    'precision': precisions,\n",
    "    'recall': recalls\n",
    "})\n",
    "\n",
    "# Compute weighted average metrics\n",
    "average_accuracy = sum(weighted_accuracies) / total_instances if total_instances > 0 else 0\n",
    "average_fscore = sum(weighted_fscores) / total_instances if total_instances > 0 else 0\n",
    "average_precision = sum(weighted_precisions) / total_instances if total_instances > 0 else 0\n",
    "average_recall = sum(weighted_recalls) / total_instances if total_instances > 0 else 0\n",
    "\n",
    "# Append the weighted averages to the DataFrame\n",
    "weighted_mean_row = {\n",
    "    'k': 'Weighted Mean',\n",
    "    'weight': '',\n",
    "    'accuracy': average_accuracy,\n",
    "    'fscore': average_fscore,\n",
    "    'precision': average_precision,\n",
    "    'recall': average_recall\n",
    "}\n",
    "df = pd.concat([df, pd.DataFrame([weighted_mean_row])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{dataset_name}_next_activity.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1426/1426 [==============================] - 10s 5ms/step - loss: 0.1582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8bf0353d00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"sepsis.csv\",  \n",
    "#                                     columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S%z\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.NEXT_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_next_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# check the k-values #########################################\n",
    "# # Evaluate over all the prefixes (k) and save the results\n",
    "# k, maes, mses, rmses = [],[],[],[]\n",
    "# for i in range(max_case_length):\n",
    "#     test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "#     if len(test_data_subset) > 0:\n",
    "#         test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "#             test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "#         y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "#         _test_y = y_scaler.inverse_transform(test_y)\n",
    "#         _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "#         k.append(i)\n",
    "#         maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "#         mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "#         rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# k.append(i + 1)\n",
    "# maes.append(np.mean(maes))\n",
    "# mses.append(np.mean(mses))\n",
    "# rmses.append(np.mean(rmses))  \n",
    "# print('Average MAE across all prefixes:', np.mean(maes))\n",
    "# print('Average MSE across all prefixes:', np.mean(mses))\n",
    "# print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# # results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "# #     \"mean_squared_error\":mses, \n",
    "# #     \"root_mean_squared_error\":rmses})\n",
    "# # results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "                k    weight       mae         mse       rmse\n",
      "0               0  0.215987  4.419344   43.472816   6.593392\n",
      "1               1  0.215987  4.965712   60.825153   7.799048\n",
      "2               2  0.215987  7.265642  111.144920  10.542529\n",
      "3               3  0.203961  2.522247   25.831480   5.082468\n",
      "4               4  0.084178  2.596650   28.551064   5.343319\n",
      "5               5  0.033247  2.725149   27.500820   5.244123\n",
      "6               6   0.01627  2.704281   24.284027   4.927883\n",
      "7               7  0.007781  2.892667   29.694319   5.449249\n",
      "8               8  0.003065  4.789437   46.134148   6.792212\n",
      "9               9  0.001651  1.597954    6.120871   2.474039\n",
      "10             10  0.000943  5.232703   39.146851   6.256744\n",
      "11             11  0.000707  0.808178    0.874968   0.935397\n",
      "12             12  0.000236  1.776427    3.155694   1.776427\n",
      "13             13       0.0  0.000000    0.000000   0.000000\n",
      "14             14       0.0  0.000000    0.000000   0.000000\n",
      "15  Weighted Mean            4.509707   55.935093   7.200856\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics and counts\n",
    "k_list, maes, mses, rmses = [], [], [], []\n",
    "num_instances_list = []\n",
    "\n",
    "# Loop over each prefix length\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_samples = len(test_data_subset)\n",
    "    \n",
    "    if num_samples > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False\n",
    "        )\n",
    "        \n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(_test_y, _y_pred)\n",
    "        mse = metrics.mean_squared_error(_test_y, _y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(num_samples)\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "    else:\n",
    "        # If there are no instances for this prefix, append zeros\n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(0)\n",
    "        maes.append(0)\n",
    "        mses.append(0)\n",
    "        rmses.append(0)\n",
    "\n",
    "# Compute weights for each prefix\n",
    "total_instances = sum(num_instances_list)\n",
    "weights = [n / total_instances if total_instances > 0 else 0 for n in num_instances_list]\n",
    "\n",
    "# Compute weighted average metrics\n",
    "weighted_mae = np.average(maes, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_mse = np.average(mses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_rmse = np.average(rmses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "\n",
    "# Create a DataFrame with the collected metrics\n",
    "df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'weight': weights,\n",
    "    'mae': maes,\n",
    "    'mse': mses,\n",
    "    'rmse': rmses\n",
    "})\n",
    "\n",
    "# Append the weighted averages to the DataFrame\n",
    "weighted_mean_row = {\n",
    "    'k': 'Weighted Mean',\n",
    "    'weight': '',\n",
    "    'mae': weighted_mae,\n",
    "    'mse': weighted_mse,\n",
    "    'rmse': weighted_rmse\n",
    "}\n",
    "df = pd.concat([df, pd.DataFrame([weighted_mean_row])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{dataset_name}_next_time.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1426/1426 [==============================] - 9s 5ms/step - loss: 0.0922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8bf0538c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"sepsis.csv\",  \n",
    "#                                     columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S%z\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.REMAINING_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.REMAINING_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_remaining_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_remaining_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# check the k-values #########################################\n",
    "# # Evaluate over all the prefixes (k) and save the results\n",
    "# k, maes, mses, rmses = [],[],[],[]\n",
    "# for i in range(max_case_length):\n",
    "#     test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "#     if len(test_data_subset) > 0:\n",
    "#         test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "#             test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "#         y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "#         _test_y = y_scaler.inverse_transform(test_y)\n",
    "#         _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "#         k.append(i)\n",
    "#         maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "#         mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "#         rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# k.append(i + 1)\n",
    "# maes.append(np.mean(maes))\n",
    "# mses.append(np.mean(mses))\n",
    "# rmses.append(np.mean(rmses))  \n",
    "# print('Average MAE across all prefixes:', np.mean(maes))\n",
    "# print('Average MSE across all prefixes:', np.mean(mses))\n",
    "# print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# # results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "# #     \"mean_squared_error\":mses, \n",
    "# #     \"root_mean_squared_error\":rmses})\n",
    "# # results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "                k    weight       mae        mse      rmse\n",
      "0               0  0.215987  7.048288  70.069344  8.370744\n",
      "1               1  0.215987  6.926454  67.150589  8.194547\n",
      "2               2  0.215987  6.392058  59.608566  7.720658\n",
      "3               3  0.203961  2.974392  22.346392  4.727197\n",
      "4               4  0.084178  3.021626  21.677521  4.655912\n",
      "5               5  0.033247  3.657636  28.404425  5.329580\n",
      "6               6   0.01627  3.653376  29.144491  5.398564\n",
      "7               7  0.007781  3.222015  19.224220  4.384543\n",
      "8               8  0.003065  5.172973  49.334621  7.023861\n",
      "9               9  0.001651  3.464543  12.552718  3.542982\n",
      "10             10  0.000943  3.312524  14.327503  3.785169\n",
      "11             11  0.000707  3.892715  16.436295  4.054170\n",
      "12             12  0.000236  2.852385   8.136098  2.852385\n",
      "13             13       0.0  0.000000   0.000000  0.000000\n",
      "14             14       0.0  0.000000   0.000000  0.000000\n",
      "15  Weighted Mean            5.494217  50.662055  6.935167\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics and counts\n",
    "k_list, maes, mses, rmses = [], [], [], []\n",
    "num_instances_list = []\n",
    "\n",
    "# Loop over each prefix length\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_samples = len(test_data_subset)\n",
    "    \n",
    "    if num_samples > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False\n",
    "        )\n",
    "        \n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(_test_y, _y_pred)\n",
    "        mse = metrics.mean_squared_error(_test_y, _y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(num_samples)\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "    else:\n",
    "        # If there are no instances for this prefix, append zeros\n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(0)\n",
    "        maes.append(0)\n",
    "        mses.append(0)\n",
    "        rmses.append(0)\n",
    "\n",
    "# Compute weights for each prefix\n",
    "total_instances = sum(num_instances_list)\n",
    "weights = [n / total_instances if total_instances > 0 else 0 for n in num_instances_list]\n",
    "\n",
    "# Compute weighted average metrics\n",
    "weighted_mae = np.average(maes, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_mse = np.average(mses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_rmse = np.average(rmses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "\n",
    "# Append weighted averages to the lists\n",
    "k_list.append('Weighted Mean')\n",
    "weights.append('')\n",
    "maes.append(weighted_mae)\n",
    "mses.append(weighted_mse)\n",
    "rmses.append(weighted_rmse)\n",
    "\n",
    "# Create a DataFrame with the collected metrics\n",
    "df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'weight': weights,\n",
    "    'mae': maes,\n",
    "    'mse': mses,\n",
    "    'rmse': rmses\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{dataset_name}_remaining_time.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
