{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1121/1121 [==============================] - 24s 16ms/step - loss: 0.7940 - sparse_categorical_accuracy: 0.7441\n",
      "Epoch 2/5\n",
      "1121/1121 [==============================] - 18s 16ms/step - loss: 0.6533 - sparse_categorical_accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "1121/1121 [==============================] - 17s 15ms/step - loss: 0.6440 - sparse_categorical_accuracy: 0.7906\n",
      "Epoch 4/5\n",
      "1121/1121 [==============================] - 19s 17ms/step - loss: 0.6425 - sparse_categorical_accuracy: 0.7922\n",
      "Epoch 5/5\n",
      "1121/1121 [==============================] - 24s 21ms/step - loss: 0.6363 - sparse_categorical_accuracy: 0.7929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17d721b1e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics \n",
    "\n",
    "from package.processtransformer import constants\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "if not os.path.exists(data_dir): \n",
    "  os.mkdir(data_dir)\n",
    "  \n",
    "dataset_name = \"helpdesk\"\n",
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"helpdesk.csv\",  \n",
    "#                                     columns = [\"Case ID\", \"Activity\", \"Complete Timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S.%f\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.NEXT_ACTIVITY, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_ACTIVITY)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "    x_word_dict, y_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 5\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_activity_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 145ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Average accuracy across all prefixes: 0.851619184481714\n",
      "Average f-score across all prefixes: 0.7961396312085119\n",
      "Average precision across all prefixes: 0.7988560937711622\n",
      "Average recall across all prefixes: 0.8392541165218569\n"
     ]
    }
   ],
   "source": [
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "            x_word_dict, y_word_dict, max_case_length)   \n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\")\n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "k.append(i + 1)\n",
    "accuracies.append(np.mean(accuracy))\n",
    "fscores.append(np.mean(fscores))\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "\n",
    "print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "print('Average precision across all prefixes:', np.mean(precisions))\n",
    "print('Average recall across all prefixes:', np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 105ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Weighted average accuracy across all prefixes: 0.7984962406015037\n",
      "Weighted average f-score across all prefixes: 0.7362141988873097\n",
      "Weighted average precision across all prefixes: 0.7304172434950408\n",
      "Weighted average recall across all prefixes: 0.7984962406015037\n"
     ]
    }
   ],
   "source": [
    "k, accuracies, fscores, precisions, recalls = [], [], [], [], []\n",
    "weighted_accuracies, weighted_fscores, weighted_precisions, weighted_recalls = [], [], [], []\n",
    "\n",
    "total_instances = 0\n",
    "\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_instances = len(test_data_subset)\n",
    "    \n",
    "    if num_instances > 0:\n",
    "        total_instances += num_instances\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(\n",
    "            test_data_subset, x_word_dict, y_word_dict, max_case_length\n",
    "        )\n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\"\n",
    "        )\n",
    "        \n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        weighted_accuracies.append(accuracy * num_instances)\n",
    "        weighted_fscores.append(fscore * num_instances)\n",
    "        weighted_precisions.append(precision * num_instances)\n",
    "        weighted_recalls.append(recall * num_instances)\n",
    "\n",
    "# Compute weighted averages\n",
    "average_accuracy = sum(weighted_accuracies) / total_instances\n",
    "average_fscore = sum(weighted_fscores) / total_instances\n",
    "average_precision = sum(weighted_precisions) / total_instances\n",
    "average_recall = sum(weighted_recalls) / total_instances\n",
    "\n",
    "print('Weighted average accuracy across all prefixes:', average_accuracy)\n",
    "print('Weighted average f-score across all prefixes:', average_fscore)\n",
    "print('Weighted average precision across all prefixes:', average_precision)\n",
    "print('Weighted average recall across all prefixes:', average_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1426/1426 [==============================] - 38s 21ms/step - loss: 0.1537\n",
      "Epoch 2/3\n",
      "1426/1426 [==============================] - 29s 21ms/step - loss: 0.1303\n",
      "Epoch 3/3\n",
      "1426/1426 [==============================] - 37s 26ms/step - loss: 0.1277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17d70b949a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics \n",
    "\n",
    "from package.processtransformer import constants\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "if not os.path.exists(data_dir): \n",
    "  os.mkdir(data_dir)\n",
    "  \n",
    "  \n",
    "dataset_name = \"helpdesk\"\n",
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"helpdesk.csv\",  \n",
    "#                                     columns = [\"Case ID\", \"Activity\", \"Complete Timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S.%f\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.NEXT_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_next_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 3\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 21ms/step\n",
      "29/29 [==============================] - 1s 32ms/step\n",
      "29/29 [==============================] - 1s 27ms/step\n",
      "28/28 [==============================] - 1s 23ms/step\n",
      "12/12 [==============================] - 0s 17ms/step\n",
      "5/5 [==============================] - 0s 24ms/step\n",
      "3/3 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Average MAE across all prefixes: 3.0287704\n",
      "Average MSE across all prefixes: 32.57313\n",
      "Average RMSE across all prefixes: 5.136947\n"
     ]
    }
   ],
   "source": [
    "################# check the k-values #########################################\n",
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, maes, mses, rmses = [],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(np.mean(maes))\n",
    "mses.append(np.mean(mses))\n",
    "rmses.append(np.mean(rmses))  \n",
    "print('Average MAE across all prefixes:', np.mean(maes))\n",
    "print('Average MSE across all prefixes:', np.mean(mses))\n",
    "print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "#     \"mean_squared_error\":mses, \n",
    "#     \"root_mean_squared_error\":rmses})\n",
    "# results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 14ms/step\n",
      "29/29 [==============================] - 1s 17ms/step\n",
      "29/29 [==============================] - 1s 19ms/step\n",
      "28/28 [==============================] - 1s 17ms/step\n",
      "12/12 [==============================] - 1s 28ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      "3/3 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Weighted MAE across all prefixes: 4.139796846755072\n",
      "Weighted MSE across all prefixes: 53.59351683905371\n",
      "Weighted RMSE across all prefixes: 7.068014570871801\n"
     ]
    }
   ],
   "source": [
    "k, maes, mses, rmses = [],[],[],[]\n",
    "weights = []\n",
    "\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        num_samples = len(test_data_subset)\n",
    "        weights.append(num_samples)\n",
    "\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# Calculate weighted averages\n",
    "total_weight = np.sum(weights)\n",
    "weighted_mae = np.average(maes, weights=weights)\n",
    "weighted_mse = np.average(mses, weights=weights)\n",
    "weighted_rmse = np.average(rmses, weights=weights)\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(weighted_mae)\n",
    "mses.append(weighted_mse)\n",
    "rmses.append(weighted_rmse)\n",
    "\n",
    "print('Weighted MAE across all prefixes:', weighted_mae)\n",
    "print('Weighted MSE across all prefixes:', weighted_mse)\n",
    "print('Weighted RMSE across all prefixes:', weighted_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
