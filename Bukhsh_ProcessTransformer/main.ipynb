{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 01:16:33.446979: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-05 01:16:33.447036: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-05 01:16:33.447048: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-05 01:16:33.454212: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device Name: NVIDIA RTX A6000\n",
      "Total Memory: 48669.75 MB\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get CUDA device information\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "device = cuda.Device(0)\n",
    "print(\"Device Name:\", device.name())\n",
    "print(\"Total Memory:\", device.total_memory() / (1024 ** 2), \"MB\")\n",
    "print(\"Compute Capability:\", device.compute_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics \n",
    "\n",
    "from package.processtransformer import constants\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "if not os.path.exists(data_dir): \n",
    "  os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 01:16:35.762898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46604 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 01:16:38.338641: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f805e7bc230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-05 01:16:38.338678: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-10-05 01:16:38.342916: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-05 01:16:38.357350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2024-10-05 01:16:38.424935: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121/1121 [==============================] - 9s 5ms/step - loss: 0.7832 - sparse_categorical_accuracy: 0.7482\n",
      "Epoch 2/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6526 - sparse_categorical_accuracy: 0.7891\n",
      "Epoch 3/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6433 - sparse_categorical_accuracy: 0.7909\n",
      "Epoch 4/100\n",
      "1121/1121 [==============================] - 4s 4ms/step - loss: 0.6385 - sparse_categorical_accuracy: 0.7906\n",
      "Epoch 5/100\n",
      "1121/1121 [==============================] - 4s 4ms/step - loss: 0.6368 - sparse_categorical_accuracy: 0.7898\n",
      "Epoch 6/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6377 - sparse_categorical_accuracy: 0.7909\n",
      "Epoch 7/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6361 - sparse_categorical_accuracy: 0.7929\n",
      "Epoch 8/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6332 - sparse_categorical_accuracy: 0.7922\n",
      "Epoch 9/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6331 - sparse_categorical_accuracy: 0.7924\n",
      "Epoch 10/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6306 - sparse_categorical_accuracy: 0.7933\n",
      "Epoch 11/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6316 - sparse_categorical_accuracy: 0.7929\n",
      "Epoch 12/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6341 - sparse_categorical_accuracy: 0.7926\n",
      "Epoch 13/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6284 - sparse_categorical_accuracy: 0.7938\n",
      "Epoch 14/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6295 - sparse_categorical_accuracy: 0.7947\n",
      "Epoch 15/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6285 - sparse_categorical_accuracy: 0.7925\n",
      "Epoch 16/100\n",
      "1121/1121 [==============================] - 5s 4ms/step - loss: 0.6288 - sparse_categorical_accuracy: 0.7922\n",
      "Epoch 17/100\n",
      " 168/1121 [===>..........................] - ETA: 3s - loss: 0.6405 - sparse_categorical_accuracy: 0.7937"
     ]
    }
   ],
   "source": [
    "dataset_name = \"helpdesk\"\n",
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"sepsis.csv\",  \n",
    "#                                     columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S%z\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.NEXT_ACTIVITY, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_ACTIVITY)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "    x_word_dict, y_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_activity_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate over all the prefixes (k) and save the results\n",
    "# k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "# for i in range(max_case_length):\n",
    "#     test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "#     if len(test_data_subset) > 0:\n",
    "#         test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "#             x_word_dict, y_word_dict, max_case_length)   \n",
    "#         y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "#         accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "#         precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "#             test_token_y, y_pred, average=\"weighted\")\n",
    "#         k.append(i)\n",
    "#         accuracies.append(accuracy)\n",
    "#         fscores.append(fscore)\n",
    "#         precisions.append(precision)\n",
    "#         recalls.append(recall)\n",
    "\n",
    "# k.append(i + 1)\n",
    "# accuracies.append(np.mean(accuracy))\n",
    "# fscores.append(np.mean(fscores))\n",
    "# precisions.append(np.mean(precisions))\n",
    "# recalls.append(np.mean(recalls))\n",
    "\n",
    "# print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "# print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "# print('Average precision across all prefixes:', np.mean(precisions))\n",
    "# print('Average recall across all prefixes:', np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics and counts\n",
    "k_list, accuracies, fscores, precisions, recalls = [], [], [], [], []\n",
    "weighted_accuracies, weighted_fscores, weighted_precisions, weighted_recalls = [], [], [], []\n",
    "num_instances_list = []\n",
    "total_instances = 0\n",
    "\n",
    "# Loop over each prefix length\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_instances = len(test_data_subset)\n",
    "    k_list.append(i)\n",
    "    num_instances_list.append(num_instances)\n",
    "    \n",
    "    if num_instances > 0:\n",
    "        total_instances += num_instances\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(\n",
    "            test_data_subset, x_word_dict, y_word_dict, max_case_length\n",
    "        )\n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\"\n",
    "        )\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        weighted_accuracies.append(accuracy * num_instances)\n",
    "        weighted_fscores.append(fscore * num_instances)\n",
    "        weighted_precisions.append(precision * num_instances)\n",
    "        weighted_recalls.append(recall * num_instances)\n",
    "    else:\n",
    "        # If there are no instances for this prefix, append zeros\n",
    "        accuracies.append(0)\n",
    "        fscores.append(0)\n",
    "        precisions.append(0)\n",
    "        recalls.append(0)\n",
    "        weighted_accuracies.append(0)\n",
    "        weighted_fscores.append(0)\n",
    "        weighted_precisions.append(0)\n",
    "        weighted_recalls.append(0)\n",
    "\n",
    "# Compute weights for each prefix\n",
    "weights = [n / total_instances if total_instances > 0 else 0 for n in num_instances_list]\n",
    "\n",
    "# Create a DataFrame with the collected metrics\n",
    "df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'weight': weights,\n",
    "    'accuracy': accuracies,\n",
    "    'fscore': fscores,\n",
    "    'precision': precisions,\n",
    "    'recall': recalls\n",
    "})\n",
    "\n",
    "# Compute weighted average metrics\n",
    "average_accuracy = sum(weighted_accuracies) / total_instances if total_instances > 0 else 0\n",
    "average_fscore = sum(weighted_fscores) / total_instances if total_instances > 0 else 0\n",
    "average_precision = sum(weighted_precisions) / total_instances if total_instances > 0 else 0\n",
    "average_recall = sum(weighted_recalls) / total_instances if total_instances > 0 else 0\n",
    "\n",
    "# Append the weighted averages to the DataFrame\n",
    "weighted_mean_row = {\n",
    "    'k': 'Weighted Mean',\n",
    "    'weight': '',\n",
    "    'accuracy': average_accuracy,\n",
    "    'fscore': average_fscore,\n",
    "    'precision': average_precision,\n",
    "    'recall': average_recall\n",
    "}\n",
    "df = pd.concat([df, pd.DataFrame([weighted_mean_row])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{dataset_name}_next_activity.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"sepsis.csv\",  \n",
    "#                                     columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S%z\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.NEXT_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_next_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# check the k-values #########################################\n",
    "# # Evaluate over all the prefixes (k) and save the results\n",
    "# k, maes, mses, rmses = [],[],[],[]\n",
    "# for i in range(max_case_length):\n",
    "#     test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "#     if len(test_data_subset) > 0:\n",
    "#         test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "#             test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "#         y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "#         _test_y = y_scaler.inverse_transform(test_y)\n",
    "#         _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "#         k.append(i)\n",
    "#         maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "#         mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "#         rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# k.append(i + 1)\n",
    "# maes.append(np.mean(maes))\n",
    "# mses.append(np.mean(mses))\n",
    "# rmses.append(np.mean(rmses))  \n",
    "# print('Average MAE across all prefixes:', np.mean(maes))\n",
    "# print('Average MSE across all prefixes:', np.mean(mses))\n",
    "# print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# # results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "# #     \"mean_squared_error\":mses, \n",
    "# #     \"root_mean_squared_error\":rmses})\n",
    "# # results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics and counts\n",
    "k_list, maes, mses, rmses = [], [], [], []\n",
    "num_instances_list = []\n",
    "\n",
    "# Loop over each prefix length\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_samples = len(test_data_subset)\n",
    "    \n",
    "    if num_samples > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False\n",
    "        )\n",
    "        \n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(_test_y, _y_pred)\n",
    "        mse = metrics.mean_squared_error(_test_y, _y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(num_samples)\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "    else:\n",
    "        # If there are no instances for this prefix, append zeros\n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(0)\n",
    "        maes.append(0)\n",
    "        mses.append(0)\n",
    "        rmses.append(0)\n",
    "\n",
    "# Compute weights for each prefix\n",
    "total_instances = sum(num_instances_list)\n",
    "weights = [n / total_instances if total_instances > 0 else 0 for n in num_instances_list]\n",
    "\n",
    "# Compute weighted average metrics\n",
    "weighted_mae = np.average(maes, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_mse = np.average(mses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_rmse = np.average(rmses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "\n",
    "# Create a DataFrame with the collected metrics\n",
    "df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'weight': weights,\n",
    "    'mae': maes,\n",
    "    'mse': mses,\n",
    "    'rmse': rmses\n",
    "})\n",
    "\n",
    "# Append the weighted averages to the DataFrame\n",
    "weighted_mean_row = {\n",
    "    'k': 'Weighted Mean',\n",
    "    'weight': '',\n",
    "    'mae': weighted_mae,\n",
    "    'mse': weighted_mse,\n",
    "    'rmse': weighted_rmse\n",
    "}\n",
    "df = pd.concat([df, pd.DataFrame([weighted_mean_row])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{dataset_name}_next_time.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processor = LogsDataProcessor(name=dataset_name, filepath=\"sepsis.csv\",  \n",
    "#                                     columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "#                                     dir_path='datasets', datetime_format=\"%Y-%m-%d %H:%M:%S%z\", pool = 4)\n",
    "# data_processor.process_logs(task=constants.Task.REMAINING_TIME, sort_temporally= False)\n",
    "\n",
    "# Load data\n",
    "data_loader = LogsDataLoader(name = dataset_name)\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.REMAINING_TIME)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_time_x, train_token_y, time_scaler, y_scaler = data_loader.prepare_data_remaining_time(train_df, \n",
    "                                                        x_word_dict, max_case_length)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 100\n",
    "\n",
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_remaining_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with equal weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# check the k-values #########################################\n",
    "# # Evaluate over all the prefixes (k) and save the results\n",
    "# k, maes, mses, rmses = [],[],[],[]\n",
    "# for i in range(max_case_length):\n",
    "#     test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "#     if len(test_data_subset) > 0:\n",
    "#         test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "#             test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "#         y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "#         _test_y = y_scaler.inverse_transform(test_y)\n",
    "#         _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "#         k.append(i)\n",
    "#         maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "#         mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "#         rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "# k.append(i + 1)\n",
    "# maes.append(np.mean(maes))\n",
    "# mses.append(np.mean(mses))\n",
    "# rmses.append(np.mean(rmses))  \n",
    "# print('Average MAE across all prefixes:', np.mean(maes))\n",
    "# print('Average MSE across all prefixes:', np.mean(mses))\n",
    "# print('Average RMSE across all prefixes:', np.mean(rmses))\n",
    "\n",
    "\n",
    "# # results_df = pd.DataFrame({\"k\":k, \"mean_absolute_error\":maes, \n",
    "# #     \"mean_squared_error\":mses, \n",
    "# #     \"root_mean_squared_error\":rmses})\n",
    "# # results_df.to_csv(result_path+\"_next_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with weighted prefixes (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics and counts\n",
    "k_list, maes, mses, rmses = [], [], [], []\n",
    "num_instances_list = []\n",
    "\n",
    "# Loop over each prefix length\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    num_samples = len(test_data_subset)\n",
    "    \n",
    "    if num_samples > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False\n",
    "        )\n",
    "        \n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(_test_y, _y_pred)\n",
    "        mse = metrics.mean_squared_error(_test_y, _y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(num_samples)\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "    else:\n",
    "        # If there are no instances for this prefix, append zeros\n",
    "        k_list.append(i)\n",
    "        num_instances_list.append(0)\n",
    "        maes.append(0)\n",
    "        mses.append(0)\n",
    "        rmses.append(0)\n",
    "\n",
    "# Compute weights for each prefix\n",
    "total_instances = sum(num_instances_list)\n",
    "weights = [n / total_instances if total_instances > 0 else 0 for n in num_instances_list]\n",
    "\n",
    "# Compute weighted average metrics\n",
    "weighted_mae = np.average(maes, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_mse = np.average(mses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "weighted_rmse = np.average(rmses, weights=num_instances_list) if total_instances > 0 else 0\n",
    "\n",
    "# Append weighted averages to the lists\n",
    "k_list.append('Weighted Mean')\n",
    "weights.append('')\n",
    "maes.append(weighted_mae)\n",
    "mses.append(weighted_mse)\n",
    "rmses.append(weighted_rmse)\n",
    "\n",
    "# Create a DataFrame with the collected metrics\n",
    "df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'weight': weights,\n",
    "    'mae': maes,\n",
    "    'mse': mses,\n",
    "    'rmse': rmses\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{dataset_name}_remaining_time.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
