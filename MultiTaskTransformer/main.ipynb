{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from typing import List, Optional\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import pm4py\n",
    "from package import transformer\n",
    "from package.loader import LogsDataLoader\n",
    "from package.processor import LogsDataProcessor, masked_standard_scaler, masked_min_max_scaler\n",
    "from package.constants import Feature_Type, Target, Temporal_Feature, Model_Architecture\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize data dir, if not exists\n",
    "if not os.path.exists(\"datasets\"): \n",
    "    os.mkdir(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, dataset_name: str, filepath: str, sorting: bool, columns: List[str],\n",
    "                 additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, model_epochs: int, warmup_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: Dict[str, Target], temporal_features: Dict[Temporal_Feature, bool],\n",
    "                 cross_val: bool, model_architecture: Model_Architecture):\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.sorting: bool = sorting\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.warmup_epochs: int = warmup_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[str, Target] = target_columns\n",
    "        for target_col in target_columns.keys():\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[\"concept_name\"] = self.target_columns.pop(target_col)\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        self.temporal_features: Dict[Temporal_Feature, bool] = temporal_features\n",
    "        self.cross_val = cross_val\n",
    "        self.model_architecture = model_architecture\n",
    "        self.start_timestamp = None\n",
    "        self.end_timestamp = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def save_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            sorting=self.sorting,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,  # Add all additional columns here, first all categorical, then all numerical features\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            datetime_format=self.datetime_format,\n",
    "            temporal_features=self.temporal_features,\n",
    "            pool=4\n",
    "        )\n",
    "        \n",
    "        # TODO: sanitize columns\n",
    "        # self.columns = [data_processor.sanitize_filename(col) for col in self.columns]\n",
    "        \n",
    "        # self.additional_columns = {\n",
    "        #                         feature_type: [data_processor.sanitize_filename(feature) for feature in feature_lst] for feature_type,\n",
    "        #                         feature_lst in self.additional_columns.items()\n",
    "        #                         } if len(self.additional_columns)>0 else self.additional_columns\n",
    "        self.target_columns = {data_processor.sanitize_filename(feature, self.columns): target for feature, target in self.target_columns.items()}\n",
    "        self.input_columns = [data_processor.sanitize_filename(col, self.columns) for col in self.input_columns]\n",
    "        self.columns = [data_processor.sanitize_filename(col, self.columns) for col in self.columns]\n",
    "        \n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs()\n",
    "        # flatten self.additional_columns to get all used features\n",
    "        self.additional_columns = data_processor.additional_columns\n",
    "        self.used_features = [item for sublist in self.additional_columns.values() for item in sublist]\n",
    "        \n",
    "        \n",
    "        # TODO: Compute the number of unique classes in each categorical column\n",
    "        # train_df = pd.read_csv(os.path.join(\"datasets\", self.dataset_name, \"processed\", f\"{self._preprocessing_id}_train.csv\"))\n",
    "        # num_classes_list = data_processor._compute_num_classes(train_df)\n",
    "        \n",
    "        # return num_classes_list\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, sorting=self.sorting, input_columns=self.input_columns,\n",
    "                                     target_columns=self.target_columns, temporal_features=self.temporal_features)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict, mask = data_loader.load_data()\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame], x_scaler=None, y_scaler=None,\n",
    "                     train: bool = True) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data...\")\n",
    "        # initialize max_case_length\n",
    "        max_case_length = False\n",
    "        # initialize token dicts\n",
    "        x_token_dict, y_token_dict, x_token_dict_numerical, y_token_dict_numerical = {}, {}, {}, {}\n",
    "        \n",
    "        # initialize case_id_df\n",
    "        case_ids = next(iter(dfs.values()))[\"case_id\"]\n",
    "        \n",
    "        # loop over all feature dfs\n",
    "        for idx, (feature, feature_df) in enumerate(dfs.items()):\n",
    "\n",
    "            feature_type = None\n",
    "            # get current feature_type\n",
    "            for _feature_type, feature_lst in self.additional_columns.items():\n",
    "                if feature in feature_lst:\n",
    "                    feature_type = _feature_type\n",
    "                    break\n",
    "            \n",
    "            if idx == 0 and train:\n",
    "                (x_tokens, y_tokens, max_case_length\n",
    "                ) = data_loader.prepare_data(feature=feature, df=feature_df, max_case_length=True)\n",
    "            else:\n",
    "                x_tokens, y_tokens = data_loader.prepare_data(feature=feature, df=feature_df)\n",
    "            \n",
    "            if feature_type is Feature_Type.TIMESTAMP or feature_type is Feature_Type.NUMERICAL:\n",
    "                x_token_dict_numerical.update(x_tokens)\n",
    "                y_token_dict_numerical.update(y_tokens)\n",
    "            else:\n",
    "                # update x_token_dict\n",
    "                x_token_dict.update(x_tokens)\n",
    "                y_token_dict.update(y_tokens)\n",
    "            \n",
    "        # TODO:\n",
    "        if len(x_token_dict_numerical) > 0  and len(list(x_token_dict_numerical.values())[0]) > 0:\n",
    "            # Concatenate all the feature arrays along the rows (axis=0)\n",
    "            combined_data = np.vstack(list(x_token_dict_numerical.values()))\n",
    "            if x_scaler is None:\n",
    "                # Initialize the StandardScaler\n",
    "                # x_scaler = StandardScaler()\n",
    "                # x_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                # x_scaler = FunctionTransformer(masked_standard_scaler, kw_args={'padding_value': -1})\n",
    "                x_scaler = FunctionTransformer(masked_min_max_scaler, kw_args={'padding_value': -1})\n",
    "                # Fit the scaler on the combined data\n",
    "                x_scaler.fit(combined_data)\n",
    "            # Transform the combined data\n",
    "            scaled_combined_data = x_scaler.transform(combined_data)\n",
    "            # split the scaled combined data back into the original feature dict\n",
    "            split_indices = np.cumsum([value.shape[0] for value in x_token_dict_numerical.values()])[:-1]\n",
    "            scaled_data_parts = np.vsplit(scaled_combined_data, split_indices)\n",
    "            # Reconstruct the dictionary with scaled data\n",
    "            scaled_dict = {key: scaled_data_parts[i] for i, key in enumerate(x_token_dict_numerical.keys())}\n",
    "            # update x_token_dict\n",
    "            x_token_dict.update(scaled_dict)\n",
    "        if len(y_token_dict_numerical) > 0:\n",
    "            # Prepare list to store valid arrays (non-empty)\n",
    "            valid_arrays = []\n",
    "            valid_keys = []\n",
    "\n",
    "            # Check for empty arrays and prepare data for scaling\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size > 0:  # Only consider non-empty arrays\n",
    "                    valid_arrays.append(value.reshape(-1, 1))  # Reshape to 2D\n",
    "                    valid_keys.append(key)\n",
    "\n",
    "            # If there are valid arrays to scale\n",
    "            if valid_arrays:\n",
    "                combined_data = np.hstack(valid_arrays)  # Horizontal stacking for features\n",
    "\n",
    "                if y_scaler is None:\n",
    "                    # Initialize the StandardScaler\n",
    "                    # y_scaler = StandardScaler()\n",
    "                    y_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                    # Fit the scaler on the combined data\n",
    "                    y_scaler.fit(combined_data)\n",
    "\n",
    "                # Transform the combined data\n",
    "                scaled_combined_data = y_scaler.transform(combined_data)\n",
    "\n",
    "                # Split the scaled combined data back into individual features\n",
    "                scaled_data_parts = np.hsplit(scaled_combined_data, scaled_combined_data.shape[1])\n",
    "\n",
    "                # Reconstruct the dictionary with scaled data\n",
    "                scaled_dict = {key: scaled_data_parts[i].flatten() for i, key in enumerate(valid_keys)}\n",
    "\n",
    "                # Update y_token_dict with the scaled data\n",
    "                y_token_dict.update(scaled_dict)\n",
    "\n",
    "            # Handle any empty arrays (if necessary)\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size == 0:\n",
    "                    # Optionally, you can handle empty arrays here, e.g., leave them as-is\n",
    "                    y_token_dict[key] = value\n",
    "            \n",
    "            \n",
    "        # sort dicts\n",
    "        x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "        y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "\n",
    "        return case_ids, x_token_dict, y_token_dict, x_scaler, y_scaler, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "            case_ids: pd.DataFrame,\n",
    "            feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "            train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "            train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "            word_dicts: Dict[str, Dict[str, int]],\n",
    "            max_case_length: int,\n",
    "            y_scaler,\n",
    "            mask # Fraction of the training data to be used for validation\n",
    "            ) -> tf.keras.Model:\n",
    "\n",
    "        # Ensure that input columns and dictionaries are sorted\n",
    "        self.input_columns.sort()\n",
    "        self.target_columns = dict(sorted(self.target_columns.items()))\n",
    "        train_token_dict_x = dict(sorted(train_token_dict_x.items()))\n",
    "        train_token_dict_y = dict(sorted(train_token_dict_y.items()))\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "\n",
    "        # initialize model_wrapper with data for model\n",
    "        model_wrapper = transformer.ModelWrapper(\n",
    "                                                dataset_name = self.dataset_name,\n",
    "                                                case_ids = case_ids,\n",
    "                                                input_columns=self.input_columns,\n",
    "                                                target_columns=self.target_columns,\n",
    "                                                additional_columns=self.additional_columns,\n",
    "                                                word_dicts=word_dicts,\n",
    "                                                max_case_length=max_case_length,\n",
    "                                                feature_type_dict=feature_type_dict,\n",
    "                                                temporal_features=self.temporal_features,\n",
    "                                                model_architecture=self.model_architecture,\n",
    "                                                sorting=self.sorting,\n",
    "                                                masking=True\n",
    "                                                )\n",
    "        \n",
    "        # train the model\n",
    "        models, histories = model_wrapper.train_model(\n",
    "                                                    train_token_dict_x = train_token_dict_x,\n",
    "                                                    train_token_dict_y = train_token_dict_y,\n",
    "                                                    cross_val = self.cross_val,\n",
    "                                                    y_scaler = y_scaler,\n",
    "                                                    model_epochs = self.model_epochs,\n",
    "                                                    batch_size = 12,\n",
    "                                                    warmup_epochs = self.warmup_epochs,\n",
    "                                                    initial_lr = 1e-5,\n",
    "                                                    target_lr = 1e-3\n",
    "                                                    )\n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(histories)\n",
    "        return models, histories\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, histories):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # If there are multiple histories (cross-validation), plot for each fold\n",
    "        if isinstance(histories, list):\n",
    "            for i, history in enumerate(histories):\n",
    "                plt.plot(history.history['loss'], label=f'Training Loss Fold {i+1}')\n",
    "                if 'val_loss' in history.history:\n",
    "                    plt.plot(history.history['val_loss'], label=f'Validation Loss Fold {i+1}')\n",
    "        else:\n",
    "            # Single history (no cross-validation)\n",
    "            plt.plot(histories.history['loss'], label='Training Loss')\n",
    "            if 'val_loss' in histories.history:\n",
    "                plt.plot(histories.history['val_loss'], label='Validation Loss')\n",
    "        \n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def evaluate(self, models, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame],\n",
    "                 max_case_length: int, x_scaler=None, y_scaler=None):\n",
    "        \n",
    "        #TODO: testing\n",
    "        # print(f\"Unscaled MAE training: {y_scaler.scale_ * 0.3030}\")\n",
    "        results, preds = [], []\n",
    "        for idx, model in enumerate(models):\n",
    "            print(f\"Evaluating model {idx+1}...\")\n",
    "\n",
    "            # Prepare lists to store evaluation metrics\n",
    "            k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "            mae, mse, rmse, r2 = {}, {}, {}, {}\n",
    "            \n",
    "            for target_col in self.target_columns.keys():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        k.update({target_col: []})\n",
    "                        weights.update({target_col: []})\n",
    "                        \n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            accuracies.update({target_col: []})\n",
    "                            fscores.update({target_col: []})\n",
    "                            precisions.update({target_col: []})\n",
    "                            recalls.update({target_col: []})\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            mae.update({target_col: []})\n",
    "                            mse.update({target_col: []})\n",
    "                            rmse.update({target_col: []})\n",
    "                            r2.update({target_col: []})\n",
    "\n",
    "            # Calculate total number of samples\n",
    "            total_samples = len(list(test_dfs.values())[0])\n",
    "\n",
    "            # Iterate over all prefixes (k)\n",
    "            for i in range(1, max_case_length + 1):\n",
    "                print(\"Prefix length: \" + str(i))\n",
    "                test_data_subsets = {}\n",
    "\n",
    "                for key, df in test_dfs.items():\n",
    "                    if (Feature_Type.TIMESTAMP in self.additional_columns\n",
    "                            and key in self.additional_columns[Feature_Type.TIMESTAMP]):\n",
    "                        prefix_str = f\"{key}##Prefix Length\"\n",
    "                    else:\n",
    "                        prefix_str = \"Prefix Length\"\n",
    "                    filtered_df = df[df[prefix_str] == i]\n",
    "                    test_data_subsets.update({key: filtered_df})\n",
    "\n",
    "\n",
    "                _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_data_subsets,\n",
    "                                                                x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "\n",
    "                # sort dicts\n",
    "                x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "                y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "\n",
    "                if len(test_data_subsets[self.input_columns[0]]) > 0:\n",
    "\n",
    "                    # Make predictions\n",
    "                    predictions = model.predict(x_token_dict)\n",
    "                    \n",
    "                    # Handle multiple outputs for multitask learning\n",
    "                    if len(self.target_columns) > 1:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "                    else:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "\n",
    "                    # Compute metrics\n",
    "                    for feature, result in result_dict.items():\n",
    "                        for feature_type, feature_lst in self.additional_columns.items():\n",
    "                            if feature in feature_lst:\n",
    "                                if feature_type is Feature_Type.CATEGORICAL:\n",
    "                                    result = np.argmax(result, axis=1)\n",
    "                                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}\"], result)\n",
    "                                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                                        y_token_dict[f\"output_{feature}\"], result, average=\"weighted\", zero_division=0)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[feature].append(i)\n",
    "                                    accuracies[feature].append(accuracy)\n",
    "                                    fscores[feature].append(fscore)\n",
    "                                    precisions[feature].append(precision)\n",
    "                                    recalls[feature].append(recall)\n",
    "                                    weights[feature].append(weight)\n",
    "                                \n",
    "                                elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                                    y_true_unscaled = y_token_dict[f\"output_{feature}\"]\n",
    "                                    y_true = y_scaler.inverse_transform( y_true_unscaled.reshape(-1, y_true_unscaled.shape[-1])\n",
    "                                                                        ).reshape(y_true_unscaled.shape)\n",
    "                                    y_pred = y_scaler.inverse_transform( result )\n",
    "                                    mae_value = metrics.mean_absolute_error(y_true, y_pred)\n",
    "                                    mse_value = metrics.mean_squared_error(y_true, y_pred)\n",
    "                                    rmse_value = np.sqrt(mse_value)\n",
    "                                    r2_value = metrics.r2_score(y_true, y_pred)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[feature].append(i)\n",
    "                                    mae[feature].append(mae_value)\n",
    "                                    mse[feature].append(mse_value)\n",
    "                                    rmse[feature].append(rmse_value)\n",
    "                                    r2[feature].append(r2_value)\n",
    "                                    weights[feature].append(weight)\n",
    "            feature_results = []\n",
    "            for target_col in self.target_columns.keys():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_accuracy = np.average(accuracies[target_col], weights=weights[target_col])\n",
    "                            weighted_fscore = np.average(fscores[target_col], weights=weights[target_col])\n",
    "                            weighted_precision = np.average(precisions[target_col], weights=weights[target_col])\n",
    "                            weighted_recall = np.average(recalls[target_col], weights=weights[target_col])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_col].append(\"\")\n",
    "                            k[target_col].append(\"Weighted Mean\")\n",
    "                            accuracies[target_col].append(weighted_accuracy)\n",
    "                            fscores[target_col].append(weighted_fscore)\n",
    "                            precisions[target_col].append(weighted_precision)\n",
    "                            recalls[target_col].append(weighted_recall)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_col}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_col],\n",
    "                                'weight': weights[target_col],\n",
    "                                'accuracy': accuracies[target_col],\n",
    "                                'fscore': fscores[target_col],\n",
    "                                'precision': precisions[target_col],\n",
    "                                'recall': recalls[target_col]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "                        \n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_mae = np.average(mae[target_col], weights=weights[target_col])\n",
    "                            weighted_mse = np.average(mse[target_col], weights=weights[target_col])\n",
    "                            weighted_rmse = np.average(rmse[target_col], weights=weights[target_col])\n",
    "                            weighted_r2 = np.average(r2[target_col], weights=weights[target_col])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_col].append(\"\")\n",
    "                            k[target_col].append(\"Weighted Mean\")\n",
    "                            mae[target_col].append(weighted_mae)\n",
    "                            mse[target_col].append(weighted_mse)\n",
    "                            rmse[target_col].append(weighted_rmse)\n",
    "                            r2[target_col].append(weighted_r2)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_col}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_col],\n",
    "                                'weight': weights[target_col],\n",
    "                                'mae': mae[target_col],\n",
    "                                'mse': mse[target_col],\n",
    "                                'rmse': rmse[target_col],\n",
    "                                'r2': r2[target_col]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "            results.append(feature_results)\n",
    "            print(\"_____________________________________________\")\n",
    "            \n",
    "          \n",
    "            \n",
    "            # calculate predictions for all test data\n",
    "            _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_dfs,\n",
    "                                                    x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "            # sort dicts\n",
    "            x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "            y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(x_token_dict)\n",
    "            \n",
    "            # Handle multiple outputs for multitask learning\n",
    "            if len(self.target_columns) > 1:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "            else:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "                \n",
    "            feature_preds = []\n",
    "            for feature, result in result_dict.items():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if feature in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            y_true = y_token_dict[f\"output_{feature}\"]\n",
    "                            y_pred = np.argmax(result, axis=1)\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            y_true_unscaled = y_token_dict[f\"output_{feature}\"]\n",
    "                            y_true = y_scaler.inverse_transform( y_true_unscaled.reshape(-1, y_true_unscaled.shape[-1])\n",
    "                                                                ).reshape(y_true_unscaled.shape)\n",
    "                            y_pred = y_scaler.inverse_transform( result )\n",
    "                        preds_df = pd.DataFrame({\n",
    "                                        'y_true': y_true.reshape(-1),\n",
    "                                        'y_pred': y_pred.reshape(-1)\n",
    "                                    })\n",
    "                        feature_preds.append(preds_df)\n",
    "            preds.append(feature_preds)\n",
    "                    \n",
    "        \n",
    "        return results, preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            \n",
    "    def safe_results(self, y_scaler, histories: list, results: list, preds: list):\n",
    "        \n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        elapsed_time = self.start_timestamp - self.end_timestamp\n",
    "        \n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name, \"results\", timestamp )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Safe parameters in json\n",
    "        parameters = {\"Input Columns\": self.input_columns,\n",
    "                      \"Target Columns\": {key: value.value for key, value in self.target_columns.items()},\n",
    "                      \"Model Epochs\": self.model_epochs,\n",
    "                      \"Transformer Layers\": self.model_num_layers,\n",
    "                      \"Sorting\": self.sorting,\n",
    "                      \"Cross Validation\": self.cross_val,\n",
    "                      \"Elapsed Time\": elapsed_time\n",
    "                      }\n",
    "        coded_json = json.dumps(parameters)\n",
    "        with open(os.path.join(dir_path, \"parameters.json\"), \"w\") as metadata_file:\n",
    "            metadata_file.write(coded_json)\n",
    "        \n",
    "        # Save histories and results\n",
    "        for model_idx, (history, result, pred) in enumerate(zip(histories, results, preds)):\n",
    "            history_df = pd.DataFrame(history.history)\n",
    "            # reverse transform history\n",
    "            for col in history_df.columns:\n",
    "                if \"mean_absolute_error\" in col:\n",
    "                    history_df[col] = y_scaler.inverse_transform( history_df[col].values.reshape(-1, 1) ).flatten()\n",
    "            # Save history as CSV\n",
    "            history_path = os.path.join(dir_path, f\"history_{model_idx+1}.csv\")\n",
    "            history_df.to_csv(history_path, index=False)\n",
    "            \n",
    "            for output_idx, (output_result_df, output_pred_df) in enumerate(zip(result, pred)):\n",
    "                feature = list(self.target_columns.keys())[output_idx]\n",
    "                # Save results DataFrame as CSV\n",
    "                results_path = os.path.join(dir_path, f\"results_{model_idx+1}__{feature}.csv\")\n",
    "                output_result_df.to_csv(results_path, index=False)\n",
    "                \n",
    "                # Save predictions DataFrame as CSV\n",
    "                results_path = os.path.join(dir_path, f\"predictions_{model_idx+1}__{feature}.csv\")\n",
    "                output_pred_df.to_csv(results_path, index=False)\n",
    "\n",
    "        print(f\"Histories and results saved to {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "# helper function to save xes file as csv\n",
    "def save_csv(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    pipe.save_as_csv()\n",
    "    \n",
    "\n",
    "# helper function: do only preprocessing on data\n",
    "def preprocess(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "\n",
    "# helper function\n",
    "def run(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    pipe.start_timestamp = time.time()\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    case_ids, train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    #TODO: debugging\n",
    "    # print(\"debug case_ids\")\n",
    "    # print(case_ids)\n",
    "    case_ids = case_ids.astype(str)\n",
    "    # Check for NaN or None values using pd.Series.isna()\n",
    "    assert not case_ids.isna().any(), \"case_ids contains NaN or None values!\"\n",
    "\n",
    "    # train the model\n",
    "    models, histories = pipe.train(\n",
    "                case_ids = case_ids,\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length,\n",
    "                y_scaler = y_scaler,\n",
    "                mask = mask\n",
    "                )\n",
    "\n",
    "    # evaluate the model\n",
    "    results, preds = pipe.evaluate(models=models, data_loader=data_loader, test_dfs=test_dfs, x_scaler=x_scaler,\n",
    "                                y_scaler=y_scaler, max_case_length=max_case_length)\n",
    "    \n",
    "    pipe.end_timestamp = time.time()\n",
    "    \n",
    "    # safe the training histories and results\n",
    "    pipe.safe_results(y_scaler=y_scaler, histories=histories, results=results, preds=preds)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"======================================\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    \n",
    "# function for testing out code\n",
    "def test(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # # train the model\n",
    "    # model = pipe.train(\n",
    "    #             feature_type_dict = feature_type_dict,\n",
    "    #             train_token_dict_x = train_token_dict_x,\n",
    "    #             train_token_dict_y = train_token_dict_y,\n",
    "    #             word_dicts = word_dicts,\n",
    "    #             max_case_length = max_case_length\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Syntax lookup\n",
    "# # \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "# # \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "# # \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "\n",
    "\n",
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_sepsis = {\n",
    "#         \"dataset_name\": \"sepsis\",\n",
    "#         \"filepath\": \"sepsis.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2012 = {\n",
    "#         \"dataset_name\": \"bpi_2012\",\n",
    "#         \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2013 = {\n",
    "#         \"dataset_name\": \"bpi_2013\",\n",
    "#         \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2015_1 = {\n",
    "#         \"dataset_name\": \"bpi_2015_1\",\n",
    "#         \"filepath\": \"BPIC15_1.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2020 = {\n",
    "#         \"dataset_name\": \"bpi_2020\",\n",
    "#         \"filepath\": \"InternationalDeclarations.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# # suspended:\n",
    "# # args_bpi_2012\n",
    "# # [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "\n",
    "# processing_queue = [args_helpdesk, args_sepsis]\n",
    "# for dataset in processing_queue:\n",
    "#     run(dataset)\n",
    "\n",
    "# # run(args_sepsis)\n",
    "# # preprocess(args_bpi_2020)\n",
    "# # save_csv(args_bpi_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: 'helpdesk'\n",
      "filepath: 'helpdesk.csv'\n",
      "columns: '['Case ID', 'Activity', 'Complete Timestamp']'\n",
      "additional_columns: '{}'\n",
      "datetime_format: '%Y-%m-%d %H:%M:%S.%f'\n",
      "Model Epochs: '10'\n",
      "Number of Transformer Layers in Model: '1'\n",
      "Target columns: '{'concept_name': <Target.NEXT_FEATURE: 'next_feature'>}'\n",
      "Input columns: '['concept_name']'\n",
      "\n",
      "All processed files for current spec found. Preprocessing skipped.\n",
      "Loading data from preprocessed train-test split...\n",
      "['concept_name']\n",
      "Preparing data...\n",
      "Using regular train-validation split\n",
      "Creating model...\n",
      "Masking active.\n",
      "Using Single-Task Learning Setup\n",
      "Loading model weights from: datasets\\helpdesk\\train_savepoints\\best_model.h5\n",
      "INFO:tensorflow:Error reported to Coordinator: Layer \"next_categorical_transformer\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'cond/Identity:0' shape=(None, 14) dtype=float32>, <tf.Tensor 'cond/Identity_1:0' shape=(None,) dtype=float32>]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vince\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\vince\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py\", line 386, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"C:\\Users\\vince\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 689, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "  File \"C:\\Users\\vince\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 377, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "  File \"C:\\Users\\vince\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 458, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step\n",
      "    outputs = model.test_step(data)\n",
      "  File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n",
      "    y_pred = self(x, training=False)\n",
      "  File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: Layer \"next_categorical_transformer\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'cond/Identity:0' shape=(None, 14) dtype=float32>, <tf.Tensor 'cond/Identity_1:0' shape=(None,) dtype=float32>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\six.py\", line 719, in reraise\n        raise value\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"next_categorical_transformer\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'cond/Identity:0' shape=(None, 14) dtype=float32>, <tf.Tensor 'cond/Identity_1:0' shape=(None,) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 115\u001b[0m\n\u001b[0;32m     92\u001b[0m args_bpi_2020 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpi_2020\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInternationalDeclarations.xes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_val\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         }\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# processing_queue = [args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# for dataset in processing_queue:\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m#     run(dataset)\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_helpdesk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m case_ids\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase_ids contains NaN or None values!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m models, histories \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcase_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcase_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_type_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_type_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_token_dict_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_token_dict_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_token_dict_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_token_dict_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mword_dicts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mword_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_case_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_case_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_scaler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m     57\u001b[0m results, preds \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mevaluate(models\u001b[38;5;241m=\u001b[39mmodels, data_loader\u001b[38;5;241m=\u001b[39mdata_loader, test_dfs\u001b[38;5;241m=\u001b[39mtest_dfs, x_scaler\u001b[38;5;241m=\u001b[39mx_scaler,\n\u001b[0;32m     58\u001b[0m                             y_scaler\u001b[38;5;241m=\u001b[39my_scaler, max_case_length\u001b[38;5;241m=\u001b[39mmax_case_length)\n",
      "Cell \u001b[1;32mIn[2], line 251\u001b[0m, in \u001b[0;36mpipeline.train\u001b[1;34m(self, case_ids, feature_type_dict, train_token_dict_x, train_token_dict_y, word_dicts, max_case_length, y_scaler, mask)\u001b[0m\n\u001b[0;32m    235\u001b[0m model_wrapper \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mModelWrapper(\n\u001b[0;32m    236\u001b[0m                                         dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name,\n\u001b[0;32m    237\u001b[0m                                         case_ids \u001b[38;5;241m=\u001b[39m case_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                         masking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    248\u001b[0m                                         )\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m models, histories \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_token_dict_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_token_dict_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_token_dict_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_token_dict_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcross_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43my_scaler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmodel_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtarget_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[0;32m    261\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_training_loss(histories)\n",
      "File \u001b[1;32mc:\\Users\\vince\\OneDrive\\Dokumente\\Studium-Vincents-Surface\\Master\\Unterlagen\\Master Thesis\\Repository\\MasterThesis\\MultiTaskTransformer\\package\\transformer.py:512\u001b[0m, in \u001b[0;36mModelWrapper.train_model\u001b[1;34m(self, train_token_dict_x, train_token_dict_y, cross_val, y_scaler, model_epochs, batch_size, model_learning_rate, n_splits, warmup_epochs, initial_lr, target_lr)\u001b[0m\n\u001b[0;32m    509\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m prepare_tf_dataset(train_token_dict_x, train_token_dict_y, val_indices)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# Train the model without cross-validation\u001b[39;00m\n\u001b[1;32m--> 512\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_lr\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m [model]\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, [history]\n",
      "File \u001b[1;32mc:\\Users\\vince\\OneDrive\\Dokumente\\Studium-Vincents-Surface\\Master\\Unterlagen\\Master Thesis\\Repository\\MasterThesis\\MultiTaskTransformer\\package\\transformer.py:800\u001b[0m, in \u001b[0;36mModelWrapper._train_single_fold\u001b[1;34m(self, model, train_dataset, val_dataset, model_epochs, batch_size, fold, warmup_epochs, initial_lr, target_lr)\u001b[0m\n\u001b[0;32m    796\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [early_stopping_after_warmup_callback, early_stopping_callback, best_model_callback, lr_scheduler_callback,\n\u001b[0;32m    797\u001b[0m              model_savepoint_callback, epoch_savepoint_callback]\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No need to pass x and y separately\u001b[39;49;00m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Both inputs and labels are contained in the datasets\u001b[39;49;00m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Can remove this if already done during dataset preparation\u001b[39;49;00m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\vince\\OneDrive\\Dokumente\\Studium-Vincents-Surface\\Master\\Unterlagen\\Master Thesis\\Repository\\MasterThesis\\MultiTaskTransformer\\package\\transformer.py:697\u001b[0m, in \u001b[0;36mModelWrapper._train_single_fold.<locals>.BestModelCheckpoint.on_train_begin\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# Load the existing model or weights when training starts\u001b[39;00m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting_best_model_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_loaded:\n\u001b[1;32m--> 697\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_existing_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Mark that weights have been loaded\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# Continue with the parent class's on_train_begin method\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vince\\OneDrive\\Dokumente\\Studium-Vincents-Surface\\Master\\Unterlagen\\Master Thesis\\Repository\\MasterThesis\\MultiTaskTransformer\\package\\transformer.py:717\u001b[0m, in \u001b[0;36mModelWrapper._train_single_fold.<locals>.BestModelCheckpoint._load_existing_best_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_data, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;66;03m# Handle tf.data.Dataset by extracting a batch and evaluating\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     val_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_data))  \u001b[38;5;66;03m# Get one batch\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;66;03m# Handle tuple (x_val, y_val)\u001b[39;00m\n\u001b[0;32m    720\u001b[0m     x_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileswm87io1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\six.py\", line 719, in reraise\n        raise value\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"next_categorical_transformer\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'cond/Identity:0' shape=(None, 14) dtype=float32>, <tf.Tensor 'cond/Identity_1:0' shape=(None,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Syntax lookup\n",
    "# \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "# \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "# \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "\n",
    "\n",
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"model_epochs\": 10,\n",
    "        \"warmup_epochs\": 0,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"Activity\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2012 = {\n",
    "        \"dataset_name\": \"bpi_2012\",\n",
    "        \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2020 = {\n",
    "        \"dataset_name\": \"bpi_2020\",\n",
    "        \"filepath\": \"InternationalDeclarations.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "# [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "\n",
    "# processing_queue = [args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "# for dataset in processing_queue:\n",
    "#     run(dataset)\n",
    "\n",
    "run(args_helpdesk)\n",
    "# preprocess(args_bpi_2020)\n",
    "# save_csv(args_bpi_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\", \"Resource\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
