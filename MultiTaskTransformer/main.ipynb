{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from typing import List, Optional\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import pm4py\n",
    "\n",
    "from package.processtransformer import constants\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "from package.processtransformer.constants import Task, Feature_Type, Target\n",
    "\n",
    "\n",
    "# Initialize data dir, if not exists\n",
    "if not os.path.exists(\"datasets\"): \n",
    "    os.mkdir(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Next Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, dataset_name: str, filepath: str, columns: List[str], additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, task: Task, model_learning_rate: float, model_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: Dict[str, Target] ):\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.task: Task = task\n",
    "        self.model_learning_rate: float = model_learning_rate\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[str, Target] = target_columns\n",
    "        for target_col in target_columns.keys():\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[\"concept_name\"] = self.target_columns.pop(target_col)\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        \n",
    "        # self._model_id: str = (\n",
    "        #     f\"{dataset_name}\"\n",
    "        #     f\"##{'#'.join(self.columns)}\"\n",
    "        #     f\"##{'#'.join(self.additional_columns)}\"\n",
    "        #     f\"##{'#'.join(self.task.value)}\"\n",
    "        #     f\"##{self.model_learning_rate}\"\n",
    "        #     f\"##{self.model_epochs}\"\n",
    "        #     f\"##{self.model_num_layers}\")\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"task: '{self.task.value}'\\n\"\n",
    "            f\"Model learning rate: '{self.model_learning_rate}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def safe_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,  # Add all additional columns here, first all categorical, then all numerical features\n",
    "            datetime_format=self.datetime_format,\n",
    "            pool=4\n",
    "        )\n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs(task=self.task, sort_temporally=False)\n",
    "        \n",
    "        # TODO: Compute the number of unique classes in each categorical column\n",
    "        # train_df = pd.read_csv(os.path.join(\"datasets\", self.dataset_name, \"processed\", f\"{self._preprocessing_id}_train.csv\"))\n",
    "        # num_classes_list = data_processor._compute_num_classes(train_df)\n",
    "        \n",
    "        # return num_classes_list\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, input_columns=self.input_columns, target_columns=self.target_columns)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict = data_loader.load_data()\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame] ) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data for task next_categorical...\")\n",
    "        # Prepare training examples for next categorical prediction task\n",
    "        # train_token_x, train_token_y, train_additional_features, num_categorical_features, num_numerical_features = data_loader.prepare_data_next_categorical(\n",
    "        #     train_df, x_word_dict, y_word_dict, max_case_length, full_df=pd.concat([train_df, test_df])\n",
    "        # )\n",
    "        for idx, (feature, df) in enumerate(dfs.items()):\n",
    "            if idx == 0:\n",
    "                token_dict_x, token_dict_y_next, token_dict_y_last, max_case_length = data_loader.prepare_data(df=df, max_case_length=True)\n",
    "                if feature in self.target_columns.keys():\n",
    "                    if self.target_columns[feature] == Target.NEXT_FEATURE:\n",
    "                        token_dict_y = token_dict_y_next\n",
    "                    elif self.target_columns[feature] == Target.LAST_FEATURE:\n",
    "                        token_dict_y = token_dict_y_last\n",
    "                    else: raise ValueError(\"Target type not defined\")\n",
    "            else:\n",
    "                x_tokens, y_next_tokens, y_last_tokens = data_loader.prepare_data(df=df)\n",
    "                token_dict_x.update(x_tokens)\n",
    "                # token_dict_y_next.update(y_next_tokens)\n",
    "                # token_dict_y_last.update(y_last_tokens)\n",
    "                if feature in self.target_columns.keys():\n",
    "                    if self.target_columns[feature] == Target.NEXT_FEATURE:\n",
    "                        token_dict_y.update(y_next_tokens)\n",
    "                    elif self.target_columns[feature] == Target.LAST_FEATURE:\n",
    "                        token_dict_y.update(y_last_tokens)\n",
    "                    else: raise ValueError(\"Target type not defined\")\n",
    "                    \n",
    "                \n",
    "        return token_dict_x, token_dict_y, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "            feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "            train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "            train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "            word_dicts: Dict[str, Dict[str, int]],\n",
    "            max_case_length: int\n",
    "            ) -> tf.keras.Model:\n",
    "        \n",
    "        batch_size = 12\n",
    "    \n",
    "        # Define and compile the model\n",
    "        model = transformer.get_model(\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            word_dicts=word_dicts,\n",
    "            max_case_length=max_case_length,\n",
    "            feature_type_dict=feature_type_dict\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(self.model_learning_rate),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "            \n",
    "        # Train the model\n",
    "        history = model.fit({f\"input_{key}\": value for key, value in train_token_dict_x.items() if key in self.input_columns},\n",
    "                            {f\"output_{key}\": value for key, value in train_token_dict_y.items() if key in self.target_columns.keys()},\n",
    "                            epochs=self.model_epochs, batch_size=batch_size)\n",
    "            \n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(history)\n",
    "        return model\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self, model, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame], max_case_length: int):\n",
    "        print(\"Evaluating...\")\n",
    "        \n",
    "        # Prepare lists to store evaluation metrics\n",
    "        k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "        for target_col in self.target_columns.keys():\n",
    "            k.update({target_col: []})\n",
    "            accuracies.update({target_col: []})\n",
    "            fscores.update({target_col: []})\n",
    "            precisions.update({target_col: []})\n",
    "            recalls.update({target_col: []})\n",
    "            weights.update({target_col: []})\n",
    "\n",
    "        # Calculate total number of samples\n",
    "        # total_samples = len(test_df)\n",
    "        total_samples = len( list(test_dfs.values())[0] )\n",
    "\n",
    "        # Iterate over all prefixes (k)\n",
    "        for i in range(1, max_case_length+1):\n",
    "            print( \"Prefix length: \" + str(i) )\n",
    "            test_data_subsets = {}\n",
    "            for key, df in test_dfs.items():\n",
    "                filtered_df = df[df[\"Prefix Length\"] == i]\n",
    "                test_data_subsets.update({key: filtered_df})\n",
    "                \n",
    "            # test_data_subset = test_df[test_df[\"concept_name_prefix-length\"] == i]\n",
    "            \n",
    "            # if len(test_data_subset) > 0:\n",
    "            if len( test_data_subsets[self.input_columns[0]] ) > 0:\n",
    "                # Calculate weight for this prefix\n",
    "                # weight = len(test_data_subset) / total_samples\n",
    "                \n",
    "                # Prepare the test data\n",
    "                # test_token_x, test_token_y, test_additional_features, _, _ = data_loader.prepare_data_next_categorical(\n",
    "                #     test_data_subset, x_word_dict, y_word_dict, max_case_length, full_df=pd.concat([train_df, test_df])\n",
    "                # )\n",
    "                # Prepare the test data\n",
    "                # x_token_dict, y_token_dict = data_loader.prepare_data(test_data_subset)\n",
    "                \n",
    "                # Prepare the test data\n",
    "                for idx, test_data_subset in enumerate(test_data_subsets.values()):\n",
    "                    if idx == 0:\n",
    "                        x_token_dict, y_next_token_dict, y_last_token_dict = data_loader.prepare_data(df=test_data_subset)\n",
    "                    if feature in self.target_columns.keys():\n",
    "                        if self.target_columns[key] == Target.NEXT_FEATURE:\n",
    "                            token_dict_y = y_next_token_dict\n",
    "                        elif self.target_columns[key] == Target.LAST_FEATURE:\n",
    "                            token_dict_y = y_last_token_dict\n",
    "                        else: raise ValueError(\"Target type not defined\")\n",
    "                    else:\n",
    "                        x_tokens, y_tokens = data_loader.prepare_data(df=test_data_subset)\n",
    "                        x_token_dict.update(x_tokens)\n",
    "                        # y_token_dict.update(y_tokens)\n",
    "                        \n",
    "                        if feature in self.target_columns.keys():\n",
    "                            if self.target_columns[key] == Target.NEXT_FEATURE:\n",
    "                                y_token_dict.update(y_next_token_dict)\n",
    "                            elif self.target_columns[key] == Target.LAST_FEATURE:\n",
    "                                y_token_dict.update(y_last_token_dict)\n",
    "                            else: raise ValueError(\"Target type not defined\")\n",
    "                \n",
    "                \n",
    "                # Filter x_token_dict and y_token_dict for input and target columns\n",
    "                x_token_dict = {f\"input_{key}\": value for key, value in x_token_dict.items() if key in self.input_columns}\n",
    "                y_token_dict = {f\"output_{key}\": value for key, value in y_token_dict.items() if key in self.target_columns.keys()}\n",
    "                \n",
    "                # Make predictions\n",
    "                if len(self.target_columns) > 1:\n",
    "                    result_dict = dict(zip( self.target_columns.keys(), [np.argmax(pred, axis=1) for pred in model.predict(x_token_dict)] ))\n",
    "                else:\n",
    "                    result_dict = dict(zip( self.target_columns.keys(), [np.argmax(model.predict(x_token_dict), axis=1)] ))\n",
    "                # y_pred = np.argmax(model.predict(x_token_dict), axis=1)\n",
    "                \n",
    "                # Compute metrics\n",
    "                for feature, result in result_dict.items():\n",
    "                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}\"], result)\n",
    "                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_token_dict[f\"output_{feature}\"],\n",
    "                                                                                           result, average=\"weighted\", zero_division=0)\n",
    "                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "                    \n",
    "                    k[feature].append(i)\n",
    "                    accuracies[feature].append(accuracy)\n",
    "                    fscores[feature].append(fscore)\n",
    "                    precisions[feature].append(precision)\n",
    "                    recalls[feature].append(recall)\n",
    "                    weights[feature].append(weight)\n",
    "                    \n",
    "                # accuracy = metrics.accuracy_score(y_token_dict[self.target_column], y_pred)\n",
    "                # precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_token_dict[self.target_column], y_pred, average=\"weighted\", zero_division=0)\n",
    "                # Store metrics and weight\n",
    "                # k.append(i)\n",
    "                # accuracies.append(accuracy)\n",
    "                # fscores.append(fscore)\n",
    "                # precisions.append(precision)\n",
    "                # recalls.append(recall)\n",
    "                # weights.append(weight)\n",
    "\n",
    "        for target_col in self.target_columns.keys():\n",
    "            # Compute weighted mean metrics over all k\n",
    "            weighted_accuracy = np.average(accuracies[target_col], weights=weights[target_col])\n",
    "            weighted_fscore = np.average(fscores[target_col], weights=weights[target_col])\n",
    "            weighted_precision = np.average(precisions[target_col], weights=weights[target_col])\n",
    "            weighted_recall = np.average(recalls[target_col], weights=weights[target_col])\n",
    "            # Append weighted mean metrics to the lists\n",
    "            weights[target_col].append(\"\")\n",
    "            k[target_col].append(\"Weighted Mean\")\n",
    "            accuracies[target_col].append(weighted_accuracy)\n",
    "            fscores[target_col].append(weighted_fscore)\n",
    "            precisions[target_col].append(weighted_precision)\n",
    "            recalls[target_col].append(weighted_recall)\n",
    "            # Create a DataFrame to display the results\n",
    "            print(f\"Results for {target_col}\")\n",
    "            results_df = pd.DataFrame({\n",
    "                'k': k[target_col],\n",
    "                'weight': weights[target_col],\n",
    "                'accuracy': accuracies[target_col],\n",
    "                'fscore': fscores[target_col],\n",
    "                'precision': precisions[target_col],\n",
    "                'recall': recalls[target_col]\n",
    "            })\n",
    "            # Display the results\n",
    "            print(results_df)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # weighted_accuracy = np.average(accuracies, weights=weights)\n",
    "        # weighted_fscore = np.average(fscores, weights=weights)\n",
    "        # weighted_precision = np.average(precisions, weights=weights)\n",
    "        # weighted_recall = np.average(recalls, weights=weights)\n",
    "\n",
    "        # Append weighted mean metrics to the lists\n",
    "        # weights.append(\"\")\n",
    "        # k.append(\"Weighted Mean\")\n",
    "        # accuracies.append(weighted_accuracy)\n",
    "        # fscores.append(weighted_fscore)\n",
    "        # precisions.append(weighted_precision)\n",
    "        # recalls.append(weighted_recall)\n",
    "\n",
    "        # # Create a DataFrame to display the results\n",
    "        # results_df = pd.DataFrame({\n",
    "        #     'k': k,\n",
    "        #     'weight': weights,\n",
    "        #     'accuracy': accuracies,\n",
    "        #     'fscore': fscores,\n",
    "        #     'precision': precisions,\n",
    "        #     'recall': recalls\n",
    "        # })\n",
    "\n",
    "        # # Display the results\n",
    "        # print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, dataset_name: str, filepath: str, columns: List[str], additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, task: Task, model_learning_rate: float, model_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: List[str] ):\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.task: Task = task\n",
    "        self.model_learning_rate: float = model_learning_rate\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[str, Target] = target_columns\n",
    "        for idx, target_col in enumerate(target_columns):\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        \n",
    "        # self._model_id: str = (\n",
    "        #     f\"{dataset_name}\"\n",
    "        #     f\"##{'#'.join(self.columns)}\"\n",
    "        #     f\"##{'#'.join(self.additional_columns)}\"\n",
    "        #     f\"##{'#'.join(self.task.value)}\"\n",
    "        #     f\"##{self.model_learning_rate}\"\n",
    "        #     f\"##{self.model_epochs}\"\n",
    "        #     f\"##{self.model_num_layers}\")\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"task: '{self.task.value}'\\n\"\n",
    "            f\"Model learning rate: '{self.model_learning_rate}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def safe_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,  # Add all additional columns here, first all categorical, then all numerical features\n",
    "            datetime_format=self.datetime_format,\n",
    "            pool=4\n",
    "        )\n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs(task=self.task, sort_temporally=False)\n",
    "        \n",
    "        # TODO: Compute the number of unique classes in each categorical column\n",
    "        # train_df = pd.read_csv(os.path.join(\"datasets\", self.dataset_name, \"processed\", f\"{self._preprocessing_id}_train.csv\"))\n",
    "        # num_classes_list = data_processor._compute_num_classes(train_df)\n",
    "        \n",
    "        # return num_classes_list\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, input_columns=self.input_columns, target_columns=self.target_columns)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict = data_loader.load_data()\n",
    "        # num_output = len(y_word_dict[self.target_column])\n",
    "        num_output = 0\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame] ) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data for task next_categorical...\")\n",
    "        # Prepare training examples for next categorical prediction task\n",
    "        # train_token_x, train_token_y, train_additional_features, num_categorical_features, num_numerical_features = data_loader.prepare_data_next_categorical(\n",
    "        #     train_df, x_word_dict, y_word_dict, max_case_length, full_df=pd.concat([train_df, test_df])\n",
    "        # )\n",
    "        for idx, df in enumerate(dfs.values()):\n",
    "            if idx == 0:\n",
    "                token_dict_x, token_dict_y, max_case_length = data_loader.prepare_data(df=df, max_case_length=True)\n",
    "            else:\n",
    "                x_tokens, y_tokens = data_loader.prepare_data(df=df)\n",
    "                token_dict_x.update(x_tokens)\n",
    "                token_dict_y.update(y_tokens)\n",
    "        return token_dict_x, token_dict_y, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "            feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "            train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "            train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "            word_dicts: Dict[str, Dict[str, int]],\n",
    "            max_case_length: int\n",
    "            ) -> tf.keras.Model:\n",
    "        \n",
    "        batch_size = 12\n",
    "    \n",
    "        # Define and compile the model\n",
    "        model = transformer.get_model(\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            word_dicts=word_dicts,\n",
    "            max_case_length=max_case_length,\n",
    "            feature_type_dict=feature_type_dict\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(self.model_learning_rate),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "            \n",
    "        # Train the model\n",
    "        history = model.fit({f\"input_{key}\": value for key, value in train_token_dict_x.items() if key in self.input_columns},\n",
    "                            {f\"output_{key}\": value for key, value in train_token_dict_y.items() if key in self.target_columns},\n",
    "                            epochs=self.model_epochs, batch_size=batch_size)\n",
    "            \n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(history)\n",
    "        return model\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self, model, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame], max_case_length: int):\n",
    "        print(\"Evaluating...\")\n",
    "        \n",
    "        # Prepare lists to store evaluation metrics\n",
    "        k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "        for target_col in self.target_columns:\n",
    "            k.update({target_col: []})\n",
    "            accuracies.update({target_col: []})\n",
    "            fscores.update({target_col: []})\n",
    "            precisions.update({target_col: []})\n",
    "            recalls.update({target_col: []})\n",
    "            weights.update({target_col: []})\n",
    "\n",
    "        # Calculate total number of samples\n",
    "        # total_samples = len(test_df)\n",
    "        total_samples = len( list(test_dfs.values())[0] )\n",
    "\n",
    "        # Iterate over all prefixes (k)\n",
    "        for i in range(1, max_case_length+1):\n",
    "            print( \"Prefix length: \" + str(i) )\n",
    "            test_data_subsets = {}\n",
    "            for key, df in test_dfs.items():\n",
    "                filtered_df = df[df[\"Prefix Length\"] == i]\n",
    "                test_data_subsets.update({key: filtered_df})\n",
    "                \n",
    "            # test_data_subset = test_df[test_df[\"concept_name_prefix-length\"] == i]\n",
    "            \n",
    "            # if len(test_data_subset) > 0:\n",
    "            if len( test_data_subsets[self.input_columns[0]] ) > 0:\n",
    "                # Calculate weight for this prefix\n",
    "                # weight = len(test_data_subset) / total_samples\n",
    "                \n",
    "                # Prepare the test data\n",
    "                # test_token_x, test_token_y, test_additional_features, _, _ = data_loader.prepare_data_next_categorical(\n",
    "                #     test_data_subset, x_word_dict, y_word_dict, max_case_length, full_df=pd.concat([train_df, test_df])\n",
    "                # )\n",
    "                # Prepare the test data\n",
    "                # x_token_dict, y_token_dict = data_loader.prepare_data(test_data_subset)\n",
    "                \n",
    "                # Prepare the test data\n",
    "                for idx, test_data_subset in enumerate(test_data_subsets.values()):\n",
    "                    if idx == 0:\n",
    "                        x_token_dict, y_token_dict = data_loader.prepare_data(df=test_data_subset)\n",
    "                    else:\n",
    "                        x_tokens, y_tokens = data_loader.prepare_data(df=test_data_subset)\n",
    "                        x_token_dict.update(x_tokens)\n",
    "                        y_token_dict.update(y_tokens)\n",
    "                \n",
    "                \n",
    "                # Filter x_token_dict and y_token_dict for input and target columns\n",
    "                x_token_dict = {f\"input_{key}\": value for key, value in x_token_dict.items() if key in self.input_columns}\n",
    "                y_token_dict = {f\"output_{key}\": value for key, value in y_token_dict.items() if key in self.target_columns.keys()}\n",
    "                \n",
    "                # Make predictions\n",
    "                if len(self.target_columns) > 1:\n",
    "                    result_dict = dict(zip( self.target_columns.keys(), [np.argmax(pred, axis=1) for pred in model.predict(x_token_dict)] ))\n",
    "                else:\n",
    "                    result_dict = dict(zip( self.target_columns.keys(), [np.argmax(model.predict(x_token_dict), axis=1)] ))\n",
    "                # y_pred = np.argmax(model.predict(x_token_dict), axis=1)\n",
    "                \n",
    "                # Compute metrics\n",
    "                for feature, result in result_dict.items():\n",
    "                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}\"], result)\n",
    "                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_token_dict[f\"output_{feature}\"],\n",
    "                                                                                           result, average=\"weighted\", zero_division=0)\n",
    "                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "                    \n",
    "                    k[feature].append(i)\n",
    "                    accuracies[feature].append(accuracy)\n",
    "                    fscores[feature].append(fscore)\n",
    "                    precisions[feature].append(precision)\n",
    "                    recalls[feature].append(recall)\n",
    "                    weights[feature].append(weight)\n",
    "                    \n",
    "                # accuracy = metrics.accuracy_score(y_token_dict[self.target_column], y_pred)\n",
    "                # precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_token_dict[self.target_column], y_pred, average=\"weighted\", zero_division=0)\n",
    "                # Store metrics and weight\n",
    "                # k.append(i)\n",
    "                # accuracies.append(accuracy)\n",
    "                # fscores.append(fscore)\n",
    "                # precisions.append(precision)\n",
    "                # recalls.append(recall)\n",
    "                # weights.append(weight)\n",
    "\n",
    "        for target_col in self.target_columns.keys():\n",
    "            # Compute weighted mean metrics over all k\n",
    "            weighted_accuracy = np.average(accuracies[target_col], weights=weights[target_col])\n",
    "            weighted_fscore = np.average(fscores[target_col], weights=weights[target_col])\n",
    "            weighted_precision = np.average(precisions[target_col], weights=weights[target_col])\n",
    "            weighted_recall = np.average(recalls[target_col], weights=weights[target_col])\n",
    "            # Append weighted mean metrics to the lists\n",
    "            weights[target_col].append(\"\")\n",
    "            k[target_col].append(\"Weighted Mean\")\n",
    "            accuracies[target_col].append(weighted_accuracy)\n",
    "            fscores[target_col].append(weighted_fscore)\n",
    "            precisions[target_col].append(weighted_precision)\n",
    "            recalls[target_col].append(weighted_recall)\n",
    "            # Create a DataFrame to display the results\n",
    "            print(f\"Results for {target_col}\")\n",
    "            results_df = pd.DataFrame({\n",
    "                'k': k[target_col],\n",
    "                'weight': weights[target_col],\n",
    "                'accuracy': accuracies[target_col],\n",
    "                'fscore': fscores[target_col],\n",
    "                'precision': precisions[target_col],\n",
    "                'recall': recalls[target_col]\n",
    "            })\n",
    "            # Display the results\n",
    "            print(results_df)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # weighted_accuracy = np.average(accuracies, weights=weights)\n",
    "        # weighted_fscore = np.average(fscores, weights=weights)\n",
    "        # weighted_precision = np.average(precisions, weights=weights)\n",
    "        # weighted_recall = np.average(recalls, weights=weights)\n",
    "\n",
    "        # Append weighted mean metrics to the lists\n",
    "        # weights.append(\"\")\n",
    "        # k.append(\"Weighted Mean\")\n",
    "        # accuracies.append(weighted_accuracy)\n",
    "        # fscores.append(weighted_fscore)\n",
    "        # precisions.append(weighted_precision)\n",
    "        # recalls.append(weighted_recall)\n",
    "\n",
    "        # # Create a DataFrame to display the results\n",
    "        # results_df = pd.DataFrame({\n",
    "        #     'k': k,\n",
    "        #     'weight': weights,\n",
    "        #     'accuracy': accuracies,\n",
    "        #     'fscore': fscores,\n",
    "        #     'precision': precisions,\n",
    "        #     'recall': recalls\n",
    "        # })\n",
    "\n",
    "        # # Display the results\n",
    "        # print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "# helper function: do only preprocessing on data\n",
    "def preprocess(additional_columns, input_columns, target_columns):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(\n",
    "        dataset_name = \"helpdesk\",\n",
    "        filepath = \"helpdesk.csv\",\n",
    "        columns = [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        additional_columns = additional_columns,\n",
    "        datetime_format = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        task = constants.Task.NEXT_CATEGORICAL,\n",
    "        model_learning_rate = 0.001,\n",
    "        model_epochs = 1,\n",
    "        model_num_layers = 1,\n",
    "        target_columns=target_columns,\n",
    "        input_columns=input_columns)  # Examples: \"concept_name\", \"Resource\"\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "\n",
    "# helper function\n",
    "def run(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    train_token_dict_x, train_token_dict_y, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # train the model\n",
    "    model = pipe.train(\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length\n",
    "                )\n",
    "\n",
    "    # evaluate the model\n",
    "    pipe.evaluate(model, data_loader, test_dfs, max_case_length)\n",
    "    print(\"\")\n",
    "    print(\"======================================\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    \n",
    "# function for testing out code\n",
    "def test(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    \n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "    \n",
    "    # safe xes as csv\n",
    "    # pipe.safe_as_csv()\n",
    "    \n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "    \n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "    \n",
    "    # prepare data\n",
    "    train_token_dict_x, train_token_dict_y, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "    \n",
    "    # train the model\n",
    "    model = pipe.train(\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: 'helpdesk'\n",
      "filepath: 'helpdesk.csv'\n",
      "columns: '['Case ID', 'Activity', 'Complete Timestamp']'\n",
      "additional_columns: '{<Feature_Type.CATEGORICAL: 'categorical'>: ['Resource']}'\n",
      "datetime_format: '%Y-%m-%d %H:%M:%S.%f'\n",
      "task: 'next_categorical'\n",
      "Model learning rate: '0.001'\n",
      "Model Epochs: '1'\n",
      "Number of Transformer Layers in Model: '1'\n",
      "Target columns: '{'concept_name': <Target.NEXT_FEATURE: 'next_feature'>}'\n",
      "Input columns: '['concept_name']'\n",
      "\n",
      "All processed files for current spec found. Preprocessing skipped.\n",
      "Loading data from preprocessed train-test split...\n",
      "['concept_name']\n",
      "Preparing data for task next_categorical...\n",
      "Creating model for task next_categorical...\n",
      "1121/1121 [==============================] - 87s 48ms/step - loss: 0.7606 - sparse_categorical_accuracy: 0.7527\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Prefix length: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m\n\u001b[0;32m     29\u001b[0m args_bpi_2013 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpi_2013\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBPI_Challenge_2013_incidents.xes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcept_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     41\u001b[0m         }\n\u001b[0;32m     43\u001b[0m args_bpi_2015_1 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpi_2015_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBPIC15_1.xes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcept_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     55\u001b[0m         }\n\u001b[1;32m---> 57\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_helpdesk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     41\u001b[0m             feature_type_dict \u001b[38;5;241m=\u001b[39m feature_type_dict,\n\u001b[0;32m     42\u001b[0m             train_token_dict_x \u001b[38;5;241m=\u001b[39m train_token_dict_x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m             max_case_length \u001b[38;5;241m=\u001b[39m max_case_length\n\u001b[0;32m     46\u001b[0m             )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_case_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 218\u001b[0m, in \u001b[0;36mpipeline.evaluate\u001b[1;34m(self, model, data_loader, test_dfs, max_case_length)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, test_data_subset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_data_subsets\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 218\u001b[0m         x_token_dict, y_token_dict \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mprepare_data(df\u001b[38;5;241m=\u001b[39mtest_data_subset)\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m         x_tokens, y_tokens \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mprepare_data(df\u001b[38;5;241m=\u001b[39mtest_data_subset)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"task\": constants.Task.NEXT_CATEGORICAL,\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 1,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept_name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept_name\"]\n",
    "        }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"task\": constants.Task.NEXT_CATEGORICAL,\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": [\"concept_name\"],\n",
    "        \"input_columns\": [\"concept_name\"]\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "        \"task\": constants.Task.NEXT_CATEGORICAL,\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": [\"concept_name\"],\n",
    "        \"input_columns\": [\"concept_name\"]\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "        \"task\": constants.Task.NEXT_CATEGORICAL,\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": [\"concept_name\"],\n",
    "        \"input_columns\": [\"concept_name\"]\n",
    "        }\n",
    "\n",
    "run(args_helpdesk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\", \"Resource\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring all Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and load data for the next activity task\n",
    "dataset_name = \"helpdesk\"\n",
    "data_loader, train_df, test_df, x_word_dict, y_word_dict, max_case_length, vocab_size, num_output, num_classes_list = process_and_load_data(\n",
    "    dataset_name = dataset_name,\n",
    "    filepath = \"helpdesk.csv\",\n",
    "    columns = [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "    additional_columns = [\"Resource\"],\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "    task = constants.Task.NEXT_ACTIVITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing\n",
    "dataset_name = \"sepsis\"\n",
    "data_processor = LogsDataProcessor(\n",
    "    name=dataset_name,\n",
    "    filepath=\"sepsis.xes\",\n",
    "    columns=[\"case:concept:name\", \"concept:name\", \"time:timestamp\"],  # specify the columns name containing case_id, activity name and timestamp\n",
    "    additional_columns=[\"org:group\"],\n",
    "    datetime_format=\"%Y-%m-%d %H:%M:%S%z\",\n",
    "    pool=4\n",
    ")\n",
    "data_processor.process_logs(task=constants.Task.NEXT_ACTIVITY, sort_temporally=False)\n",
    "\n",
    "# Garbage collection\n",
    "del data_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_token_y, train_additional_features, num_categorical_features, num_numerical_features = data_loader.prepare_data_next_activity(\n",
    "    train_df, x_word_dict, y_word_dict, max_case_length, full_df=pd.concat([train_df, test_df])\n",
    ")\n",
    "\n",
    "# Garbage collection\n",
    "del data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 3\n",
    "\n",
    "# Define and compile the model\n",
    "model = transformer.get_next_activity_model(\n",
    "    max_case_length=max_case_length,\n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output,\n",
    "    num_categorical_features=num_categorical_features,\n",
    "    num_numerical_features=num_numerical_features,\n",
    "    num_classes_list=num_classes_list,  # Pass the computed number of classes list\n",
    "    num_layers=1\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "if train_additional_features.shape[1] == 0:\n",
    "    model.fit([train_token_x], train_token_y, epochs=epochs, batch_size=batch_size)\n",
    "else:\n",
    "    model.fit([train_token_x, train_additional_features], train_token_y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare lists to store evaluation metrics\n",
    "k, accuracies, fscores, precisions, recalls, weights = [], [], [], [], [], []\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_samples = len(test_df)\n",
    "\n",
    "# Iterate over all prefixes (k)\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        # Calculate weight for this prefix\n",
    "        weight = len(test_data_subset) / total_samples\n",
    "        \n",
    "        # Prepare the test data\n",
    "        test_token_x, test_token_y, test_additional_features, _, _ = data_loader.prepare_data_next_activity(\n",
    "            test_data_subset, x_word_dict, y_word_dict, max_case_length, full_df=pd.concat([train_df, test_df])\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        if test_additional_features.shape[1] != 0:\n",
    "            y_pred = np.argmax(model.predict([test_token_x, test_additional_features]), axis=1)\n",
    "        else:\n",
    "            y_pred = np.argmax(model.predict([test_token_x]), axis=1)\n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(test_token_y, y_pred, average=\"weighted\")\n",
    "        \n",
    "        # Store metrics and weight\n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        weights.append(weight)\n",
    "\n",
    "# Compute weighted mean metrics over all k\n",
    "weighted_accuracy = np.average(accuracies, weights=weights)\n",
    "weighted_fscore = np.average(fscores, weights=weights)\n",
    "weighted_precision = np.average(precisions, weights=weights)\n",
    "weighted_recall = np.average(recalls, weights=weights)\n",
    "\n",
    "# Append weighted mean metrics to the lists\n",
    "k.append(max_case_length)\n",
    "accuracies.append(weighted_accuracy)\n",
    "fscores.append(weighted_fscore)\n",
    "precisions.append(weighted_precision)\n",
    "recalls.append(weighted_recall)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'k': k,\n",
    "    'accuracy': accuracies,\n",
    "    'fscore': fscores,\n",
    "    'precision': precisions,\n",
    "    'recall': recalls\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "print('Average precision across all prefixes:', np.mean(precisions))\n",
    "print('Average recall across all prefixes:', np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Next Time  -- Ignored for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and load data for the next activity task\n",
    "dataset_name = \"helpdesk\"\n",
    "data_loader, train_df, test_df, x_word_dict, y_word_dict, max_case_length, vocab_size, num_output, num_classes_list = process_and_load_data(\n",
    "    dataset_name = dataset_name,\n",
    "    filepath = \"helpdesk.csv\",\n",
    "    columns = [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "    additional_columns = [\"Resource\", \"product\"],\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "    task = constants.Task.NEXT_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training examples for next time prediction task\n",
    "train_token_x, train_time_x, train_token_y, train_additional_features, time_scaler, y_scaler, num_categorical_features, num_numerical_features = data_loader.prepare_data_next_time(\n",
    "    train_df, x_word_dict, max_case_length, shuffle=True)\n",
    "\n",
    "# Garbage collection\n",
    "del data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Remaining Time -- Ignored for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpdesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and load data for the next time task\n",
    "dataset_name = \"sepsis\"\n",
    "data_loader, train_df, test_df, x_word_dict, y_word_dict, max_case_length, vocab_size, num_output, num_classes_list = process_and_load_data(\n",
    "    dataset_name, \"sepsis.xes\", [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], [\"org:group\"], \"%Y-%m-%d %H:%M:%S%z\", constants.Task.NEXT_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
