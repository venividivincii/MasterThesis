{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from typing import List, Optional\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import pm4py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from package.processtransformer.models import transformer\n",
    "from package.processtransformer.data.loader import LogsDataLoader\n",
    "from package.processtransformer.data.processor import LogsDataProcessor\n",
    "from package.processtransformer.constants import Feature_Type, Target, Temporal_Feature\n",
    "\n",
    "\n",
    "# Initialize data dir, if not exists\n",
    "if not os.path.exists(\"datasets\"): \n",
    "    os.mkdir(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, dataset_name: str, filepath: str, columns: List[str], additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, model_learning_rate: float, model_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: Dict[str, Target], temporal_features: Dict[Temporal_Feature, bool] ):\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.model_learning_rate: float = model_learning_rate\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[str, Target] = target_columns\n",
    "        for target_col in target_columns.keys():\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[\"concept_name\"] = self.target_columns.pop(target_col)\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        self.temporal_features: Dict[Temporal_Feature, bool] = temporal_features\n",
    "        \n",
    "        # self._model_id: str = (\n",
    "        #     f\"{dataset_name}\"\n",
    "        #     f\"##{'#'.join(self.columns)}\"\n",
    "        #     f\"##{'#'.join(self.additional_columns)}\"\n",
    "        #     f\"##{'#'.join(self.task.value)}\"\n",
    "        #     f\"##{self.model_learning_rate}\"\n",
    "        #     f\"##{self.model_epochs}\"\n",
    "        #     f\"##{self.model_num_layers}\")\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"Model learning rate: '{self.model_learning_rate}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def save_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,  # Add all additional columns here, first all categorical, then all numerical features\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            datetime_format=self.datetime_format,\n",
    "            temporal_features=self.temporal_features,\n",
    "            pool=4\n",
    "        )\n",
    "        \n",
    "        # TODO: sanitize columns\n",
    "        # self.columns = [data_processor.sanitize_filename(col) for col in self.columns]\n",
    "        \n",
    "        self.additional_columns = {\n",
    "                                feature_type: [data_processor.sanitize_filename(feature) for feature in feature_lst] for feature_type,\n",
    "                                feature_lst in self.additional_columns.items()\n",
    "                                } if len(self.additional_columns)>0 else self.additional_columns\n",
    "        self.target_columns = {data_processor.sanitize_filename(feature, self.columns): target for feature, target in self.target_columns.items()}\n",
    "        self.input_columns = [data_processor.sanitize_filename(col, self.columns) for col in self.input_columns]\n",
    "        self.columns = [data_processor.sanitize_filename(col, self.columns) for col in self.columns]\n",
    "        \n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs()\n",
    "        \n",
    "        # TODO: Compute the number of unique classes in each categorical column\n",
    "        # train_df = pd.read_csv(os.path.join(\"datasets\", self.dataset_name, \"processed\", f\"{self._preprocessing_id}_train.csv\"))\n",
    "        # num_classes_list = data_processor._compute_num_classes(train_df)\n",
    "        \n",
    "        # return num_classes_list\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, input_columns=self.input_columns,\n",
    "                                     target_columns=self.target_columns, temporal_features=self.temporal_features)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict = data_loader.load_data()\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame] ) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        # initialize token dicts\n",
    "        x_token_dict, y_token_dict = {}, {}\n",
    "        \n",
    "        # loop over all feature dfs\n",
    "        for idx, (feature, df) in enumerate(dfs.items()):\n",
    "            \n",
    "            if idx == 0:\n",
    "                x_tokens, y_next_tokens, y_last_tokens, max_case_length = data_loader.prepare_data(df=df, max_case_length=True, shuffle=True)\n",
    "            else:\n",
    "                x_tokens, y_next_tokens, y_last_tokens = data_loader.prepare_data(df=df, shuffle=True)\n",
    "            \n",
    "            # update x_token_dict\n",
    "            x_token_dict.update(x_tokens)\n",
    "                \n",
    "            # if feature is target column\n",
    "            if feature in self.target_columns.keys():\n",
    "                # update y_token_dict with next_feature as target\n",
    "                if self.target_columns[feature] == Target.NEXT_FEATURE:\n",
    "                    y_token_dict.update(y_next_tokens)\n",
    "                # update y_token_dict with last_feature as target\n",
    "                elif self.target_columns[feature] == Target.LAST_FEATURE:\n",
    "                    y_token_dict.update(y_last_tokens)\n",
    "                else: raise ValueError(\"Target type not defined\")\n",
    "\n",
    "        return x_token_dict, y_token_dict, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "                feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "                train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "                train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "                word_dicts: Dict[str, Dict[str, int]],\n",
    "                max_case_length: int,\n",
    "                validation_split: float = 0.2  # Fraction of the training data to be used for validation\n",
    "                ) -> tf.keras.Model:\n",
    "        \n",
    "        batch_size = 12\n",
    "    \n",
    "        # Define and compile the model\n",
    "        model = transformer.get_model(\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            word_dicts=word_dicts,\n",
    "            max_case_length=max_case_length,\n",
    "            feature_type_dict=feature_type_dict\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(self.model_learning_rate),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        ######################   Train-Validation Split    ######################\n",
    "        # Prepare the data for validation\n",
    "        first_key = next(iter(train_token_dict_x.keys()))\n",
    "        n_samples = train_token_dict_x[first_key].shape[0]\n",
    "        \n",
    "        # Generate a train-validation split index\n",
    "        indices = np.arange(n_samples)\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=validation_split, random_state=42)\n",
    "        \n",
    "        # Flatten the dictionary of inputs and targets for the train_test_split\n",
    "        train_token_dict_x_split = {}\n",
    "        val_token_dict_x_split = {}\n",
    "        train_token_dict_y_split = {}\n",
    "        val_token_dict_y_split = {}\n",
    "        \n",
    "        # Apply the split indices to all keys in train_token_dict_x\n",
    "        for key, x_data in train_token_dict_x.items():\n",
    "            train_token_dict_x_split[key] = x_data[train_indices]\n",
    "            val_token_dict_x_split[key] = x_data[val_indices]\n",
    "        \n",
    "        # Apply the split indices to all keys in train_token_dict_y\n",
    "        for key, y_data in train_token_dict_y.items():\n",
    "            train_token_dict_y_split[key] = y_data[train_indices]\n",
    "            val_token_dict_y_split[key] = y_data[val_indices]\n",
    "        \n",
    "        # EarlyStopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',  # You can also monitor 'val_sparse_categorical_accuracy'\n",
    "            patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "            restore_best_weights=True,  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "            min_delta=0.001\n",
    "        )\n",
    "        ############################################\n",
    "        \n",
    "        # make model_specs_dir, if not exists\n",
    "        model_specs_dir = os.path.join( \"datasets\", self.dataset_name, \"model_specs\" )\n",
    "        os.makedirs(model_specs_dir, exist_ok=True)\n",
    "        \n",
    "        # EarlyStopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',  # You can also monitor 'val_sparse_categorical_accuracy'\n",
    "            patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "            restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "        )\n",
    "        \n",
    "        # save model weights for highes accuracy\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join( model_specs_dir, \"best_model.h5\" ),\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_sparse_categorical_accuracy\",\n",
    "        mode=\"max\", save_best_only=True)\n",
    "        \n",
    "            \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "                            {f\"input_{key}\": value for key, value in train_token_dict_x_split.items() if key in self.input_columns},\n",
    "                            {f\"output_{key}\": value for key, value in train_token_dict_y_split.items() if key in self.target_columns.keys()},\n",
    "                            validation_data=(\n",
    "                                {f\"input_{key}\": value for key, value in val_token_dict_x_split.items() if key in self.input_columns},\n",
    "                                {f\"output_{key}\": value for key, value in val_token_dict_y_split.items() if key in self.target_columns.keys()}\n",
    "                            ),\n",
    "                            epochs=self.model_epochs, batch_size=batch_size, shuffle=True,\n",
    "                            callbacks=[early_stopping, model_checkpoint_callback])\n",
    "            \n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(history)\n",
    "        return model\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate(self, model, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame], max_case_length: int):\n",
    "        print(\"Evaluating...\")\n",
    "        \n",
    "        # Prepare lists to store evaluation metrics\n",
    "        k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "        for target_col in self.target_columns.keys():\n",
    "            k.update({target_col: []})\n",
    "            accuracies.update({target_col: []})\n",
    "            fscores.update({target_col: []})\n",
    "            precisions.update({target_col: []})\n",
    "            recalls.update({target_col: []})\n",
    "            weights.update({target_col: []})\n",
    "\n",
    "        # Calculate total number of samples\n",
    "        total_samples = len( list(test_dfs.values())[0] )\n",
    "\n",
    "        # Iterate over all prefixes (k)\n",
    "        for i in range(1, max_case_length+1):\n",
    "            print( \"Prefix length: \" + str(i) )\n",
    "            test_data_subsets = {}\n",
    "            \n",
    "            for key, df in test_dfs.items():\n",
    "                filtered_df = df[df[\"Prefix Length\"] == i]\n",
    "                test_data_subsets.update({key: filtered_df})\n",
    "                \n",
    "            if len( test_data_subsets[self.input_columns[0]] ) > 0:\n",
    "                \n",
    "                # initialize token dicts\n",
    "                x_token_dict, y_token_dict = {}, {}\n",
    "                \n",
    "                # Prepare the test data\n",
    "                for feature, test_data_subset in test_data_subsets.items():\n",
    "                        \n",
    "                    # prepare data of subset batch\n",
    "                    x_tokens, y_next_tokens, y_last_tokens = data_loader.prepare_data(df=test_data_subset)\n",
    "                    \n",
    "                    # update x_token_dict\n",
    "                    x_token_dict.update(x_tokens)\n",
    "                        \n",
    "                    # if feature is a target feature\n",
    "                    if feature in self.target_columns.keys():\n",
    "                        # update y_token_dict with next_feature\n",
    "                        if self.target_columns[feature] == Target.NEXT_FEATURE:\n",
    "                            y_token_dict.update(y_next_tokens)\n",
    "                        # update y_token_dict with next_feature\n",
    "                        elif self.target_columns[feature] == Target.LAST_FEATURE:\n",
    "                            y_token_dict.update(y_last_tokens)\n",
    "                        else: raise ValueError(\"Target type not defined\")\n",
    "                \n",
    "                # Filter x_token_dict and y_token_dict for input and target columns\n",
    "                x_token_dict = {f\"input_{key}\": value for key, value in x_token_dict.items() if key in self.input_columns}\n",
    "                y_token_dict = {f\"output_{key}\": value for key, value in y_token_dict.items() if key in self.target_columns.keys()}\n",
    "                \n",
    "                # Make predictions\n",
    "                if len(self.target_columns) > 1:\n",
    "                    result_dict = dict(zip( self.target_columns.keys(), [np.argmax(pred, axis=1) for pred in model.predict(x_token_dict)] ))\n",
    "                else:\n",
    "                    result_dict = dict(zip( self.target_columns.keys(), [np.argmax(model.predict(x_token_dict), axis=1)] ))\n",
    "                # y_pred = np.argmax(model.predict(x_token_dict), axis=1)\n",
    "                \n",
    "                # Compute metrics\n",
    "                for feature, result in result_dict.items():\n",
    "                    \n",
    "                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}\"], result)\n",
    "                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_token_dict[f\"output_{feature}\"],\n",
    "                                                                                           result, average=\"weighted\", zero_division=0)\n",
    "                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "                    \n",
    "                    k[feature].append(i)\n",
    "                    accuracies[feature].append(accuracy)\n",
    "                    fscores[feature].append(fscore)\n",
    "                    precisions[feature].append(precision)\n",
    "                    recalls[feature].append(recall)\n",
    "                    weights[feature].append(weight)\n",
    "\n",
    "        for target_col in self.target_columns.keys():\n",
    "            # Compute weighted mean metrics over all k\n",
    "            weighted_accuracy = np.average(accuracies[target_col], weights=weights[target_col])\n",
    "            weighted_fscore = np.average(fscores[target_col], weights=weights[target_col])\n",
    "            weighted_precision = np.average(precisions[target_col], weights=weights[target_col])\n",
    "            weighted_recall = np.average(recalls[target_col], weights=weights[target_col])\n",
    "            # Append weighted mean metrics to the lists\n",
    "            weights[target_col].append(\"\")\n",
    "            k[target_col].append(\"Weighted Mean\")\n",
    "            accuracies[target_col].append(weighted_accuracy)\n",
    "            fscores[target_col].append(weighted_fscore)\n",
    "            precisions[target_col].append(weighted_precision)\n",
    "            recalls[target_col].append(weighted_recall)\n",
    "            # Create a DataFrame to display the results\n",
    "            print(f\"Results for {target_col}\")\n",
    "            results_df = pd.DataFrame({\n",
    "                'k': k[target_col],\n",
    "                'weight': weights[target_col],\n",
    "                'accuracy': accuracies[target_col],\n",
    "                'fscore': fscores[target_col],\n",
    "                'precision': precisions[target_col],\n",
    "                'recall': recalls[target_col]\n",
    "            })\n",
    "            # Display the results\n",
    "            print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "# helper function to save xes file as csv\n",
    "def save_csv(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    pipe.save_as_csv()\n",
    "    \n",
    "\n",
    "# helper function: do only preprocessing on data\n",
    "def preprocess(additional_columns, input_columns, target_columns):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(\n",
    "        dataset_name = \"helpdesk\",\n",
    "        filepath = \"helpdesk.csv\",\n",
    "        columns = [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        additional_columns = additional_columns,\n",
    "        datetime_format = \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        model_learning_rate = 0.001,\n",
    "        model_epochs = 1,\n",
    "        model_num_layers = 1,\n",
    "        target_columns=target_columns,\n",
    "        input_columns=input_columns)  # Examples: \"concept_name\", \"Resource\"\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "\n",
    "# helper function\n",
    "def run(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    train_token_dict_x, train_token_dict_y, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # train the model\n",
    "    model = pipe.train(\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length\n",
    "                )\n",
    "\n",
    "    # evaluate the model\n",
    "    pipe.evaluate(model, data_loader, test_dfs, max_case_length)\n",
    "    print(\"\")\n",
    "    print(\"======================================\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    \n",
    "# function for testing out code\n",
    "def test(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # # load data\n",
    "    # data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "\n",
    "    # # prepare data\n",
    "    # train_token_dict_x, train_token_dict_y, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # # train the model\n",
    "    # model = pipe.train(\n",
    "    #             feature_type_dict = feature_type_dict,\n",
    "    #             train_token_dict_x = train_token_dict_x,\n",
    "    #             train_token_dict_y = train_token_dict_y,\n",
    "    #             word_dicts = word_dicts,\n",
    "    #             max_case_length = max_case_length\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: 'helpdesk'\n",
      "filepath: 'helpdesk.csv'\n",
      "columns: '['Case ID', 'Activity', 'Complete Timestamp']'\n",
      "additional_columns: '{}'\n",
      "datetime_format: '%Y-%m-%d %H:%M:%S.%f'\n",
      "Model learning rate: '0.001'\n",
      "Model Epochs: '1'\n",
      "Number of Transformer Layers in Model: '1'\n",
      "Target columns: '{'concept_name': <Target.NEXT_FEATURE: 'next_feature'>}'\n",
      "Input columns: '['concept_name', 'Complete Timestamp']'\n",
      "\n",
      "Parsing Event-Log...\n",
      "No Processed features found\n",
      "Preprocessing...\n",
      "Processing feature prefixes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting log metadata\n",
      "Processing Categorical Features...\n",
      "Coding categorical log Meta-Data...\n",
      "Processing Timestamp Features...\n",
      "Writing results in csv-files...\n"
     ]
    }
   ],
   "source": [
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 1,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"Activity\", \"Complete Timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: True, Temporal_Feature.HOUR_OF_DAY: True}\n",
    "        }\n",
    "\n",
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_learning_rate\": 0.001,\n",
    "#         \"model_epochs\": 20,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False}\n",
    "#         }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:group\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 1,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:group\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False}\n",
    "        }\n",
    "\n",
    "args_bpi_2012 = {\n",
    "        \"dataset_name\": \"bpi_2012\",\n",
    "        \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\"]\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 2,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\"]\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 2,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept_name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept_name\", \"org_resource\"]\n",
    "        }\n",
    "\n",
    "\n",
    "test(args_helpdesk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change settings and run again\n",
    "\n",
    "# args_bpi_2012[\"additional_columns\"] = {}\n",
    "# args_bpi_2012[\"input_columns\"] = [\"concept:name\"]\n",
    "# run(args_bpi_2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\", \"Resource\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
