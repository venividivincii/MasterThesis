{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from typing import List, Optional\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import pm4py\n",
    "from package import transformer\n",
    "from package.loader import LogsDataLoader\n",
    "from package.processor import LogsDataProcessor, masked_standard_scaler, masked_min_max_scaler\n",
    "from package.constants import Feature_Type, Target, Temporal_Feature, Model_Architecture\n",
    "\n",
    "\n",
    "# Initialize data dir, if not exists\n",
    "if not os.path.exists(\"datasets\"): \n",
    "    os.mkdir(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, dataset_name: str, filepath: str, sorting: bool, columns: List[str],\n",
    "                 additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, model_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: Dict[str, Target], temporal_features: Dict[Temporal_Feature, bool],\n",
    "                 cross_val: bool, model_architecture: Model_Architecture):\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.sorting: bool = sorting\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[str, Target] = target_columns\n",
    "        for target_col in target_columns.keys():\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[\"concept_name\"] = self.target_columns.pop(target_col)\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        self.temporal_features: Dict[Temporal_Feature, bool] = temporal_features\n",
    "        self.cross_val = cross_val\n",
    "        self.model_architecture = model_architecture\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def save_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            sorting=self.sorting,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,  # Add all additional columns here, first all categorical, then all numerical features\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            datetime_format=self.datetime_format,\n",
    "            temporal_features=self.temporal_features,\n",
    "            pool=4\n",
    "        )\n",
    "        \n",
    "        # TODO: sanitize columns\n",
    "        # self.columns = [data_processor.sanitize_filename(col) for col in self.columns]\n",
    "        \n",
    "        # self.additional_columns = {\n",
    "        #                         feature_type: [data_processor.sanitize_filename(feature) for feature in feature_lst] for feature_type,\n",
    "        #                         feature_lst in self.additional_columns.items()\n",
    "        #                         } if len(self.additional_columns)>0 else self.additional_columns\n",
    "        self.target_columns = {data_processor.sanitize_filename(feature, self.columns): target for feature, target in self.target_columns.items()}\n",
    "        self.input_columns = [data_processor.sanitize_filename(col, self.columns) for col in self.input_columns]\n",
    "        self.columns = [data_processor.sanitize_filename(col, self.columns) for col in self.columns]\n",
    "        \n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs()\n",
    "        # flatten self.additional_columns to get all used features\n",
    "        self.additional_columns = data_processor.additional_columns\n",
    "        self.used_features = [item for sublist in self.additional_columns.values() for item in sublist]\n",
    "        \n",
    "        \n",
    "        # TODO: Compute the number of unique classes in each categorical column\n",
    "        # train_df = pd.read_csv(os.path.join(\"datasets\", self.dataset_name, \"processed\", f\"{self._preprocessing_id}_train.csv\"))\n",
    "        # num_classes_list = data_processor._compute_num_classes(train_df)\n",
    "        \n",
    "        # return num_classes_list\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, sorting=self.sorting, input_columns=self.input_columns,\n",
    "                                     target_columns=self.target_columns, temporal_features=self.temporal_features)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict, mask = data_loader.load_data()\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame], x_scaler=None, y_scaler=None,\n",
    "                     train: bool = True) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data...\")\n",
    "        # initialize max_case_length\n",
    "        max_case_length = False\n",
    "        # initialize token dicts\n",
    "        x_token_dict, y_token_dict, x_token_dict_numerical, y_token_dict_numerical = {}, {}, {}, {}\n",
    "        \n",
    "        # initialize case_id_df\n",
    "        case_ids = next(iter(dfs.values()))[\"case_id\"]\n",
    "        \n",
    "        # loop over all feature dfs\n",
    "        for idx, (feature, feature_df) in enumerate(dfs.items()):\n",
    "            \n",
    "            # get current feature_type\n",
    "            for feature_type, feature_lst in self.additional_columns.items():\n",
    "                if feature in feature_lst: break\n",
    "            \n",
    "            if idx == 0 and train:\n",
    "                (x_tokens, y_tokens, max_case_length\n",
    "                ) = data_loader.prepare_data(feature=feature, df=feature_df, max_case_length=True)\n",
    "            else:\n",
    "                x_tokens, y_tokens = data_loader.prepare_data(feature=feature, df=feature_df)\n",
    "            \n",
    "            if feature_type is Feature_Type.TIMESTAMP or feature_type is Feature_Type.NUMERICAL:\n",
    "                x_token_dict_numerical.update(x_tokens)\n",
    "                y_token_dict_numerical.update(y_tokens)\n",
    "            else:\n",
    "                # update x_token_dict\n",
    "                x_token_dict.update(x_tokens)\n",
    "                y_token_dict.update(y_tokens)\n",
    "            \n",
    "        # TODO:\n",
    "        if len(x_token_dict_numerical) > 0  and len(list(x_token_dict_numerical.values())[0]) > 0:\n",
    "            # Concatenate all the feature arrays along the rows (axis=0)\n",
    "            combined_data = np.vstack(list(x_token_dict_numerical.values()))\n",
    "            if x_scaler is None:\n",
    "                # Initialize the StandardScaler\n",
    "                # x_scaler = StandardScaler()\n",
    "                # x_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                # x_scaler = FunctionTransformer(masked_standard_scaler, kw_args={'padding_value': -1})\n",
    "                x_scaler = FunctionTransformer(masked_min_max_scaler, kw_args={'padding_value': -1})\n",
    "                # Fit the scaler on the combined data\n",
    "                x_scaler.fit(combined_data)\n",
    "            # Transform the combined data\n",
    "            scaled_combined_data = x_scaler.transform(combined_data)\n",
    "            # split the scaled combined data back into the original feature dict\n",
    "            split_indices = np.cumsum([value.shape[0] for value in x_token_dict_numerical.values()])[:-1]\n",
    "            scaled_data_parts = np.vsplit(scaled_combined_data, split_indices)\n",
    "            # Reconstruct the dictionary with scaled data\n",
    "            scaled_dict = {key: scaled_data_parts[i] for i, key in enumerate(x_token_dict_numerical.keys())}\n",
    "            # update x_token_dict\n",
    "            x_token_dict.update(scaled_dict)\n",
    "        if len(y_token_dict_numerical) > 0:\n",
    "            # Prepare list to store valid arrays (non-empty)\n",
    "            valid_arrays = []\n",
    "            valid_keys = []\n",
    "\n",
    "            # Check for empty arrays and prepare data for scaling\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size > 0:  # Only consider non-empty arrays\n",
    "                    valid_arrays.append(value.reshape(-1, 1))  # Reshape to 2D\n",
    "                    valid_keys.append(key)\n",
    "\n",
    "            # If there are valid arrays to scale\n",
    "            if valid_arrays:\n",
    "                combined_data = np.hstack(valid_arrays)  # Horizontal stacking for features\n",
    "\n",
    "                if y_scaler is None:\n",
    "                    # Initialize the StandardScaler\n",
    "                    # y_scaler = StandardScaler()\n",
    "                    y_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                    # Fit the scaler on the combined data\n",
    "                    y_scaler.fit(combined_data)\n",
    "\n",
    "                # Transform the combined data\n",
    "                scaled_combined_data = y_scaler.transform(combined_data)\n",
    "\n",
    "                # Split the scaled combined data back into individual features\n",
    "                scaled_data_parts = np.hsplit(scaled_combined_data, scaled_combined_data.shape[1])\n",
    "\n",
    "                # Reconstruct the dictionary with scaled data\n",
    "                scaled_dict = {key: scaled_data_parts[i].flatten() for i, key in enumerate(valid_keys)}\n",
    "\n",
    "                # Update y_token_dict with the scaled data\n",
    "                y_token_dict.update(scaled_dict)\n",
    "\n",
    "            # Handle any empty arrays (if necessary)\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size == 0:\n",
    "                    # Optionally, you can handle empty arrays here, e.g., leave them as-is\n",
    "                    y_token_dict[key] = value\n",
    "            \n",
    "            \n",
    "        # sort dicts\n",
    "        x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "        y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "\n",
    "        return case_ids, x_token_dict, y_token_dict, x_scaler, y_scaler, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "            case_ids: pd.DataFrame,\n",
    "            feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "            train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "            train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "            word_dicts: Dict[str, Dict[str, int]],\n",
    "            max_case_length: int,\n",
    "            y_scaler,\n",
    "            mask # Fraction of the training data to be used for validation\n",
    "            ) -> tf.keras.Model:\n",
    "\n",
    "        # Ensure that input columns and dictionaries are sorted\n",
    "        self.input_columns.sort()\n",
    "        self.target_columns = dict(sorted(self.target_columns.items()))\n",
    "        train_token_dict_x = dict(sorted(train_token_dict_x.items()))\n",
    "        train_token_dict_y = dict(sorted(train_token_dict_y.items()))\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "\n",
    "        # initialize model_wrapper with data for model\n",
    "        model_wrapper = transformer.ModelWrapper(\n",
    "                                                dataset_name = self.dataset_name,\n",
    "                                                case_ids = case_ids,\n",
    "                                                input_columns=self.input_columns,\n",
    "                                                target_columns=self.target_columns,\n",
    "                                                additional_columns=self.additional_columns,\n",
    "                                                word_dicts=word_dicts,\n",
    "                                                max_case_length=max_case_length,\n",
    "                                                feature_type_dict=feature_type_dict,\n",
    "                                                temporal_features=self.temporal_features,\n",
    "                                                model_architecture=self.model_architecture,\n",
    "                                                sorting=self.sorting,\n",
    "                                                masking=True\n",
    "                                                )\n",
    "        \n",
    "        # train the model\n",
    "        models, histories = model_wrapper.train_model(\n",
    "                                                    train_token_dict_x = train_token_dict_x,\n",
    "                                                    train_token_dict_y = train_token_dict_y,\n",
    "                                                    cross_val = self.cross_val,\n",
    "                                                    y_scaler = y_scaler,\n",
    "                                                    model_epochs = self.model_epochs,\n",
    "                                                    batch_size = 12,\n",
    "                                                    model_learning_rate = 0.001\n",
    "                                                    )\n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(histories)\n",
    "        return models, histories\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, histories):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # If there are multiple histories (cross-validation), plot for each fold\n",
    "        if isinstance(histories, list):\n",
    "            for i, history in enumerate(histories):\n",
    "                plt.plot(history.history['loss'], label=f'Training Loss Fold {i+1}')\n",
    "                if 'val_loss' in history.history:\n",
    "                    plt.plot(history.history['val_loss'], label=f'Validation Loss Fold {i+1}')\n",
    "        else:\n",
    "            # Single history (no cross-validation)\n",
    "            plt.plot(histories.history['loss'], label='Training Loss')\n",
    "            if 'val_loss' in histories.history:\n",
    "                plt.plot(histories.history['val_loss'], label='Validation Loss')\n",
    "        \n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def evaluate(self, models, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame],\n",
    "                 max_case_length: int, x_scaler=None, y_scaler=None):\n",
    "        \n",
    "        #TODO: testing\n",
    "        # print(f\"Unscaled MAE training: {y_scaler.scale_ * 0.3030}\")\n",
    "        results, preds = [], []\n",
    "        for idx, model in enumerate(models):\n",
    "            print(f\"Evaluating model {idx+1}...\")\n",
    "\n",
    "            # Prepare lists to store evaluation metrics\n",
    "            k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "            mae, mse, rmse, r2 = {}, {}, {}, {}\n",
    "            \n",
    "            for target_col in self.target_columns.keys():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        k.update({target_col: []})\n",
    "                        weights.update({target_col: []})\n",
    "                        \n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            accuracies.update({target_col: []})\n",
    "                            fscores.update({target_col: []})\n",
    "                            precisions.update({target_col: []})\n",
    "                            recalls.update({target_col: []})\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            mae.update({target_col: []})\n",
    "                            mse.update({target_col: []})\n",
    "                            rmse.update({target_col: []})\n",
    "                            r2.update({target_col: []})\n",
    "\n",
    "            # Calculate total number of samples\n",
    "            total_samples = len(list(test_dfs.values())[0])\n",
    "\n",
    "            # Iterate over all prefixes (k)\n",
    "            for i in range(1, max_case_length + 1):\n",
    "                print(\"Prefix length: \" + str(i))\n",
    "                test_data_subsets = {}\n",
    "\n",
    "                for key, df in test_dfs.items():\n",
    "                    if (Feature_Type.TIMESTAMP in self.additional_columns\n",
    "                            and key in self.additional_columns[Feature_Type.TIMESTAMP]):\n",
    "                        prefix_str = f\"{key}##Prefix Length\"\n",
    "                    else:\n",
    "                        prefix_str = \"Prefix Length\"\n",
    "                    filtered_df = df[df[prefix_str] == i]\n",
    "                    test_data_subsets.update({key: filtered_df})\n",
    "\n",
    "\n",
    "                _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_data_subsets,\n",
    "                                                                x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "\n",
    "                # sort dicts\n",
    "                x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "                y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "\n",
    "                if len(test_data_subsets[self.input_columns[0]]) > 0:\n",
    "\n",
    "                    # Make predictions\n",
    "                    predictions = model.predict(x_token_dict)\n",
    "                    \n",
    "                    # Handle multiple outputs for multitask learning\n",
    "                    if len(self.target_columns) > 1:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "                    else:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "\n",
    "                    # Compute metrics\n",
    "                    for feature, result in result_dict.items():\n",
    "                        for feature_type, feature_lst in self.additional_columns.items():\n",
    "                            if feature in feature_lst:\n",
    "                                if feature_type is Feature_Type.CATEGORICAL:\n",
    "                                    result = np.argmax(result, axis=1)\n",
    "                                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}\"], result)\n",
    "                                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                                        y_token_dict[f\"output_{feature}\"], result, average=\"weighted\", zero_division=0)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[feature].append(i)\n",
    "                                    accuracies[feature].append(accuracy)\n",
    "                                    fscores[feature].append(fscore)\n",
    "                                    precisions[feature].append(precision)\n",
    "                                    recalls[feature].append(recall)\n",
    "                                    weights[feature].append(weight)\n",
    "                                \n",
    "                                elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                                    y_true_unscaled = y_token_dict[f\"output_{feature}\"]\n",
    "                                    y_true = y_scaler.inverse_transform( y_true_unscaled.reshape(-1, y_true_unscaled.shape[-1])\n",
    "                                                                        ).reshape(y_true_unscaled.shape)\n",
    "                                    y_pred = y_scaler.inverse_transform( result )\n",
    "                                    mae_value = metrics.mean_absolute_error(y_true, y_pred)\n",
    "                                    mse_value = metrics.mean_squared_error(y_true, y_pred)\n",
    "                                    rmse_value = np.sqrt(mse_value)\n",
    "                                    r2_value = metrics.r2_score(y_true, y_pred)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[feature].append(i)\n",
    "                                    mae[feature].append(mae_value)\n",
    "                                    mse[feature].append(mse_value)\n",
    "                                    rmse[feature].append(rmse_value)\n",
    "                                    r2[feature].append(r2_value)\n",
    "                                    weights[feature].append(weight)\n",
    "            for target_col in self.target_columns.keys():\n",
    "                feature_results = []\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_accuracy = np.average(accuracies[target_col], weights=weights[target_col])\n",
    "                            weighted_fscore = np.average(fscores[target_col], weights=weights[target_col])\n",
    "                            weighted_precision = np.average(precisions[target_col], weights=weights[target_col])\n",
    "                            weighted_recall = np.average(recalls[target_col], weights=weights[target_col])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_col].append(\"\")\n",
    "                            k[target_col].append(\"Weighted Mean\")\n",
    "                            accuracies[target_col].append(weighted_accuracy)\n",
    "                            fscores[target_col].append(weighted_fscore)\n",
    "                            precisions[target_col].append(weighted_precision)\n",
    "                            recalls[target_col].append(weighted_recall)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_col}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_col],\n",
    "                                'weight': weights[target_col],\n",
    "                                'accuracy': accuracies[target_col],\n",
    "                                'fscore': fscores[target_col],\n",
    "                                'precision': precisions[target_col],\n",
    "                                'recall': recalls[target_col]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "                        \n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_mae = np.average(mae[target_col], weights=weights[target_col])\n",
    "                            weighted_mse = np.average(mse[target_col], weights=weights[target_col])\n",
    "                            weighted_rmse = np.average(rmse[target_col], weights=weights[target_col])\n",
    "                            weighted_r2 = np.average(r2[target_col], weights=weights[target_col])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_col].append(\"\")\n",
    "                            k[target_col].append(\"Weighted Mean\")\n",
    "                            mae[target_col].append(weighted_mae)\n",
    "                            mse[target_col].append(weighted_mse)\n",
    "                            rmse[target_col].append(weighted_rmse)\n",
    "                            r2[target_col].append(weighted_r2)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_col}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_col],\n",
    "                                'weight': weights[target_col],\n",
    "                                'mae': mae[target_col],\n",
    "                                'mse': mse[target_col],\n",
    "                                'rmse': rmse[target_col],\n",
    "                                'r2': r2[target_col]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "            results.append(feature_results)\n",
    "            print(\"_____________________________________________\")\n",
    "            \n",
    "          \n",
    "            \n",
    "            # calculate predictions for all test data\n",
    "            _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_dfs,\n",
    "                                                    x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "            # sort dicts\n",
    "            x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "            y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(x_token_dict)\n",
    "            \n",
    "            # Handle multiple outputs for multitask learning\n",
    "            if len(self.target_columns) > 1:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "            else:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "                \n",
    "            feature_preds = []\n",
    "            for feature, result in result_dict.items():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if feature in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            y_true = y_token_dict[f\"output_{feature}\"]\n",
    "                            y_pred = np.argmax(result, axis=1)\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            y_true_unscaled = y_token_dict[f\"output_{feature}\"]\n",
    "                            y_true = y_scaler.inverse_transform( y_true_unscaled.reshape(-1, y_true_unscaled.shape[-1])\n",
    "                                                                ).reshape(y_true_unscaled.shape)\n",
    "                            y_pred = y_scaler.inverse_transform( result )\n",
    "                        preds_df = pd.DataFrame({\n",
    "                                        'y_true': y_true,\n",
    "                                        'y_pred': y_pred\n",
    "                                    })\n",
    "                        feature_preds.append(preds_df)\n",
    "            preds.append(feature_preds)\n",
    "                    \n",
    "        \n",
    "        return results, preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            \n",
    "    def safe_results(self, histories: list, results: list, preds: list):\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name, \"results\", timestamp )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Save histories and results\n",
    "        for model_idx, (history, result, pred) in enumerate(zip(histories, results, preds)):\n",
    "            # Save history as CSV\n",
    "            history_df = pd.DataFrame(history.history)\n",
    "            history_path = os.path.join(dir_path, f\"history_{model_idx+1}.csv\")\n",
    "            history_df.to_csv(history_path, index=False)\n",
    "            \n",
    "            for output_idx, (output_result_df, output_pred_df) in enumerate(zip(result, pred)):\n",
    "                # Save results DataFrame as CSV\n",
    "                results_path = os.path.join(dir_path, f\"results_{model_idx+1}__output_{output_idx+1}.csv\")\n",
    "                output_result_df.to_csv(results_path, index=False)\n",
    "                \n",
    "                # Save predictions DataFrame as CSV\n",
    "                results_path = os.path.join(dir_path, f\"predictions_{model_idx+1}__output_{output_idx+1}.csv\")\n",
    "                output_pred_df.to_csv(results_path, index=False)\n",
    "\n",
    "        print(f\"Histories and results saved to {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "# helper function to save xes file as csv\n",
    "def save_csv(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    pipe.save_as_csv()\n",
    "    \n",
    "\n",
    "# helper function: do only preprocessing on data\n",
    "def preprocess(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "\n",
    "# helper function\n",
    "def run(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    case_ids, train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # train the model\n",
    "    models, histories = pipe.train(\n",
    "                case_ids = case_ids,\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length,\n",
    "                y_scaler = y_scaler,\n",
    "                mask = mask\n",
    "                )\n",
    "\n",
    "    # evaluate the model\n",
    "    results, preds = pipe.evaluate(models=models, data_loader=data_loader, test_dfs=test_dfs, x_scaler=x_scaler,\n",
    "                                y_scaler=y_scaler, max_case_length=max_case_length)\n",
    "    \n",
    "    # safe the training histories and results\n",
    "    pipe.safe_results(histories=histories, results=results, preds=preds)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"======================================\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    \n",
    "# function for testing out code\n",
    "def test(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # # train the model\n",
    "    # model = pipe.train(\n",
    "    #             feature_type_dict = feature_type_dict,\n",
    "    #             train_token_dict_x = train_token_dict_x,\n",
    "    #             train_token_dict_y = train_token_dict_y,\n",
    "    #             word_dicts = word_dicts,\n",
    "    #             max_case_length = max_case_length\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: 'helpdesk'\n",
      "filepath: 'helpdesk.csv'\n",
      "columns: '['Case ID', 'Activity', 'Complete Timestamp']'\n",
      "additional_columns: '{<Feature_Type.CATEGORICAL: 'categorical'>: ['Resource']}'\n",
      "datetime_format: '%Y-%m-%d %H:%M:%S.%f'\n",
      "Model Epochs: '1'\n",
      "Number of Transformer Layers in Model: '1'\n",
      "Target columns: '{'concept_name': <Target.NEXT_FEATURE: 'next_feature'>}'\n",
      "Input columns: '['concept_name', 'Resource', 'Complete Timestamp']'\n",
      "\n",
      "All processed files for current spec found. Preprocessing skipped.\n",
      "Loading data from preprocessed train-test split...\n",
      "['time_timestamp', 'concept_name', 'Resource']\n",
      "Preparing data...\n",
      "Using regular train-validation split\n",
      "Creating model...\n",
      "Masking active.\n",
      "Using Single-Task Learning Setup\n",
      "891/891 [==============================] - 38s 34ms/step - loss: 0.7702 - sparse_categorical_accuracy: 0.7621 - val_loss: 0.6862 - val_sparse_categorical_accuracy: 0.7863\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 1...\n",
      "Prefix length: 1\n",
      "Preparing data...\n",
      "29/29 [==============================] - 2s 24ms/step\n",
      "Prefix length: 2\n",
      "Preparing data...\n",
      "29/29 [==============================] - 1s 26ms/step\n",
      "Prefix length: 3\n",
      "Preparing data...\n",
      "28/28 [==============================] - 1s 20ms/step\n",
      "Prefix length: 4\n",
      "Preparing data...\n",
      "12/12 [==============================] - 0s 18ms/step\n",
      "Prefix length: 5\n",
      "Preparing data...\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "Prefix length: 6\n",
      "Preparing data...\n",
      "2/2 [==============================] - 0s 25ms/step\n",
      "Prefix length: 7\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Prefix length: 8\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "Prefix length: 9\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "Prefix length: 10\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "Prefix length: 11\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "Prefix length: 12\n",
      "Preparing data...\n",
      "Prefix length: 13\n",
      "Preparing data...\n",
      "Prefix length: 14\n",
      "Preparing data...\n",
      "Results for concept_name\n",
      "                k    weight  accuracy    fscore  precision    recall\n",
      "0               1  0.274745  0.853712  0.809355   0.830355  0.853712\n",
      "1               2  0.274445  0.692896  0.589248   0.516039  0.692896\n",
      "2               3  0.262448  0.840000  0.804806   0.799457  0.840000\n",
      "3               4  0.111878  0.836461  0.796711   0.766952  0.836461\n",
      "4               5  0.046191  0.863636  0.827584   0.822685  0.863636\n",
      "5               6  0.017996  0.833333  0.785689   0.812865  0.833333\n",
      "6               7  0.007199  0.791667  0.709105   0.652976  0.791667\n",
      "7               8  0.003299  0.636364  0.606061   0.727273  0.636364\n",
      "8               9    0.0012  0.750000  0.750000   0.750000  0.750000\n",
      "9              10    0.0003  1.000000  1.000000   1.000000  1.000000\n",
      "10             11    0.0003  1.000000  1.000000   1.000000  1.000000\n",
      "11  Weighted Mean            0.802939  0.745406   0.726609  0.802939\n",
      "_____________________________________________\n",
      "Preparing data...\n",
      "105/105 [==============================] - 4s 32ms/step\n",
      "Histories and results saved to datasets\\helpdesk\\results\\20240912-194202\n",
      "\n",
      "======================================\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_learning_rate\": 0.001,\n",
    "#         \"model_epochs\": 3,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Complete Timestamp\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF\n",
    "#         }\n",
    "\n",
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"model_epochs\": 1,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"Activity\", \"Resource\", \"Complete Timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_learning_rate\": 0.001,\n",
    "#         \"model_epochs\": 1,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\", \"Resource\", \"Complete Timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: True, Temporal_Feature.HOUR_OF_DAY: True}\n",
    "#         }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:group\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 1,\n",
    "        \"model_num_layers\": 10,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:group\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: True, Temporal_Feature.HOUR_OF_DAY: True}\n",
    "        }\n",
    "\n",
    "args_bpi_2012 = {\n",
    "        \"dataset_name\": \"bpi_2012\",\n",
    "        \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\"]\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 2,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\"]\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_learning_rate\": 0.001,\n",
    "        \"model_epochs\": 2,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept_name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept_name\", \"org_resource\"]\n",
    "        }\n",
    "\n",
    "\n",
    "run(args_helpdesk)\n",
    "# preprocess(args_helpdesk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change settings and run again\n",
    "\n",
    "# args_bpi_2012[\"additional_columns\"] = {}\n",
    "# args_bpi_2012[\"input_columns\"] = [\"concept:name\"]\n",
    "# run(args_bpi_2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(additional_columns={Feature_Type.CATEGORICAL: [\"Resource\"]}, input_columns=[\"Activity\", \"Resource\"], target_columns=[\"Activity\", \"Resource\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
