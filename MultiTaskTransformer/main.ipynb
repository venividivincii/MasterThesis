{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:05:21.378583: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-23 22:05:21.378632: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-23 22:05:21.378641: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-23 22:05:21.385480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device Name: NVIDIA RTX A6000\n",
      "Total Memory: 48669.75 MB\n",
      "Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get CUDA device information\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "device = cuda.Device(0)\n",
    "print(\"Device Name:\", device.name())\n",
    "print(\"Total Memory:\", device.total_memory() / (1024 ** 2), \"MB\")\n",
    "print(\"Compute Capability:\", device.compute_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pip installs\n",
    "# pip install numpy pandas tqdm matplotlib scikit-learn tensorflow pm4py\n",
    "# !pip install numpy pandas tqdm matplotlib scikit-learn tensorflow pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:05:24.324779: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-09-23 22:05:24.325449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46604 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:25:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from typing import List, Optional\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import pm4py\n",
    "from package import transformer\n",
    "from package.loader import LogsDataLoader\n",
    "from package.processor import LogsDataProcessor, masked_standard_scaler, masked_min_max_scaler\n",
    "from package.constants import Feature_Type, Target, Temporal_Feature, Model_Architecture\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize data dir, if not exists\n",
    "if not os.path.exists(\"datasets\"): \n",
    "    os.mkdir(\"datasets\")\n",
    "\n",
    "# Set global random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, job_id: str, dataset_name: str, filepath: str, sorting: bool, columns: List[str],\n",
    "                 additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, model_epochs: int, warmup_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: Dict[str, Target], temporal_features: Dict[Temporal_Feature, bool],\n",
    "                 cross_val: bool, model_architecture: Model_Architecture):\n",
    "        self.job_id: str = job_id\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.sorting: bool = sorting\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.warmup_epochs: int = warmup_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[tuple, Target] = target_columns\n",
    "        for target_col, suffix in target_columns.keys():\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[(\"concept_name\", suffix)] = self.target_columns.pop((target_col, suffix))\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        self.temporal_features: Dict[Temporal_Feature, bool] = temporal_features\n",
    "        self.cross_val = cross_val\n",
    "        self.model_architecture = model_architecture\n",
    "        self.start_timestamp = None\n",
    "        self.end_timestamp = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def save_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            sorting=self.sorting,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,  # Add all additional columns here, first all categorical, then all numerical features\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            datetime_format=self.datetime_format,\n",
    "            temporal_features=self.temporal_features,\n",
    "            pool=4\n",
    "        )\n",
    "        \n",
    "        self.target_columns = {(data_processor.sanitize_filename(feature, self.columns), suffix): target for (feature, suffix), target in self.target_columns.items()}\n",
    "        self.input_columns = [data_processor.sanitize_filename(col, self.columns) for col in self.input_columns]\n",
    "        self.columns = [data_processor.sanitize_filename(col, self.columns) for col in self.columns]\n",
    "        \n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs()\n",
    "        # flatten self.additional_columns to get all used features\n",
    "        self.additional_columns = data_processor.additional_columns\n",
    "        self.used_features = [item for sublist in self.additional_columns.values() for item in sublist]\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, sorting=self.sorting, input_columns=self.input_columns,\n",
    "                                     target_columns=self.target_columns, temporal_features=self.temporal_features)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict, mask = data_loader.load_data()\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame], x_scaler=None, y_scaler=None,\n",
    "                     train: bool = True) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data...\")\n",
    "        # initialize max_case_length\n",
    "        max_case_length = False\n",
    "        # initialize token dicts\n",
    "        x_token_dict, y_token_dict, x_token_dict_numerical, y_token_dict_numerical = {}, {}, {}, {}\n",
    "        \n",
    "        # initialize case_id_df\n",
    "        case_ids = next(iter(dfs.values()))[\"case_id\"]\n",
    "        \n",
    "        # loop over all feature dfs\n",
    "        for idx, (feature, feature_df) in enumerate(dfs.items()):\n",
    "\n",
    "            feature_type = None\n",
    "            # get current feature_type\n",
    "            for _feature_type, feature_lst in self.additional_columns.items():\n",
    "                if feature in feature_lst:\n",
    "                    feature_type = _feature_type\n",
    "                    break\n",
    "            \n",
    "            if idx == 0 and train:\n",
    "                (x_tokens, y_tokens, max_case_length\n",
    "                ) = data_loader.prepare_data(feature=feature, df=feature_df, max_case_length=True)\n",
    "            else:\n",
    "                x_tokens, y_tokens = data_loader.prepare_data(feature=feature, df=feature_df)\n",
    "            \n",
    "            if feature_type is Feature_Type.TIMESTAMP or feature_type is Feature_Type.NUMERICAL:\n",
    "                x_token_dict_numerical.update(x_tokens)\n",
    "                y_token_dict_numerical.update(y_tokens)\n",
    "            else:\n",
    "                # update x_token_dict\n",
    "                x_token_dict.update(x_tokens)\n",
    "                y_token_dict.update(y_tokens)\n",
    "        if len(x_token_dict_numerical) > 0  and len(list(x_token_dict_numerical.values())[0]) > 0:\n",
    "            # Concatenate all the feature arrays along the rows (axis=0)\n",
    "            combined_data = np.vstack(list(x_token_dict_numerical.values()))\n",
    "            if x_scaler is None:\n",
    "                # Initialize the StandardScaler\n",
    "                # x_scaler = StandardScaler()\n",
    "                # x_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                # x_scaler = FunctionTransformer(masked_standard_scaler, kw_args={'padding_value': -1})\n",
    "                x_scaler = FunctionTransformer(masked_min_max_scaler, kw_args={'padding_value': -1})\n",
    "                # Fit the scaler on the combined data\n",
    "                x_scaler.fit(combined_data)\n",
    "            # Transform the combined data\n",
    "            scaled_combined_data = x_scaler.transform(combined_data)\n",
    "            # split the scaled combined data back into the original feature dict\n",
    "            split_indices = np.cumsum([value.shape[0] for value in x_token_dict_numerical.values()])[:-1]\n",
    "            scaled_data_parts = np.vsplit(scaled_combined_data, split_indices)\n",
    "            # Reconstruct the dictionary with scaled data\n",
    "            scaled_dict = {key: scaled_data_parts[i] for i, key in enumerate(x_token_dict_numerical.keys())}\n",
    "            # update x_token_dict\n",
    "            x_token_dict.update(scaled_dict)\n",
    "        if len(y_token_dict_numerical) > 0:\n",
    "            # Prepare list to store valid arrays (non-empty)\n",
    "            valid_arrays = []\n",
    "            valid_keys = []\n",
    "\n",
    "            # Check for empty arrays and prepare data for scaling\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size > 0:  # Only consider non-empty arrays\n",
    "                    valid_arrays.append(value.reshape(-1, 1))  # Reshape to 2D\n",
    "                    valid_keys.append(key)\n",
    "\n",
    "            # If there are valid arrays to scale\n",
    "            if valid_arrays:\n",
    "                combined_data = np.hstack(valid_arrays)  # Horizontal stacking for features\n",
    "\n",
    "                if y_scaler is None:\n",
    "                    # Initialize the StandardScaler\n",
    "                    # y_scaler = StandardScaler()\n",
    "                    y_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                    # Fit the scaler on the combined data\n",
    "                    print(\"-----------COMBINED DATA SHAPE -------------\")\n",
    "                    print(type(combined_data))\n",
    "                    print(combined_data.shape)\n",
    "                    y_scaler.fit(combined_data)\n",
    "\n",
    "                # Transform the combined data\n",
    "                scaled_combined_data = y_scaler.transform(combined_data)\n",
    "\n",
    "                # Split the scaled combined data back into individual features\n",
    "                scaled_data_parts = np.hsplit(scaled_combined_data, scaled_combined_data.shape[1])\n",
    "\n",
    "                # Reconstruct the dictionary with scaled data\n",
    "                scaled_dict = {key: scaled_data_parts[i].flatten() for i, key in enumerate(valid_keys)}\n",
    "\n",
    "                # Update y_token_dict with the scaled data\n",
    "                y_token_dict.update(scaled_dict)\n",
    "\n",
    "            # Handle any empty arrays (if necessary)\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size == 0:\n",
    "                    # Optionally, you can handle empty arrays here, e.g., leave them as-is\n",
    "                    y_token_dict[key] = value\n",
    "            \n",
    "            \n",
    "        # sort dicts\n",
    "        x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "        y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "        return case_ids, x_token_dict, y_token_dict, x_scaler, y_scaler, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "            case_ids: pd.DataFrame,\n",
    "            feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "            train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "            train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "            word_dicts: Dict[str, Dict[str, int]],\n",
    "            max_case_length: int,\n",
    "            y_scaler,\n",
    "            mask # Fraction of the training data to be used for validation\n",
    "            ) -> tf.keras.Model:\n",
    "\n",
    "        # Ensure that input columns and dictionaries are sorted\n",
    "        self.input_columns.sort()\n",
    "        self.target_columns = dict(sorted(self.target_columns.items()))\n",
    "        train_token_dict_x = dict(sorted(train_token_dict_x.items()))\n",
    "        train_token_dict_y = dict(sorted(train_token_dict_y.items()))\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "\n",
    "        # initialize model_wrapper with data for model\n",
    "        model_wrapper = transformer.ModelWrapper(\n",
    "                                                job_id = self.job_id,\n",
    "                                                dataset_name = self.dataset_name,\n",
    "                                                case_ids = case_ids,\n",
    "                                                input_columns=self.input_columns,\n",
    "                                                target_columns=self.target_columns,\n",
    "                                                additional_columns=self.additional_columns,\n",
    "                                                word_dicts=word_dicts,\n",
    "                                                max_case_length=max_case_length,\n",
    "                                                feature_type_dict=feature_type_dict,\n",
    "                                                temporal_features=self.temporal_features,\n",
    "                                                model_architecture=self.model_architecture,\n",
    "                                                sorting=self.sorting,\n",
    "                                                masking=True\n",
    "                                                )\n",
    "\n",
    "        # train the model\n",
    "        models, histories = model_wrapper.train_model(\n",
    "                                                    train_token_dict_x = train_token_dict_x,\n",
    "                                                    train_token_dict_y = train_token_dict_y,\n",
    "                                                    cross_val = self.cross_val,\n",
    "                                                    y_scaler = y_scaler,\n",
    "                                                    model_epochs = self.model_epochs,\n",
    "                                                    batch_size = 12,\n",
    "                                                    warmup_epochs = self.warmup_epochs,\n",
    "                                                    initial_lr = 1e-5,\n",
    "                                                    target_lr = 1e-3\n",
    "                                                    )\n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(histories)\n",
    "        return models, histories\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, histories):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # If there are multiple histories (cross-validation), plot for each fold\n",
    "        if isinstance(histories, list):\n",
    "            for i, history in enumerate(histories):\n",
    "                # Extract the loss and validation loss from the custom history structure\n",
    "                training_loss = [epoch['loss'] for epoch in history]\n",
    "                validation_loss = [epoch['val_loss'] for epoch in history if 'val_loss' in epoch]\n",
    "                \n",
    "                plt.plot(training_loss, label=f'Training Loss Fold {i+1}')\n",
    "                if validation_loss:\n",
    "                    plt.plot(validation_loss, label=f'Validation Loss Fold {i+1}')\n",
    "        else:\n",
    "            # Single history (no cross-validation)\n",
    "            training_loss = [epoch['loss'] for epoch in histories]\n",
    "            validation_loss = [epoch['val_loss'] for epoch in histories if 'val_loss' in epoch]\n",
    "            \n",
    "            plt.plot(training_loss, label='Training Loss')\n",
    "            if validation_loss:\n",
    "                plt.plot(validation_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    # inverse transform single feature with scaler fitted on multiple features\n",
    "    def _scaler_inverse_transform(self, scaler, feature_data):\n",
    "        # flatten feature_data\n",
    "        feature_data = feature_data.flatten()\n",
    "        # Get the number of features the scaler was fitted with\n",
    "        scaler_features = scaler.n_features_in_\n",
    "        if scaler_features == 1:\n",
    "            reshaped_data = feature_data.reshape(-1, 1)\n",
    "        else:\n",
    "            # Reshape the 1D array to (batch_size, scaler_features) with dummy values for other features\n",
    "            batch_size = feature_data.shape[0]\n",
    "            reshaped_data = np.zeros((batch_size, scaler_features))\n",
    "            # Fill the first column with feature values\n",
    "            reshaped_data[:, 0] = feature_data\n",
    "        # Now you can use inverse_transform on reshaped_data\n",
    "        inverse_transformed_data = scaler.inverse_transform(reshaped_data)\n",
    "        # Extract only the inverse-transformed first column (since that's the one you care about)\n",
    "        inverse_transformed_feature = inverse_transformed_data[:, 0]\n",
    "        return inverse_transformed_feature\n",
    "    \n",
    "\n",
    "    def evaluate(self, models, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame],\n",
    "                 max_case_length: int, x_scaler=None, y_scaler=None):\n",
    "        \n",
    "        results, preds = [], []\n",
    "        for idx, model in enumerate(models):\n",
    "            print(f\"Evaluating model {idx+1}...\")\n",
    "\n",
    "            # Prepare lists to store evaluation metrics\n",
    "            k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "            mae, mse, rmse, r2 = {}, {}, {}, {}\n",
    "            \n",
    "            for (target_col, suffix) in self.target_columns.keys():\n",
    "                target_key = (target_col, suffix)\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        k.update({target_key: []})\n",
    "                        weights.update({target_key: []})\n",
    "                        \n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            accuracies.update({target_key: []})\n",
    "                            fscores.update({target_key: []})\n",
    "                            precisions.update({target_key: []})\n",
    "                            recalls.update({target_key: []})\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            mae.update({target_key: []})\n",
    "                            mse.update({target_key: []})\n",
    "                            rmse.update({target_key: []})\n",
    "                            r2.update({target_key: []})\n",
    "\n",
    "            # Calculate total number of samples\n",
    "            total_samples = len(list(test_dfs.values())[0])\n",
    "\n",
    "            # Iterate over all prefixes (k)\n",
    "            for i in range(1, max_case_length + 1):\n",
    "                print(\"Prefix length: \" + str(i))\n",
    "                test_data_subsets = {}\n",
    "\n",
    "                for key, df in test_dfs.items():\n",
    "                    if (Feature_Type.TIMESTAMP in self.additional_columns\n",
    "                            and key in self.additional_columns[Feature_Type.TIMESTAMP]):\n",
    "                        prefix_str = f\"{key}##Prefix Length\"\n",
    "                    else:\n",
    "                        prefix_str = \"Prefix Length\"\n",
    "                    filtered_df = df[df[prefix_str] == i]\n",
    "                    test_data_subsets.update({key: filtered_df})\n",
    "\n",
    "\n",
    "                _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_data_subsets,\n",
    "                                                                x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "\n",
    "                # sort dicts\n",
    "                x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "                y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "\n",
    "                if len(test_data_subsets[self.input_columns[0]]) > 0:\n",
    "\n",
    "                    # Make predictions\n",
    "                    predictions = model.predict(x_token_dict)\n",
    "                    \n",
    "                    # Handle multiple outputs for multitask learning\n",
    "                    if len(self.target_columns) > 1:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "                    else:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "\n",
    "                    # Compute metrics\n",
    "                    for (feature, suffix), result in result_dict.items():\n",
    "                        target_key = (feature, suffix)\n",
    "                        for feature_type, feature_lst in self.additional_columns.items():\n",
    "                            if feature in feature_lst:\n",
    "                                if feature_type is Feature_Type.CATEGORICAL:\n",
    "                                    result = np.argmax(result, axis=1)\n",
    "                                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}_{suffix}\"], result)\n",
    "                                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                                        y_token_dict[f\"output_{feature}_{suffix}\"], result, average=\"weighted\", zero_division=0)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[target_key].append(i)\n",
    "                                    accuracies[target_key].append(accuracy)\n",
    "                                    fscores[target_key].append(fscore)\n",
    "                                    precisions[target_key].append(precision)\n",
    "                                    recalls[target_key].append(recall)\n",
    "                                    weights[target_key].append(weight)\n",
    "                                \n",
    "                                elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                                    # inverse transform y_true and y_pred\n",
    "                                    y_true = self._scaler_inverse_transform( y_scaler, y_token_dict[f\"output_{feature}_{suffix}\"] )\n",
    "                                    y_pred = self._scaler_inverse_transform(y_scaler, result)\n",
    "                                    \n",
    "                                    mae_value = metrics.mean_absolute_error(y_true, y_pred)\n",
    "                                    mse_value = metrics.mean_squared_error(y_true, y_pred)\n",
    "                                    rmse_value = np.sqrt(mse_value)\n",
    "                                    r2_value = metrics.r2_score(y_true, y_pred)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[target_key].append(i)\n",
    "                                    mae[target_key].append(mae_value)\n",
    "                                    mse[target_key].append(mse_value)\n",
    "                                    rmse[target_key].append(rmse_value)\n",
    "                                    r2[target_key].append(r2_value)\n",
    "                                    weights[target_key].append(weight)\n",
    "            feature_results = []\n",
    "            for (target_col, suffix) in self.target_columns.keys():\n",
    "                target_key = (target_col, suffix)\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_accuracy = np.average(accuracies[target_key], weights=weights[target_key])\n",
    "                            weighted_fscore = np.average(fscores[target_key], weights=weights[target_key])\n",
    "                            weighted_precision = np.average(precisions[target_key], weights=weights[target_key])\n",
    "                            weighted_recall = np.average(recalls[target_key], weights=weights[target_key])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_key].append(\"\")\n",
    "                            k[target_key].append(\"Weighted Mean\")\n",
    "                            accuracies[target_key].append(weighted_accuracy)\n",
    "                            fscores[target_key].append(weighted_fscore)\n",
    "                            precisions[target_key].append(weighted_precision)\n",
    "                            recalls[target_key].append(weighted_recall)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_key}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_key],\n",
    "                                'weight': weights[target_key],\n",
    "                                'accuracy': accuracies[target_key],\n",
    "                                'fscore': fscores[target_key],\n",
    "                                'precision': precisions[target_key],\n",
    "                                'recall': recalls[target_key]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "                        \n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_mae = np.average(mae[target_key], weights=weights[target_key])\n",
    "                            weighted_mse = np.average(mse[target_key], weights=weights[target_key])\n",
    "                            weighted_rmse = np.average(rmse[target_key], weights=weights[target_key])\n",
    "                            weighted_r2 = np.average(r2[target_key], weights=weights[target_key])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_key].append(\"\")\n",
    "                            k[target_key].append(\"Weighted Mean\")\n",
    "                            mae[target_key].append(weighted_mae)\n",
    "                            mse[target_key].append(weighted_mse)\n",
    "                            rmse[target_key].append(weighted_rmse)\n",
    "                            r2[target_key].append(weighted_r2)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_key}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_key],\n",
    "                                'weight': weights[target_key],\n",
    "                                'mae': mae[target_key],\n",
    "                                'mse': mse[target_key],\n",
    "                                'rmse': rmse[target_key],\n",
    "                                'r2': r2[target_key]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "            results.append(feature_results)\n",
    "            print(\"_____________________________________________\")\n",
    "            \n",
    "          \n",
    "            \n",
    "            # calculate predictions for all test data\n",
    "            _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_dfs,\n",
    "                                                    x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "            # sort dicts\n",
    "            x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "            y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(x_token_dict)\n",
    "            \n",
    "            # Handle multiple outputs for multitask learning\n",
    "            if len(self.target_columns) > 1:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "            else:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "                \n",
    "            feature_preds = []\n",
    "            for (feature, suffix), result in result_dict.items():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if feature in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            y_true = y_token_dict[f\"output_{feature}_{suffix}\"]\n",
    "                            y_pred = np.argmax(result, axis=1)\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            # inverse transform y_true and y_pred\n",
    "                            y_true = self._scaler_inverse_transform( y_scaler, y_token_dict[f\"output_{feature}_{suffix}\"] )\n",
    "                            y_pred = self._scaler_inverse_transform(y_scaler, result)\n",
    "                            # y_true_unscaled = y_token_dict[f\"output_{feature}_{suffix}\"]\n",
    "                            # y_true = y_scaler.inverse_transform( y_true_unscaled.reshape(-1, y_true_unscaled.shape[-1])\n",
    "                            #                                     ).reshape(y_true_unscaled.shape)\n",
    "                            # y_pred = y_scaler.inverse_transform( result )\n",
    "                        preds_df = pd.DataFrame({\n",
    "                                        'y_true': y_true.reshape(-1),\n",
    "                                        'y_pred': y_pred.reshape(-1)\n",
    "                                    })\n",
    "                        feature_preds.append(preds_df)\n",
    "            preds.append(feature_preds)\n",
    "                    \n",
    "        \n",
    "        return results, preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            \n",
    "    def safe_results(self, y_scaler, histories: list, results: list, preds: list):\n",
    "        \n",
    "        # Timestamp for folder naming\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        elapsed_time = self.end_timestamp - self.start_timestamp\n",
    "        \n",
    "        # Directory for saving results\n",
    "        dir_path = os.path.join(\"datasets\", self.dataset_name, \"results\", timestamp)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Save parameters in JSON\n",
    "        parameters = {\n",
    "            \"Input Columns\": self.input_columns,\n",
    "            \"Target Columns\": {f\"{col}_{suff}\": value.value for (col, suff), value in self.target_columns.items()},\n",
    "            \"Model Epochs\": self.model_epochs,\n",
    "            \"Transformer Layers\": self.model_num_layers,\n",
    "            \"Sorting\": self.sorting,\n",
    "            \"Cross Validation\": self.cross_val,\n",
    "            \"Elapsed Time\": elapsed_time\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(dir_path, \"parameters.json\"), \"w\") as metadata_file:\n",
    "            json.dump(parameters, metadata_file)\n",
    "        \n",
    "        # Save histories and results\n",
    "        for model_idx, (history, result, pred) in enumerate(zip(histories, results, preds)):\n",
    "            # Create DataFrame from custom history structure\n",
    "            history_df = pd.DataFrame(history)\n",
    "            \n",
    "            # Reverse transform MAE values if y_scaler is provided\n",
    "            for col in history_df.columns:\n",
    "                if \"mean_absolute_error\" in col:\n",
    "                    # history_df[col] = y_scaler.inverse_transform(\n",
    "                    #     history_df[col].values.reshape(-1, 1)\n",
    "                    # ).flatten()\n",
    "                    history_df[col] = self._scaler_inverse_transform(y_scaler, history_df[col].to_numpy())\n",
    "            \n",
    "            # Save history as CSV\n",
    "            history_path = os.path.join(dir_path, f\"history_{model_idx+1}.csv\")\n",
    "            history_df.to_csv(history_path, index=False)\n",
    "            \n",
    "            for output_idx, (output_result_df, output_pred_df) in enumerate(zip(result, pred)):\n",
    "                feature = list(self.target_columns.keys())[output_idx]\n",
    "                \n",
    "                # Save results DataFrame as CSV\n",
    "                results_path = os.path.join(dir_path, f\"results_{model_idx+1}__{feature}.csv\")\n",
    "                output_result_df.to_csv(results_path, index=False)\n",
    "                \n",
    "                # Save predictions DataFrame as CSV\n",
    "                predictions_path = os.path.join(dir_path, f\"predictions_{model_idx+1}__{feature}.csv\")\n",
    "                output_pred_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "        print(f\"Histories and results saved to {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "# helper function to save xes file as csv\n",
    "def save_csv(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    pipe.save_as_csv()\n",
    "    \n",
    "\n",
    "# helper function: do only preprocessing on data\n",
    "def preprocess(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "\n",
    "# helper function\n",
    "def run(job_id, args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(job_id, **args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    pipe.start_timestamp = time.time()\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    case_ids, train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    case_ids = case_ids.astype(str)\n",
    "    # Check for NaN or None values using pd.Series.isna()\n",
    "    assert not case_ids.isna().any(), \"case_ids contains NaN or None values!\"\n",
    "\n",
    "    # train the model\n",
    "    models, histories = pipe.train(\n",
    "                case_ids = case_ids,\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length,\n",
    "                y_scaler = y_scaler,\n",
    "                mask = mask\n",
    "                )\n",
    "\n",
    "    # evaluate the model\n",
    "    results, preds = pipe.evaluate(models=models, data_loader=data_loader, test_dfs=test_dfs, x_scaler=x_scaler,\n",
    "                                y_scaler=y_scaler, max_case_length=max_case_length)\n",
    "    \n",
    "    pipe.end_timestamp = time.time()\n",
    "    \n",
    "    # safe the training histories and results\n",
    "    pipe.safe_results(y_scaler=y_scaler, histories=histories, results=results, preds=preds)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"======================================\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    \n",
    "# function for testing out code\n",
    "def test(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    # # train the model\n",
    "    # model = pipe.train(\n",
    "    #             feature_type_dict = feature_type_dict,\n",
    "    #             train_token_dict_x = train_token_dict_x,\n",
    "    #             train_token_dict_y = train_token_dict_y,\n",
    "    #             word_dicts = word_dicts,\n",
    "    #             max_case_length = max_case_length\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args & Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_Val Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: 'helpdesk'\n",
      "filepath: 'helpdesk.csv'\n",
      "columns: '['Case ID', 'Activity', 'Complete Timestamp']'\n",
      "additional_columns: '{}'\n",
      "datetime_format: '%Y-%m-%d %H:%M:%S.%f'\n",
      "Model Epochs: '1'\n",
      "Number of Transformer Layers in Model: '1'\n",
      "Target columns: '{('Complete Timestamp', 'next'): <Target.NEXT_FEATURE: 'next_feature'>, ('concept_name', 'next'): <Target.NEXT_FEATURE: 'next_feature'>}'\n",
      "Input columns: '['concept_name', 'Complete Timestamp']'\n",
      "\n",
      "All processed files for current spec found. Preprocessing skipped.\n",
      "Loading data from preprocessed train-test split...\n",
      "['time_timestamp', 'concept_name']\n",
      "Preparing data...\n",
      "-----------COMBINED DATA SHAPE -------------\n",
      "<class 'numpy.ndarray'>\n",
      "(13431, 1)\n",
      "------------------------------------------------------------------------\n",
      "Using 2-Fold Cross-Validation with Grouping by case_id\n",
      "Training fold 1/2...\n",
      "Creating model...\n",
      "Masking active.\n",
      "Using Multi-Task Learning Setup\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:05:28.848138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcd00010b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-23 22:05:28.848169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-09-23 22:05:28.853532: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-23 22:05:28.869529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2024-09-23 22:05:28.938175: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 12s 12ms/step - loss: 2.2797 - output_concept_name_next_loss: 0.8594 - output_time_timestamp_next_loss: 1.4203 - output_concept_name_next_sparse_categorical_accuracy: 0.7303 - output_time_timestamp_next_mean_absolute_error: 3.3779 - val_loss: 1.8764 - val_output_concept_name_next_loss: 0.7175 - val_output_time_timestamp_next_loss: 1.1589 - val_output_concept_name_next_sparse_categorical_accuracy: 0.7793 - val_output_time_timestamp_next_mean_absolute_error: 2.8868 - lr: 0.0010\n",
      "Training fold 2/2...\n",
      "Creating model...\n",
      "Masking active.\n",
      "Using Multi-Task Learning Setup\n",
      "560/560 [==============================] - 10s 12ms/step - loss: 2.3000 - output_concept_name_next_loss: 0.8640 - output_time_timestamp_next_loss: 1.4360 - output_concept_name_next_sparse_categorical_accuracy: 0.7365 - output_time_timestamp_next_mean_absolute_error: 3.4122 - val_loss: 1.8548 - val_output_concept_name_next_loss: 0.7006 - val_output_time_timestamp_next_loss: 1.1541 - val_output_concept_name_next_sparse_categorical_accuracy: 0.7735 - val_output_time_timestamp_next_mean_absolute_error: 2.8187 - lr: 0.0010\n",
      "Creating model...\n",
      "Masking active.\n",
      "Using Multi-Task Learning Setup\n",
      "Creating model...\n",
      "Masking active.\n",
      "Using Multi-Task Learning Setup\n",
      "Average validation loss across 2 folds: 1.8655917644500732\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIhCAYAAAB5deq6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABbrElEQVR4nO3deVxV1f7/8fdhnhEHBHNAU0Gc53nKOTO9eq9WpGI4VGiZlkNelbIcKtTKspvXsZwytbxlzqLmkFOoJdeycCjlamqgogiyf3/45fw6gmxA4KC+no/Hfjw466y112efs/Xh2733wmIYhiEAAAAAwB052LsAAAAAACjqCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AkAsWiyVHW0xMzF3NExUVJYvFkqexMTEx+VJDURceHq6goKA7vn/+/Hm5uLjoiSeeuGOfpKQkeXh46PHHH8/xvAsWLJDFYtGJEydyXMtfWSwWRUVF5Xi+DGfOnFFUVJRiY2MzvXc358vdCgoK0mOPPWaXuQGgMDnZuwAAuJfs3r3b5vWkSZO0detWbdmyxaY9NDT0ruYZOHCgOnfunKex9erV0+7du++6hntdqVKl9Pjjj+uLL77QpUuX5Ofnl6nPsmXLdO3aNUVERNzVXOPHj9eLL754V/swc+bMGb322msKCgpSnTp1bN67m/MFAJAzBCcAyIUmTZrYvC5VqpQcHBwytd8uOTlZHh4eOZ6nbNmyKlu2bJ5q9PHxMa3nQREREaGVK1dq8eLFGjp0aKb3582bp9KlS6tr1653Nc/DDz98V+Pv1t2cLwCAnOFWPQDIZ23atFGNGjW0fft2NWvWTB4eHnrmmWckScuXL1fHjh0VGBgod3d3VatWTWPGjNHVq1dt9pHVrVcZt0StW7dO9erVk7u7u0JCQjRv3jybflndqhceHi4vLy8dP35cjz76qLy8vFSuXDmNHDlSKSkpNuN/++03/f3vf5e3t7eKFSumsLAw7du3TxaLRQsWLMj22M+fP6/nn39eoaGh8vLykr+/vx555BHt2LHDpt+JEydksVj0zjvvaPr06apYsaK8vLzUtGlT7dmzJ9N+FyxYoODgYLm6uqpatWpatGhRtnVk6NSpk8qWLav58+dnei8uLk7fffed+vXrJycnJ23cuFHdu3dX2bJl5ebmpsqVK2vIkCH6448/TOfJ6la9pKQkDRo0SCVKlJCXl5c6d+6sn376KdPY48ePa8CAAapSpYo8PDz00EMPqVu3bjpy5Ii1T0xMjBo2bChJGjBggPWW0Ixb/rI6X9LT0/XWW28pJCRErq6u8vf3V79+/fTbb7/Z9Ms4X/ft26eWLVvKw8NDlSpV0tSpU5Wenm567Dlx/fp1jR07VhUrVpSLi4seeughRUZG6s8//7Tpt2XLFrVp00YlSpSQu7u7ypcvr169eik5OdnaZ/bs2apdu7a8vLzk7e2tkJAQvfrqq/lSJwBkhytOAFAAzp49q6efflqjRo3S5MmT5eBw6/+pfv75Zz366KMaPny4PD099d///lfTpk3T3r17M93ul5VDhw5p5MiRGjNmjEqXLq1///vfioiIUOXKldWqVatsx6ampurxxx9XRESERo4cqe3bt2vSpEny9fXVhAkTJElXr15V27ZtdfHiRU2bNk2VK1fWunXr1KdPnxwd98WLFyVJEydOVEBAgK5cuaLVq1erTZs22rx5s9q0aWPT/4MPPlBISIhmzpwp6dYtb48++qji4+Pl6+sr6VZoGjBggLp3767o6GglJiYqKipKKSkp1s/1ThwcHBQeHq433nhDhw4dUu3ata3vZYSpjFD7yy+/qGnTpho4cKB8fX114sQJTZ8+XS1atNCRI0fk7Oyco89AkgzDUI8ePbRr1y5NmDBBDRs21M6dO9WlS5dMfc+cOaMSJUpo6tSpKlWqlC5evKiFCxeqcePG+v777xUcHKx69epp/vz5GjBggP75z39ar5Bld5Xpueee08cff6yhQ4fqscce04kTJzR+/HjFxMTo4MGDKlmypLVvQkKCwsLCNHLkSE2cOFGrV6/W2LFjVaZMGfXr1y/Hx53dZ7F582aNHTtWLVu21OHDhzVx4kTt3r1bu3fvlqurq06cOKGuXbuqZcuWmjdvnooVK6bff/9d69at040bN+Th4aFly5bp+eef17Bhw/TOO+/IwcFBx48f19GjR++qRgDIEQMAkGf9+/c3PD09bdpat25tSDI2b96c7dj09HQjNTXV2LZtmyHJOHTokPW9iRMnGrf/FV2hQgXDzc3NOHnypLXt2rVrRvHixY0hQ4ZY27Zu3WpIMrZu3WpTpyTjs88+s9nno48+agQHB1tff/DBB4Yk45tvvrHpN2TIEEOSMX/+/GyP6XZpaWlGamqq0a5dO+Nvf/ubtT0+Pt6QZNSsWdNIS0uztu/du9eQZCxdutQwDMO4efOmUaZMGaNevXpGenq6td+JEycMZ2dno0KFCqY1/Prrr4bFYjFeeOEFa1tqaqoREBBgNG/ePMsxGd/NyZMnDUnGl19+aX1v/vz5hiQjPj7e2ta/f3+bWr755htDkvHuu+/a7PfNN980JBkTJ068Y71paWnGjRs3jCpVqhgvvfSStX3fvn13/A5uP1/i4uIMScbzzz9v0++7774zJBmvvvqqtS3jfP3uu+9s+oaGhhqdOnW6Y50ZKlSoYHTt2vWO769bt86QZLz11ls27cuXLzckGR9//LFhGIbx+eefG5KM2NjYO+5r6NChRrFixUxrAoCCwK16AFAA/Pz89Mgjj2Rq//XXX/XUU08pICBAjo6OcnZ2VuvWrSXdunXMTJ06dVS+fHnrazc3N1WtWlUnT540HWuxWNStWzebtlq1atmM3bZtm7y9vTMtNPDkk0+a7j/DRx99pHr16snNzU1OTk5ydnbW5s2bszy+rl27ytHR0aYeSdaajh07pjNnzuipp56yuRWtQoUKatasWY7qqVixotq2bavFixfrxo0bkqRvvvlGCQkJ1qtNknTu3Dk9++yzKleunLXuChUqSMrZd/NXW7dulSSFhYXZtD/11FOZ+qalpWny5MkKDQ2Vi4uLnJyc5OLiop9//jnX894+f3h4uE17o0aNVK1aNW3evNmmPSAgQI0aNbJpu/3cyKuMK6m31/KPf/xDnp6e1lrq1KkjFxcXDR48WAsXLtSvv/6aaV+NGjXSn3/+qSeffFJffvlljm6jBID8QnACgAIQGBiYqe3KlStq2bKlvvvuO73xxhuKiYnRvn37tGrVKknStWvXTPdbokSJTG2urq45Guvh4SE3N7dMY69fv259feHCBZUuXTrT2KzasjJ9+nQ999xzaty4sVauXKk9e/Zo37596ty5c5Y13n48rq6ukv7/Z3HhwgVJt/5hf7us2u4kIiJCFy5c0Jo1ayTduk3Py8tLvXv3lnTreaCOHTtq1apVGjVqlDZv3qy9e/dan7fKyef7VxcuXJCTk1Om48uq5hEjRmj8+PHq0aOH/vOf/+i7777Tvn37VLt27VzP+9f5pazPwzJlyljfz3A351VOanFyclKpUqVs2i0WiwICAqy1PPzww9q0aZP8/f0VGRmphx9+WA8//LDeffdd65i+fftq3rx5OnnypHr16iV/f381btxYGzduvOs6AcAMzzgBQAHI6nfqbNmyRWfOnFFMTIz1KpOkTA/I21OJEiW0d+/eTO0JCQk5Gv/pp5+qTZs2mj17tk375cuX81zPnebPaU2S1LNnT/n5+WnevHlq3bq1vvrqK/Xr109eXl6SpB9++EGHDh3SggUL1L9/f+u448eP57nutLQ0XbhwwSaUZFXzp59+qn79+mny5Mk27X/88YeKFSuW5/mlW8/a3f4c1JkzZ2yebypoGZ/F+fPnbcKTYRhKSEiwLnohSS1btlTLli118+ZN7d+/X++//76GDx+u0qVLW38f14ABAzRgwABdvXpV27dv18SJE/XYY4/pp59+sl4hBICCwBUnACgkGWEq46pKhn/961/2KCdLrVu31uXLl/XNN9/YtC9btixH4y0WS6bjO3z4cKbff5VTwcHBCgwM1NKlS2UYhrX95MmT2rVrV4734+bmpqeeekobNmzQtGnTlJqaanObXn5/N23btpUkLV682KZ9yZIlmfpm9Zl9/fXX+v33323abr8al52M20Q//fRTm/Z9+/YpLi5O7dq1M91HfsmY6/ZaVq5cqatXr2ZZi6Ojoxo3bqwPPvhAknTw4MFMfTw9PdWlSxeNGzdON27c0I8//lgA1QPA/8cVJwAoJM2aNZOfn5+effZZTZw4Uc7Ozlq8eLEOHTpk79Ks+vfvrxkzZujpp5/WG2+8ocqVK+ubb77R+vXrJcl0FbvHHntMkyZN0sSJE9W6dWsdO3ZMr7/+uipWrKi0tLRc1+Pg4KBJkyZp4MCB+tvf/qZBgwbpzz//VFRUVK5u1ZNu3a73wQcfaPr06QoJCbF5RiokJEQPP/ywxowZI8MwVLx4cf3nP//J8y1gHTt2VKtWrTRq1ChdvXpVDRo00M6dO/XJJ59k6vvYY49pwYIFCgkJUa1atXTgwAG9/fbbma4UPfzww3J3d9fixYtVrVo1eXl5qUyZMipTpkymfQYHB2vw4MF6//335eDgoC5dulhX1StXrpxeeumlPB3XnSQkJOjzzz/P1B4UFKQOHTqoU6dOGj16tJKSktS8eXPrqnp169ZV3759Jd16Nm7Lli3q2rWrypcvr+vXr1uX2m/fvr0kadCgQXJ3d1fz5s0VGBiohIQETZkyRb6+vjZXrgCgIBCcAKCQlChRQl9//bVGjhypp59+Wp6enurevbuWL1+uevXq2bs8Sbf+F3/Lli0aPny4Ro0aJYvFoo4dO+rDDz/Uo48+anrr2Lhx45ScnKy5c+fqrbfeUmhoqD766COtXr3a5vdK5UZERIQkadq0aerZs6eCgoL06quvatu2bbnaZ926dVW3bl19//33NlebJMnZ2Vn/+c9/9OKLL2rIkCFycnJS+/bttWnTJpvFOHLKwcFBa9as0YgRI/TWW2/pxo0bat68udauXauQkBCbvu+++66cnZ01ZcoUXblyRfXq1dOqVav0z3/+06afh4eH5s2bp9dee00dO3ZUamqqJk6caP1dTrebPXu2Hn74Yc2dO1cffPCBfH191blzZ02ZMiXLZ5ruxoEDB/SPf/wjU3v//v21YMECffHFF4qKitL8+fP15ptvqmTJkurbt68mT55svZJWp04dbdiwQRMnTlRCQoK8vLxUo0YNrVmzRh07dpR061a+BQsW6LPPPtOlS5dUsmRJtWjRQosWLcr0DBUA5DeL8dd7HwAAyMLkyZP1z3/+U6dOncr2dwcBAHC/4ooTAMDGrFmzJN26fS01NVVbtmzRe++9p6effprQBAB4YBGcAAA2PDw8NGPGDJ04cUIpKSkqX768Ro8enenWMQAAHiTcqgcAAAAAJliOHAAAAABMEJwAAAAAwATBCQAAAABMPHCLQ6Snp+vMmTPy9va2/qZ4AAAAAA8ewzB0+fJllSlTxvSXvD9wwenMmTMqV66cvcsAAAAAUEScPn3a9FduPHDBydvbW9KtD8fHx8fO1QAAAACwl6SkJJUrV86aEbLzwAWnjNvzfHx8CE4AAAAAcvQID4tDAAAAAIAJghMAAAAAmCA4AQAAAICJB+4ZJwAAgPvZzZs3lZqaau8ygCLD2dlZjo6Od70fghMAAMB94sqVK/rtt99kGIa9SwGKDIvForJly8rLy+uu9kNwAgAAuA/cvHlTv/32mzw8PFSqVKkcrRIG3O8Mw9D58+f122+/qUqVKnd15YngBAAAcB9ITU2VYRgqVaqU3N3d7V0OUGSUKlVKJ06cUGpq6l0FJxaHAAAAuI9wpQmwlV9/JghOAAAAAGCC4AQAAAAAJghOAAAAuK+0adNGw4cPz3H/EydOyGKxKDY2tsBqut+Eh4erR48e2fbJ7fdQ1BGcAAAAYBcWiyXbLTw8PE/7XbVqlSZNmpTj/uXKldPZs2dVo0aNPM2XU0UpoEVFRWX5mW/atKnQajh79qyeeuopBQcHy8HBociHLFbVAwAAgF2cPXvW+vPy5cs1YcIEHTt2zNp2++qAqampcnZ2Nt1v8eLFc1WHo6OjAgICcjXmflC9evVMQSm3n93dSElJUalSpTRu3DjNmDGj0ObNK644AQAA3IcMw1DyjTS7bDn9BbwBAQHWzdfXVxaLxfr6+vXrKlasmD777DO1adNGbm5u+vTTT3XhwgU9+eSTKlu2rDw8PFSzZk0tXbrUZr+33yIWFBSkyZMn65lnnpG3t7fKly+vjz/+2Pr+7VeCYmJiZLFYtHnzZjVo0EAeHh5q1qyZTaiTpDfeeEP+/v7y9vbWwIEDNWbMGNWpUydP35d0K0i88MIL8vf3l5ubm1q0aKF9+/ZZ37906ZLCwsKsS85XqVJF8+fPlyTduHFDQ4cOVWBgoNzc3BQUFKQpU6ZkO5+Tk5PNdxAQECAXFxdJ0pEjR/TII4/I3d1dJUqU0ODBg3XlypU77uvq1avq16+fvLy8FBgYqOjoaNPjDQoK0rvvvqt+/frJ19c3Jx+RXdn1itOUKVO0atUq/fe//5W7u7uaNWumadOmKTg4+I5jvv32W40ePVr//e9/lZycrAoVKmjIkCF66aWXCrFyAACAou1a6k2FTlhvl7mPvt5JHi7588/M0aNHKzo6WvPnz5erq6uuX7+u+vXra/To0fLx8dHXX3+tvn37qlKlSmrcuPEd9xMdHa1Jkybp1Vdf1eeff67nnntOrVq1UkhIyB3HjBs3TtHR0SpVqpSeffZZPfPMM9q5c6ckafHixXrzzTf14Ycfqnnz5lq2bJmio6NVsWLFPB/rqFGjtHLlSi1cuFAVKlTQW2+9pU6dOun48eMqXry4xo8fr6NHj+qbb75RyZIldfz4cV27dk2S9N5772nNmjX67LPPVL58eZ0+fVqnT5/OUx3Jycnq3LmzmjRpon379uncuXMaOHCghg4dqgULFmQ55pVXXtHWrVu1evVqBQQE6NVXX9WBAwfuKkgWNXYNTtu2bVNkZKQaNmyotLQ0jRs3Th07dtTRo0fl6emZ5RhPT08NHTpUtWrVkqenp7799lsNGTJEnp6eGjx4cCEfAQAAAArS8OHD1bNnT5u2l19+2frzsGHDtG7dOq1YsSLb4PToo4/q+eefl3QrjM2YMUMxMTHZBqc333xTrVu3liSNGTNGXbt21fXr1+Xm5qb3339fERERGjBggCRpwoQJ2rBhQ7ZXZbJz9epVzZ49WwsWLFCXLl0kSXPmzNHGjRs1d+5cvfLKKzp16pTq1q2rBg0aSLp1xSbDqVOnVKVKFbVo0UIWi0UVKlQwnfPIkSPy8vKyvg4NDdXevXu1ePFiXbt2TYsWLbL+m3zWrFnq1q2bpk2bptKlS9vs58qVK5o7d64WLVqkDh06SJIWLlyosmXL5umzKKrsGpzWrVtn83r+/Pny9/fXgQMH1KpVqyzH1K1bV3Xr1rW+DgoK0qpVq7Rjxw6CEwAAwP9xd3bU0dc72W3u/JIREjLcvHlTU6dO1fLly/X7778rJSVFKSkpd/xP9wy1atWy/pxxS+C5c+dyPCYwMFCSdO7cOZUvX17Hjh2zBrEMjRo10pYtW3J0XLf75ZdflJqaqubNm1vbnJ2d1ahRI8XFxUmSnnvuOfXq1UsHDx5Ux44d1aNHDzVr1kzSrVXuOnTooODgYHXu3FmPPfaYOnbsmO2cwcHBWrNmjfW1q6urJCkuLk61a9e2+UybN2+u9PR0HTt2LFNw+uWXX3Tjxg01bdrU2la8ePFs7yK7FxWpxSESExMl5e6htO+//167du3SG2+8keX7GX+YMiQlJd1dkQAAAPcAi8WSb7fL2dPtgSg6OlozZszQzJkzVbNmTXl6emr48OG6ceNGtvu5fVEJi8Wi9PT0HI+xWCySZDMmoy1DTp/tykrG2Kz2mdHWpUsXnTx5Ul9//bU2bdqkdu3aKTIyUu+8847q1aun+Ph4ffPNN9q0aZN69+6t9u3b6/PPP7/jnC4uLqpcuXKWtdxeR4as2u/muO8lRWZxCMMwNGLECLVo0SJHS0GWLVtWrq6uatCggSIjIzVw4MAs+02ZMkW+vr7WrVy5cvldOgAAAArJjh071L17dz399NOqXbu2KlWqpJ9//rnQ6wgODtbevXtt2vbv35/n/VWuXFkuLi769ttvrW2pqanav3+/qlWrZm0rVaqUwsPD9emnn2rmzJk2i1z4+PioT58+mjNnjpYvX66VK1fq4sWLua4lNDRUsbGxunr1qrVt586dcnBwUNWqVbOs3dnZWXv27LG2Xbp0ST/99FOu5y7Kisx/QwwdOlSHDx+2OVmys2PHDl25ckV79uzRmDFjVLlyZT355JOZ+o0dO1YjRoywvk5KSiI8AQAA3KMqV66slStXateuXfLz89P06dOVkJBgEy4Kw7BhwzRo0CA1aNBAzZo10/Lly3X48GFVqlTJdOztq/NJt8LKc889p1deeUXFixdX+fLl9dZbbyk5OVkRERGSbj1HVb9+fVWvXl0pKSn66quvrMc9Y8YMBQYGqk6dOnJwcNCKFSsUEBCgYsWK5frYwsLCNHHiRPXv319RUVE6f/68hg0bpr59+2a6TU+SvLy8FBERoVdeeUUlSpRQ6dKlNW7cODk4mF+jyVjJ8MqVKzp//rxiY2Pl4uKi0NDQXNdd0IpEcBo2bJjWrFmj7du35/ghsowVS2rWrKn//e9/ioqKyjI4ubq6Wu/XBAAAwL1t/Pjxio+PV6dOneTh4aHBgwerR48e1kc+CktYWJh+/fVXvfzyy7p+/bp69+6t8PDwTFehsvLEE09kaouPj9fUqVOVnp6uvn376vLly2rQoIHWr18vPz8/SbdurRs7dqxOnDghd3d3tWzZUsuWLZN0K7xMmzZNP//8sxwdHdWwYUOtXbs2R+Hldh4eHlq/fr1efPFFNWzYUB4eHurVq5emT59+xzFvv/22rly5oscff1ze3t4aOXJkjr6Tv65dcODAAS1ZskQVKlTQiRMncl13QbMYdrwp0TAMDRs2TKtXr1ZMTIyqVKmSp/1MmjRJc+fOzdEHnJSUJF9fXyUmJsrHxydP8wEAABQ1169fV3x8vCpWrCg3Nzd7l/NA6tChgwICAvTJJ5/YuxT8RXZ/NnKTDex6xSkyMlJLlizRl19+KW9vbyUkJEiSfH19rb8peuzYsfr999+1aNEiSdIHH3yg8uXLW5eO/Pbbb/XOO+9o2LBh9jkIAAAAPHCSk5P10UcfqVOnTnJ0dNTSpUu1adMmbdy40d6loYDYNTjNnj1b0q3f7vxX8+fPV3h4uCTp7NmzOnXqlPW99PR0jR07VvHx8XJyctLDDz+sqVOnasiQIYVVNgAAAB5wFotFa9eu1RtvvKGUlBQFBwdr5cqVat++vb1LQwGx66169sCtegAA4H7ErXpA1vLrVr0isxw5AAAAABRVBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAwD2tTZs2Gj58uPV1UFCQZs6cme0Yi8WiL7744q7nzq/9PCgK87vJbwQnAAAA2EW3bt3u+Atjd+/eLYvFooMHD+Z6v/v27dPgwYPvtjwbUVFRqlOnTqb2s2fPqkuXLvk61+0WLFigYsWKFegcOdWmTRtZLJZMW1paWqHVsH37dnXr1k1lypQp1JBFcAIAAIBdREREaMuWLTp58mSm9+bNm6c6deqoXr16ud5vqVKl5OHhkR8lmgoICJCrq2uhzFVUDBo0SGfPnrXZnJycCm3+q1evqnbt2po1a1ahzSkRnAAAAO5PhiHduGqfzTByVOJjjz0mf39/LViwwKY9OTlZy5cvV0REhC5cuKAnn3xSZcuWlYeHh2rWrKmlS5dmu9/bbwf7+eef1apVK7m5uSk0NFQbN27MNGb06NGqWrWqPDw8VKlSJY0fP16pqamSbl3xee2113To0CHrFZaMmm+/4nHkyBE98sgjcnd3V4kSJTR48GBduXLF+n54eLh69Oihd955R4GBgSpRooQiIyOtc+XFqVOn1L17d3l5ecnHx0e9e/fW//73P+v7hw4dUtu2beXt7S0fHx/Vr19f+/fvlySdPHlS3bp1k5+fnzw9PVW9enWtXbs22/k8PDwUEBBgs2VYuXKlqlevLldXVwUFBSk6OjrbfeXku7ldly5d9MYbb6hnz56mffNT4UVDAAAAFJ7UZGlyGfvM/eoZycXTtJuTk5P69eunBQsWaMKECbJYLJKkFStW6MaNGwoLC1NycrLq16+v0aNHy8fHR19//bX69u2rSpUqqXHjxqZzpKenq2fPnipZsqT27NmjpKQkm+ehMnh7e2vBggUqU6aMjhw5okGDBsnb21ujRo1Snz599MMPP2jdunXatGmTJMnX1zfTPpKTk9W5c2c1adJE+/bt07lz5zRw4EANHTrUJhxu3bpVgYGB2rp1q44fP64+ffqoTp06GjRokOnx3M4wDPXo0UOenp7atm2b0tLS9Pzzz6tPnz6KiYmRJIWFhalu3bqaPXu2HB0dFRsbK2dnZ0lSZGSkbty4oe3bt8vT01NHjx6Vl5dXruuQpAMHDqh3796KiopSnz59tGvXLj3//PMqUaKEwsPDM/XP6XdTVBCcAAAAYDfPPPOM3n77bcXExKht27aSbt2m17NnT/n5+cnPz08vv/yytf+wYcO0bt06rVixIkfBadOmTYqLi9OJEydUtmxZSdLkyZMzPZf0z3/+0/pzUFCQRo4cqeXLl2vUqFFyd3eXl5eXnJycbK6u3G7x4sW6du2aFi1aJE/PW8Fx1qxZ6tatm6ZNm6bSpUtLkvz8/DRr1iw5OjoqJCREXbt21ebNm/MUnDZt2qTDhw8rPj5e5cqVkyR98sknql69uvbt26eGDRvq1KlTeuWVVxQSEiJJqlKlinX8qVOn1KtXL9WsWVOSVKlSJdM5P/zwQ/373/+2vh4yZIiio6M1ffp0tWvXTuPHj5ckVa1aVUePHtXbb7+dZXDK6XdTVBCcAAAA7kfOHreu/Nhr7hwKCQlRs2bNNG/ePLVt21a//PKLduzYoQ0bNkiSbt68qalTp2r58uX6/ffflZKSopSUFGswMRMXF6fy5ctb/2EuSU2bNs3U7/PPP9fMmTN1/PhxXblyRWlpafLx8cnxcWTMVbt2bZvamjdvrvT0dB07dswanKpXry5HR0drn8DAQB05ciRXc/11znLlyllDkySFhoaqWLFiiouLU8OGDTVixAgNHDhQn3zyidq3b69//OMfevjhhyVJL7zwgp577jlt2LBB7du3V69evVSrVq1s5wwLC9O4ceOsrzMWroiLi1P37t1t+jZv3lwzZ87UzZs3bY45o39OvpuigmecAAAA7kcWy63b5eyx/d8tdzkVERGhlStXKikpSfPnz1eFChXUrl07SVJ0dLRmzJihUaNGacuWLYqNjVWnTp1048aNHO3byOJ5K8tt9e3Zs0dPPPGEunTpoq+++krff/+9xo0bl+M5/jrX7fvOas6M2+T++l56enqu5jKb86/tUVFR+vHHH9W1a1dt2bJFoaGhWr16tSRp4MCB+vXXX9W3b18dOXJEDRo00Pvvv5/tnL6+vqpcubJ1K1my5B1ryerzz+69O31+RQHBCQAAAHbVu3dvOTo6asmSJVq4cKEGDBhg/Qf0jh071L17dz399NOqXbu2KlWqpJ9//jnH+w4NDdWpU6d05sz/v/q2e/dumz47d+5UhQoVNG7cODVo0EBVqlTJtNKfi4uLbt68aTpXbGysrl69arNvBwcHVa1aNcc150bG8Z0+fdradvToUSUmJqpatWrWtqpVq+qll17Shg0b1LNnT82fP9/6Xrly5fTss89q1apVGjlypObMmZPnWr799lubtl27dqlq1aqZrjb9tfbsvpuihOAEAAAAu/Ly8lKfPn306quv6syZMzbPw1SuXFkbN27Url27FBcXpyFDhighISHH+27fvr2Cg4PVr18/HTp0SDt27LC5zSxjjlOnTmnZsmX65Zdf9N5771mvyGQICgpSfHy8YmNj9ccffyglJSXTXGFhYXJzc1P//v31ww8/aOvWrRo2bJj69u1rvU0vr27evKnY2Fib7ejRo2rfvr1q1aqlsLAwHTx4UHv37lW/fv3UunVrNWjQQNeuXdPQoUMVExOjkydPaufOndq3b581VA0fPlzr169XfHy8Dh48qC1bttgErtwYOXKkNm/erEmTJumnn37SwoULNWvWLJtn1P4qJ99NVq5cuWL9DCRZv5dTp07lqe6cIjgBAADA7iIiInTp0iW1b99e5cuXt7aPHz9e9erVU6dOndSmTRsFBASoR48eOd6vg4ODVq9erZSUFDVq1EgDBw7Um2++adOne/fueumllzR06FDVqVNHu3btsi5wkKFXr17q3Lmz2rZtq1KlSmW5JLqHh4fWr1+vixcvqmHDhvr73/+udu3a5cvvG7py5Yrq1q1rsz366KPW5dD9/PzUqlUrtW/fXpUqVdLy5cslSY6Ojrpw4YL69eunqlWrqnfv3urSpYtee+01SbcCWWRkpKpVq6bOnTsrODhYH374YZ5qrFevnj777DMtW7ZMNWrU0IQJE/T6669nuTCElLPvJiv79++3fgaSNGLECNWtW1cTJkzIU905ZTGyu/HwPpSUlCRfX18lJibm+oE/AACAour69euKj49XxYoV5ebmZu9ygCIjuz8buckGXHECAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAANxX2rRpo+HDh+e4/4kTJ2SxWBQbG1tgNd1vwsPD1aNHj2z75PZ7KOoITgAAALALi8WS7RYeHp6n/a5atUqTJk3Kcf9y5crp7NmzqlGjRp7my6miFNCioqKy/Mw3bdpUaDWsWrVKHTp0UKlSpeTj46OmTZtq/fr1hTZ/bjnZuwAAAAA8mM6ePWv9efny5ZowYYKOHTtmbXN3d7fpn5qaKmdnZ9P9Fi9ePFd1ODo6KiAgIFdj7gfVq1fPFJRy+9ndje3bt6tDhw6aPHmyihUrpvnz56tbt2767rvvVLdu3UKrI6e44gQAAHAfMgxDyanJdtkMw8hRjQEBAdbN19dXFovF+vr69esqVqyYPvvsM7Vp00Zubm769NNPdeHCBT355JMqW7asPDw8VLNmTS1dutRmv7ffIhYUFKTJkyfrmWeekbe3t8qXL6+PP/7Y+v7tV4JiYmJksVi0efNmNWjQQB4eHmrWrJlNqJOkN954Q/7+/vL29tbAgQM1ZswY1alTJ0/flySlpKTohRdekL+/v9zc3NSiRQvt27fP+v6lS5cUFhamUqVKyd3dXVWqVNH8+fMlSTdu3NDQoUMVGBgoNzc3BQUFacqUKdnO5+TkZPMdBAQEyMXFRZJ05MgRPfLII3J3d1eJEiU0ePBgXbly5Y77unr1qvr16ycvLy8FBgYqOjra9HhnzpypUaNGqWHDhqpSpYomT56sKlWq6D//+U9OPq5CxxUnAACA+9C1tGtqvKSxXeb+7qnv5OHskS/7Gj16tKKjozV//ny5urrq+vXrql+/vkaPHi0fHx99/fXX6tu3rypVqqTGje98vNHR0Zo0aZJeffVVff7553ruuefUqlUrhYSE3HHMuHHjFB0drVKlSunZZ5/VM888o507d0qSFi9erDfffFMffvihmjdvrmXLlik6OloVK1bM87GOGjVKK1eu1MKFC1WhQgW99dZb6tSpk44fP67ixYtr/PjxOnr0qL755huVLFlSx48f17Vr1yRJ7733ntasWaPPPvtM5cuX1+nTp3X69Ok81ZGcnKzOnTurSZMm2rdvn86dO6eBAwdq6NChWrBgQZZjXnnlFW3dulWrV69WQECAXn31VR04cCBXQTI9PV2XL18u1KteuUFwAgAAQJE1fPhw9ezZ06bt5Zdftv48bNgwrVu3TitWrMg2OD366KN6/vnnJd0KYzNmzFBMTEy2wenNN99U69atJUljxoxR165ddf36dbm5uen9999XRESEBgwYIEmaMGGCNmzYkO1VmexcvXpVs2fP1oIFC9SlSxdJ0pw5c7Rx40bNnTtXr7zyik6dOqW6deuqQYMGkm5dSctw6tQpValSRS1atJDFYlGFChVM5zxy5Ii8vLysr0NDQ7V3714tXrxY165d06JFi+Tp6SlJmjVrlrp166Zp06apdOnSNvu5cuWK5s6dq0WLFqlDhw6SpIULF6ps2bK5+gyio6N19epV9e7dO1fjCgvBCQAA4D7k7uSu7576zm5z55eMkJDh5s2bmjp1qpYvX67ff/9dKSkpSklJsf4D/05q1apl/TnjlsBz587leExgYKAk6dy5cypfvryOHTtmDWIZGjVqpC1btuTouG73yy+/KDU1Vc2bN7e2OTs7q1GjRoqLi5MkPffcc+rVq5cOHjyojh07qkePHmrWrJmkW6vcdejQQcHBwercubMee+wxdezYMds5g4ODtWbNGutrV1dXSVJcXJxq165t85k2b95c6enpOnbsWKbg9Msvv+jGjRtq2rSpta148eIKDg7O8fEvXbpUUVFR+vLLL+Xv75/jcYWJ4AQAAHAfslgs+Xa7nD3dHoiio6M1Y8YMzZw5UzVr1pSnp6eGDx+uGzduZLuf2xeVsFgsSk9Pz/EYi8UiSTZjMtoy5PTZrqxkjM1qnxltXbp00cmTJ/X1119r06ZNateunSIjI/XOO++oXr16io+P1zfffKNNmzapd+/eat++vT7//PM7zuni4qLKlStnWcvtdWTIqv1ujlu6tTBIRESEVqxYofbt29/VvgoSi0MAAADgnrFjxw51795dTz/9tGrXrq1KlSrp559/LvQ6goODtXfvXpu2/fv353l/lStXlouLi7799ltrW2pqqvbv369q1apZ20qVKqXw8HB9+umnmjlzps0iFz4+PurTp4/mzJmj5cuXa+XKlbp48WKuawkNDVVsbKyuXr1qbdu5c6ccHBxUtWrVLGt3dnbWnj17rG2XLl3STz/9ZDrX0qVLFR4eriVLlqhr1665rrUwccUJAAAA94zKlStr5cqV2rVrl/z8/DR9+nQlJCTYhIvCMGzYMA0aNEgNGjRQs2bNtHz5ch0+fFiVKlUyHXv76nzSrbDy3HPP6ZVXXlHx4sVVvnx5vfXWW0pOTlZERISkW89R1a9fX9WrV1dKSoq++uor63HPmDFDgYGBqlOnjhwcHLRixQoFBASoWLFiuT62sLAwTZw4Uf3791dUVJTOnz+vYcOGqW/fvplu05MkLy8vRURE6JVXXlGJEiVUunRpjRs3Tg4O2V+jWbp0qfr166d3331XTZo0UUJCgqRby9D7+vrmuu6CRnACAADAPWP8+PGKj49Xp06d5OHhocGDB6tHjx5KTEws1DrCwsL066+/6uWXX9b169fVu3dvhYeHZ7oKlZUnnngiU1t8fLymTp2q9PR09e3bV5cvX1aDBg20fv16+fn5Sbp1a93YsWN14sQJubu7q2XLllq2bJmkW+Fl2rRp+vnnn+Xo6KiGDRtq7dq1puElKx4eHlq/fr1efPFFNWzYUB4eHurVq5emT59+xzFvv/22rly5oscff1ze3t4aOXKk6Xfyr3/9S2lpaYqMjFRkZKS1vX///ndcvc+eLMbd3pR4j0lKSpKvr68SExPl4+Nj73IAAADyxfXr1xUfH6+KFSvKzc3N3uU8kDp06KCAgAB98skn9i4Ff5Hdn43cZAOuOAEAAAC5lJycrI8++kidOnWSo6Ojli5dqk2bNmnjxo32Lg0FhOAEAAAA5JLFYtHatWv1xhtvKCUlRcHBwVq5cmWRXhUOd4fgBAAAAOSSu7u7Nm3aZO8yUIhYjhwAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAD3tDZt2mj48OHW10FBQZo5c2a2YywWi7744ou7nju/9vOgKMzvJr8RnAAAAGAX3bp1u+MvjN29e7csFosOHjyY6/3u27dPgwcPvtvybERFRalOnTqZ2s+ePasuXbrk61y3W7BggYoVK1agc+RUmzZtZLFYMm1paWmFVsOUKVPUsGFDeXt7y9/fXz169NCxY8cKfF6CEwAAAOwiIiJCW7Zs0cmTJzO9N2/ePNWpU0f16tXL9X5LlSolDw+P/CjRVEBAgFxdXQtlrqJi0KBBOnv2rM3m5ORUaPNv27ZNkZGR2rNnjzZu3Ki0tDR17NhRV69eLdB5CU4AAAD3IcMwlJ6cbJfNMIwc1fjYY4/J399fCxYssGlPTk7W8uXLFRERoQsXLujJJ59U2bJl5eHhoZo1a2rp0qXZ7vf228F+/vlntWrVSm5ubgoNDdXGjRszjRk9erSqVq0qDw8PVapUSePHj1dqaqqkW1d8XnvtNR06dMh6hSWj5ttvKzty5IgeeeQRubu7q0SJEho8eLCuXLlifT88PFw9evTQO++8o8DAQJUoUUKRkZHWufLi1KlT6t69u7y8vOTj46PevXvrf//7n/X9Q4cOqW3btvL29paPj4/q16+v/fv3S5JOnjypbt26yc/PT56enqpevbrWrl2b7XweHh4KCAiw2TKsXLlS1atXl6urq4KCghQdHZ3tvnLy3dxu3bp1Cg8PV/Xq1VW7dm3Nnz9fp06d0oEDB0zH3o3Ci4YAAAAoNMa1azpWr75d5g4+eECWHFzxcXJyUr9+/bRgwQJNmDBBFotFkrRixQrduHFDYWFhSk5OVv369TV69Gj5+Pjo66+/Vt++fVWpUiU1btzYdI709HT17NlTJUuW1J49e5SUlGTzPFQGb29vLViwQGXKlNGRI0c0aNAgeXt7a9SoUerTp49++OEHrVu3Tps2bZIk+fr6ZtpHcnKyOnfurCZNmmjfvn06d+6cBg4cqKFDh9qEw61btyowMFBbt27V8ePH1adPH9WpU0eDBg0yPZ7bGYahHj16yNPTU9u2bVNaWpqef/559enTRzExMZKksLAw1a1bV7Nnz5ajo6NiY2Pl7OwsSYqMjNSNGze0fft2eXp66ujRo/Ly8sp1HZJ04MAB9e7dW1FRUerTp4927dql559/XiVKlFB4eHim/jn9bswkJiZKkooXL56nunOK4AQAAAC7eeaZZ/T2228rJiZGbdu2lXTrNr2ePXvKz89Pfn5+evnll639hw0bpnXr1mnFihU5Ck6bNm1SXFycTpw4obJly0qSJk+enOm5pH/+85/Wn4OCgjRy5EgtX75co0aNkru7u7y8vOTk5GRzdeV2ixcv1rVr17Ro0SJ5enpKkmbNmqVu3bpp2rRpKl26tCTJz89Ps2bNkqOjo0JCQtS1a1dt3rw5T8Fp06ZNOnz4sOLj41WuXDlJ0ieffKLq1atr3759atiwoU6dOqVXXnlFISEhkqQqVapYx586dUq9evVSzZo1JUmVKlUynfPDDz/Uv//9b+vrIUOGKDo6WtOnT1e7du00fvx4SVLVqlV19OhRvf3221kGp5x+N9kxDEMjRoxQixYtVKNGjRyPywuCEwAAwH3I4u6u4IMFe+tSdnPnVEhIiJo1a6Z58+apbdu2+uWXX7Rjxw5t2LBBknTz5k1NnTpVy5cv1++//66UlBSlpKRYg4mZuLg4lS9f3voPc0lq2rRppn6ff/65Zs6cqePHj+vKlStKS0uTj49Pjo8jY67atWvb1Na8eXOlp6fr2LFj1uBUvXp1OTo6WvsEBgbqyJEjuZrrr3OWK1fOGpokKTQ0VMWKFVNcXJwaNmyoESNGaODAgfrkk0/Uvn17/eMf/9DDDz8sSXrhhRf03HPPacOGDWrfvr169eqlWrVqZTtnWFiYxo0bZ32dsXBFXFycunfvbtO3efPmmjlzpm7evGlzzBn9c/LdZGfo0KE6fPiwvv3221yNywuecQIAALgPWSwWOXh42GXLuOUupyIiIrRy5UolJSVp/vz5qlChgtq1aydJio6O1owZMzRq1Cht2bJFsbGx6tSpk27cuJGjfWf1vNXt9e3Zs0dPPPGEunTpoq+++krff/+9xo0bl+M5/jrXnY79r+0Zt8n99b309PRczWU251/bo6Ki9OOPP6pr167asmWLQkNDtXr1aknSwIED9euvv6pv3746cuSIGjRooPfffz/bOX19fVW5cmXrVrJkyTvWkt3zbjn5brIzbNgwrVmzRlu3brUJXwWF4AQAAAC76t27txwdHbVkyRItXLhQAwYMsP4DeseOHerevbuefvpp1a5dW5UqVdLPP/+c432Hhobq1KlTOnPmjLVt9+7dNn127typChUqaNy4cWrQoIGqVKmSaaU/FxcX3bx503Su2NhYm9Xddu7cKQcHB1WtWjXHNedGxvGdPn3a2nb06FElJiaqWrVq1raqVavqpZde0oYNG9SzZ0/Nnz/f+l65cuX07LPPatWqVRo5cqTmzJmT51puv/Kza9cuVa1aNdPVpr/Wnt13kxXDMDR06FCtWrVKW7ZsUcWKFfNUb24RnAAAAGBXXl5e6tOnj1599VWdOXPG5nmYypUra+PGjdq1a5fi4uI0ZMgQJSQk5Hjf7du3V3BwsPr166dDhw5px44dNreZZcxx6tQpLVu2TL/88ovee+896xWZDEFBQYqPj1dsbKz++OMPpaSkZJorLCxMbm5u6t+/v3744Qdt3bpVw4YNU9++fa236eXVzZs3FRsba7MdPXpU7du3V61atRQWFqaDBw9q79696tevn1q3bq0GDRro2rVrGjp0qGJiYnTy5Ent3LlT+/bts4aq4cOHa/369YqPj9fBgwe1ZcsWm8CVGyNHjtTmzZs1adIk/fTTT1q4cKFmzZpl84zaX+Xku8lKZGSkPv30Uy1ZskTe3t5KSEhQQkKCrl27lqe6c4rgBAAAALuLiIjQpUuX1L59e5UvX97aPn78eNWrV0+dOnVSmzZtFBAQoB49euR4vw4ODlq9erVSUlLUqFEjDRw4UG+++aZNn+7du+ull17S0KFDVadOHe3atcu6wEGGXr16qXPnzmrbtq1KlSqV5ZLoHh4eWr9+vS5evKiGDRvq73//u9q1a6dZs2bl7sPIwpUrV1S3bl2b7dFHH7Uuh+7n56dWrVqpffv2qlSpkpYvXy5JcnR01IULF9SvXz9VrVpVvXv3VpcuXfTaa69JuhXIIiMjVa1aNXXu3FnBwcH68MMP81RjvXr19Nlnn2nZsmWqUaOGJkyYoNdffz3LhSGknH03WZk9e7YSExPVpk0bBQYGWreMYy4oFiOnC+3fJ5KSkuTr66vExMRcP/AHAABQVF2/fl3x8fGqWLGi3Nzc7F0OUGRk92cjN9mAK04AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAD3kQds3S/AVH79mSA4AQAA3AcyfsHojRs37FwJULRk/JnI6pfw5oZTfhQDAAAA+3JycpKHh4fOnz8vZ2dnOTjw/+NAenq6zp8/Lw8PDzk53V30ITgBAADcBywWiwIDAxUfH6+TJ0/auxygyHBwcFD58uVlsVjuaj8EJwAAgPuEi4uLqlSpwu16wF+4uLjkyxVYghMAAMB9xMHBQW5ubvYuA7jvcPMrAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJiwa3CaMmWKGjZsKG9vb/n7+6tHjx46duxYtmNWrVqlDh06qFSpUvLx8VHTpk21fv36QqoYAAAAwIPIrsFp27ZtioyM1J49e7Rx40alpaWpY8eOunr16h3HbN++XR06dNDatWt14MABtW3bVt26ddP3339fiJUDAAAAeJBYDMMw7F1EhvPnz8vf31/btm1Tq1atcjyuevXq6tOnjyZMmGDaNykpSb6+vkpMTJSPj8/dlAsAAADgHpabbOBUSDXlSGJioiSpePHiOR6Tnp6uy5cv33FMSkqKUlJSrK+TkpLurkgAAAAAD5wisziEYRgaMWKEWrRooRo1auR4XHR0tK5evarevXtn+f6UKVPk6+tr3cqVK5dfJQMAAAB4QBSZW/UiIyP19ddf69tvv1XZsmVzNGbp0qUaOHCgvvzyS7Vv3z7LPlldcSpXrhy36gEAAAAPuHvuVr1hw4ZpzZo12r59e45D0/LlyxUREaEVK1bcMTRJkqurq1xdXfOrVAAAAAAPILsGJ8MwNGzYMK1evVoxMTGqWLFijsYtXbpUzzzzjJYuXaquXbsWcJUAAAAAHnR2DU6RkZFasmSJvvzyS3l7eyshIUGS5OvrK3d3d0nS2LFj9fvvv2vRokWSboWmfv366d1331WTJk2sY9zd3eXr62ufAwEAAABwX7Pr4hCzZ89WYmKi2rRpo8DAQOu2fPlya5+zZ8/q1KlT1tf/+te/lJaWpsjISJsxL774oj0OAQAAAMADoMgsDlFY+D1OAAAAAKTcZYMisxw5AAAAABRVBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATdg1OU6ZMUcOGDeXt7S1/f3/16NFDx44dy3bM2bNn9dRTTyk4OFgODg4aPnx44RQLAAAA4IFl1+C0bds2RUZGas+ePdq4caPS0tLUsWNHXb169Y5jUlJSVKpUKY0bN061a9cuxGoBAAAAPKgshmEY9i4iw/nz5+Xv769t27apVatWpv3btGmjOnXqaObMmTmeIykpSb6+vkpMTJSPj89dVAsAAADgXpabbOBUSDXlSGJioiSpePHi+bbPlJQUpaSkWF8nJSXl274BAAAAPBiKzOIQhmFoxIgRatGihWrUqJFv+50yZYp8fX2tW7ly5fJt3wAAAAAeDEUmOA0dOlSHDx/W0qVL83W/Y8eOVWJionU7ffp0vu4fAAAAwP2vSNyqN2zYMK1Zs0bbt29X2bJl83Xfrq6ucnV1zdd9AgAAAHiw2DU4GYahYcOGafXq1YqJiVHFihXtWQ4AAAAAZMmuwSkyMlJLlizRl19+KW9vbyUkJEiSfH195e7uLunWrXa///67Fi1aZB0XGxsrSbpy5YrOnz+v2NhYubi4KDQ0tNCPAQAAAMD9z67LkVsslizb58+fr/DwcElSeHi4Tpw4oZiYmGzHVahQQSdOnDCdk+XIAQAAAEj30HLkOclsCxYsyNM4AAAAAMgvRWZVPQAAAAAoqghOAAAAAGCC4AQAAAAAJvIUnE6fPq3ffvvN+nrv3r0aPny4Pv7443wrDAAAAACKijwFp6eeekpbt26VJCUkJKhDhw7au3evXn31Vb3++uv5WiAAAAAA2FuegtMPP/ygRo0aSZI+++wz1ahRQ7t27dKSJUuyXAUPAAAAAO5leQpOqampcnV1lSRt2rRJjz/+uCQpJCREZ8+ezb/qAAAAAKAIyFNwql69uj766CPt2LFDGzduVOfOnSVJZ86cUYkSJfK1QAAAAACwtzwFp2nTpulf//qX2rRpoyeffFK1a9eWJK1Zs8Z6Cx8AAAAA3C8shmEYeRl48+ZNJSUlyc/Pz9p24sQJeXh4yN/fP98KzG9JSUny9fVVYmKifHx87F0OAAAAADvJTTbI0xWna9euKSUlxRqaTp48qZkzZ+rYsWNFOjQBAAAAQF7kKTh1795dixYtkiT9+eefaty4saKjo9WjRw/Nnj07XwsEAAAAAHvLU3A6ePCgWrZsKUn6/PPPVbp0aZ08eVKLFi3Se++9l68FAgAAAIC95Sk4JScny9vbW5K0YcMG9ezZUw4ODmrSpIlOnjyZrwUCAAAAgL3lKThVrlxZX3zxhU6fPq3169erY8eOkqRz586x4AIAAACA+06egtOECRP08ssvKygoSI0aNVLTpk0l3br6VLdu3XwtEAAAAADsLc/LkSckJOjs2bOqXbu2HBxu5a+9e/fKx8dHISEh+VpkfmI5cgAAAABS7rKBU14nCQgIUEBAgH777TdZLBY99NBD/PJbAAAAAPelPN2ql56ertdff12+vr6qUKGCypcvr2LFimnSpElKT0/P7xoBAAAAwK7ydMVp3Lhxmjt3rqZOnarmzZvLMAzt3LlTUVFRun79ut588838rhMAAAAA7CZPzziVKVNGH330kR5//HGb9i+//FLPP/+8fv/993wrML/xjBMAAAAAKXfZIE+36l28eDHLBSBCQkJ08eLFvOwSAAAAAIqsPAWn2rVra9asWZnaZ82apVq1at11UQAAAABQlOTpGae33npLXbt21aZNm9S0aVNZLBbt2rVLp0+f1tq1a/O7RgAAAACwqzxdcWrdurV++ukn/e1vf9Off/6pixcvqmfPnvrxxx81f/78/K4RAAAAAOwqz78ANyuHDh1SvXr1dPPmzfzaZb5jcQgAAAAAUiEsDgEAAAAADxKCEwAAAACYIDgBAAAAgIlcrarXs2fPbN//888/76YWAAAAACiSchWcfH19Td/v16/fXRUEAAAAAEVNroITS40DAAAAeBDxjBMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJuwanKVOmqGHDhvL29pa/v7969OihY8eOmY7btm2b6tevLzc3N1WqVEkfffRRIVQLAAAA4EFl1+C0bds2RUZGas+ePdq4caPS0tLUsWNHXb169Y5j4uPj9eijj6ply5b6/vvv9eqrr+qFF17QypUrC7FyAAAAAA8Si2EYhr2LyHD+/Hn5+/tr27ZtatWqVZZ9Ro8erTVr1iguLs7a9uyzz+rQoUPavXu36RxJSUny9fVVYmKifHx88q12AAAAAPeW3GSDIvWMU2JioiSpePHid+yze/dudezY0aatU6dO2r9/v1JTUzP1T0lJUVJSks0GAAAAALlRZIKTYRgaMWKEWrRooRo1atyxX0JCgkqXLm3TVrp0aaWlpemPP/7I1H/KlCny9fW1buXKlcv32gEAAADc34pMcBo6dKgOHz6spUuXmva1WCw2rzPuNry9XZLGjh2rxMRE63b69On8KRgAAADAA8PJ3gVI0rBhw7RmzRpt375dZcuWzbZvQECAEhISbNrOnTsnJycnlShRIlN/V1dXubq65mu9AAAAAB4sdr3iZBiGhg4dqlWrVmnLli2qWLGi6ZimTZtq48aNNm0bNmxQgwYN5OzsXFClAgAAAHiA2TU4RUZG6tNPP9WSJUvk7e2thIQEJSQk6Nq1a9Y+Y8eOVb9+/ayvn332WZ08eVIjRoxQXFyc5s2bp7lz5+rll1+2xyEAAAAAeADYNTjNnj1biYmJatOmjQIDA63b8uXLrX3Onj2rU6dOWV9XrFhRa9euVUxMjOrUqaNJkybpvffeU69evexxCAAAAAAeAEXq9zgVBn6PEwAAAADpHv49TgAAAABQFBGcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATNg1OG3fvl3dunVTmTJlZLFY9MUXX5iO+eCDD1StWjW5u7srODhYixYtKvhCAQAAADzQnOw5+dWrV1W7dm0NGDBAvXr1Mu0/e/ZsjR07VnPmzFHDhg21d+9eDRo0SH5+furWrVshVAwAAADgQWTX4NSlSxd16dIlx/0/+eQTDRkyRH369JEkVapUSXv27NG0adMITgAAAAAKjF2DU26lpKTIzc3Nps3d3V179+5VamqqnJ2dsxyTkpJifZ2UlFTgdQIAAAC4v9xTi0N06tRJ//73v3XgwAEZhqH9+/dr3rx5Sk1N1R9//JHlmClTpsjX19e6lStXrpCrBgAAAHCvu6eC0/jx49WlSxc1adJEzs7O6t69u8LDwyVJjo6OWY4ZO3asEhMTrdvp06cLsWIAAAAA94N7Kji5u7tr3rx5Sk5O1okTJ3Tq1CkFBQXJ29tbJUuWzHKMq6urfHx8bDYAAAAAyI176hmnDM7OzipbtqwkadmyZXrsscfk4HBPZUAAAAAA9xC7BqcrV67o+PHj1tfx8fGKjY1V8eLFVb58eY0dO1a///679Xc1/fTTT9q7d68aN26sS5cuafr06frhhx+0cOFCex0CAAAAgAeAXYPT/v371bZtW+vrESNGSJL69++vBQsW6OzZszp16pT1/Zs3byo6OlrHjh2Ts7Oz2rZtq127dikoKKiwSwcAAADwALEYhmHYu4jClJSUJF9fXyUmJvK8EwAAAPAAy0024MEgAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAE072LqCwGYYhSUpKSrJzJQAAAADsKSMTZGSE7Dxwweny5cuSpHLlytm5EgAAAABFweXLl+Xr65ttH4uRk3h1H0lPT9eZM2fk7e0ti8Vi73JwB0lJSSpXrpxOnz4tHx8fe5eDewDnDHKLcwa5xTmD3OKcKfoMw9Dly5dVpkwZOThk/xTTA3fFycHBQWXLlrV3GcghHx8f/qJBrnDOILc4Z5BbnDPILc6Zos3sSlMGFocAAAAAABMEJwAAAAAwQXBCkeTq6qqJEyfK1dXV3qXgHsE5g9zinEFucc4gtzhn7i8P3OIQAAAAAJBbXHECAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXCCXVy6dEl9+/aVr6+vfH191bdvX/3555/ZjjEMQ1FRUSpTpozc3d3Vpk0b/fjjj3fs26VLF1ksFn3xxRf5fwAodAVxzly8eFHDhg1TcHCwPDw8VL58eb3wwgtKTEws4KNBQfjwww9VsWJFubm5qX79+tqxY0e2/bdt26b69evLzc1NlSpV0kcffZSpz8qVKxUaGipXV1eFhoZq9erVBVU+7CC/z5k5c+aoZcuW8vPzk5+fn9q3b6+9e/cW5CGgkBXE3zMZli1bJovFoh49euRz1cg3BmAHnTt3NmrUqGHs2rXL2LVrl1GjRg3jsccey3bM1KlTDW9vb2PlypXGkSNHjD59+hiBgYFGUlJSpr7Tp083unTpYkgyVq9eXUBHgcJUEOfMkSNHjJ49expr1qwxjh8/bmzevNmoUqWK0atXr8I4JOSjZcuWGc7OzsacOXOMo0ePGi+++KLh6elpnDx5Msv+v/76q+Hh4WG8+OKLxtGjR405c+YYzs7Oxueff27ts2vXLsPR0dGYPHmyERcXZ0yePNlwcnIy9uzZU1iHhQJUEOfMU089ZXzwwQfG999/b8TFxRkDBgwwfH19jd9++62wDgsFqCDOmQwnTpwwHnroIaNly5ZG9+7dC/hIkFcEJxS6o0ePGpJs/vGxe/duQ5Lx3//+N8sx6enpRkBAgDF16lRr2/Xr1w1fX1/jo48+sukbGxtrlC1b1jh79izB6T5R0OfMX3322WeGi4uLkZqamn8HgALXqFEj49lnn7VpCwkJMcaMGZNl/1GjRhkhISE2bUOGDDGaNGlifd27d2+jc+fONn06depkPPHEE/lUNeypIM6Z26WlpRne3t7GwoUL775g2F1BnTNpaWlG8+bNjX//+99G//79CU5FGLfqodDt3r1bvr6+aty4sbWtSZMm8vX11a5du7IcEx8fr4SEBHXs2NHa5urqqtatW9uMSU5O1pNPPqlZs2YpICCg4A4Chaogz5nbJSYmysfHR05OTvl3AChQN27c0IEDB2y+a0nq2LHjHb/r3bt3Z+rfqVMn7d+/X6mpqdn2ye78wb2hoM6Z2yUnJys1NVXFixfPn8JhNwV5zrz++usqVaqUIiIi8r9w5CuCEwpdQkKC/P39M7X7+/srISHhjmMkqXTp0jbtpUuXthnz0ksvqVmzZurevXs+Vgx7K8hz5q8uXLigSZMmaciQIXdZMQrTH3/8oZs3b+bqu05ISMiyf1pamv74449s+9xpn7h3FNQ5c7sxY8booYceUvv27fOncNhNQZ0zO3fu1Ny5czVnzpyCKRz5iuCEfBMVFSWLxZLttn//fkmSxWLJNN4wjCzb/+r29/86Zs2aNdqyZYtmzpyZPweEAmfvc+avkpKS1LVrV4WGhmrixIl3cVSwl5x+19n1v709t/vEvaUgzpkMb731lpYuXapVq1bJzc0tH6pFUZCf58zly5f19NNPa86cOSpZsmT+F4t8x70oyDdDhw7VE088kW2foKAgHT58WP/73/8yvXf+/PlM/zOTIeO2u4SEBAUGBlrbz507Zx2zZcsW/fLLLypWrJjN2F69eqlly5aKiYnJxdGgMNj7nMlw+fJlde7cWV5eXlq9erWcnZ1zeyiwo5IlS8rR0THT//pm9V1nCAgIyLK/k5OTSpQokW2fO+0T946COmcyvPPOO5o8ebI2bdqkWrVq5W/xsIuCOGd+/PFHnThxQt26dbO+n56eLklycnLSsWPH9PDDD+fzkeBucMUJ+aZkyZIKCQnJdnNzc1PTpk2VmJhos0Trd999p8TERDVr1izLfVesWFEBAQHauHGjte3GjRvatm2bdcyYMWN0+PBhxcbGWjdJmjFjhubPn19wB448s/c5I9260tSxY0e5uLhozZo1/M/wPcjFxUX169e3+a4laePGjXc8P5o2bZqp/4YNG9SgQQNrcL5TnzvtE/eOgjpnJOntt9/WpEmTtG7dOjVo0CD/i4ddFMQ5ExISoiNHjtj8u+Xxxx9X27ZtFRsbq3LlyhXY8SCP7LQoBR5wnTt3NmrVqmXs3r3b2L17t1GzZs1MS0sHBwcbq1atsr6eOnWq4evra6xatco4cuSI8eSTT95xOfIMYlW9+0ZBnDNJSUlG48aNjZo1axrHjx83zp49a93S0tIK9fhwdzKWCZ47d65x9OhRY/jw4Yanp6dx4sQJwzAMY8yYMUbfvn2t/TOWCX7ppZeMo0ePGnPnzs20TPDOnTsNR0dHY+rUqUZcXJwxdepUliO/jxTEOTNt2jTDxcXF+Pzzz23+Prl8+XKhHx/yX0GcM7djVb2ijeAEu7hw4YIRFhZmeHt7G97e3kZYWJhx6dIlmz6SjPnz51tfp6enGxMnTjQCAgIMV1dXo1WrVsaRI0eynYfgdP8oiHNm69athqQst/j4+MI5MOSbDz74wKhQoYLh4uJi1KtXz9i2bZv1vf79+xutW7e26R8TE2PUrVvXcHFxMYKCgozZs2dn2ueKFSuM4OBgw9nZ2QgJCTFWrlxZ0IeBQpTf50yFChWy/Ptk4sSJhXA0KAwF8ffMXxGcijaLYfzfU2oAAAAAgCzxjBMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAANmwWCz64osv7F0GAMDOCE4AgCIrPDxcFosl09a5c2d7lwYAeMA42bsAAACy07lzZ82fP9+mzdXV1U7VAAAeVFxxAgAUaa6urgoICLDZ/Pz8JN26jW727Nnq0qWL3N3dVbFiRa1YscJm/JEjR/TII4/I3d1dJUqU0ODBg3XlyhWbPvPmzVP16tXl6uqqwMBADR061Ob9P/74Q3/729/k4eGhKlWqaM2aNdb3Ll26pLCwMJUqVUru7u6qUqVKpqAHALj3EZwAAPe08ePHq1evXjp06JCefvppPfnkk4qLi5MkJScnq3PnzvLz89O+ffu0YsUKbdq0ySYYzZ49W5GRkRo8eLCOHDmiNWvWqHLlyjZzvPbaa+rdu7cOHz6sRx99VGFhYbp48aJ1/qNHj+qbb75RXFycZs+erZIlSxbeBwAAKBQWwzAMexcBAEBWwsPD9emnn8rNzc2mffTo0Ro/frwsFoueffZZzZ492/pekyZNVK9ePX344YeaM2eORo8erdOnT8vT01OStHbtWnXr1k1nzpxR6dKl9dBDD2nAgAF64403sqzBYrHon//8pyZNmiRJunr1qry9vbV27Vp17txZjz/+uEqWLKl58+YV0KcAACgKeMYJAFCktW3b1iYYSVLx4sWtPzdt2tTmvaZNmyo2NlaSFBcXp9q1a1tDkyQ1b95c6enpOnbsmCwWi86cOaN27dplW0OtWrWsP3t6esrb21vnzp2TJD333HPq1auXDh48qI4dO6pHjx5q1qxZno4VAFB0EZwAAEWap6dnplvnzFgsFkmSYRjWn7Pq4+7unqP9OTs7Zxqbnp4uSerSpYtOnjypr7/+Wps2bVK7du0UGRmpd955J1c1AwCKNp5xAgDc0/bs2ZPpdUhIiCQpNDRUsbGxunr1qvX9nTt3ysHBQVWrVpW3t7eCgoK0efPmu6qhVKlS1tsKZ86cqY8//viu9gcAKHq44gQAKNJSUlKUkJBg0+bk5GRdgGHFihVq0KCBWrRoocWLF2vv3r2aO3euJCksLEwTJ05U//79FRUVpfPnz2vYsGHq27evSpcuLUmKiorSs88+K39/f3Xp0kWXL1/Wzp07NWzYsBzVN2HCBNWvX1/Vq1dXSkqKvvrqK1WrVi0fPwEAQFFAcAIAFGnr1q1TYGCgTVtwcLD++9//Srq14t2yZcv0/PPPKyAgQIsXL1ZoaKgkycPDQ+vXr9eLL76ohg0bysPDQ7169dL06dOt++rfv7+uX7+uGTNm6OWXX1bJkiX197//Pcf1ubi4aOzYsTpx4oTc3d3VsmVLLVu2LB+OHABQlLCqHgDgnmWxWLR69Wr16NHD3qUAAO5zPOMEAAAAACYITgAAAABggmecAAD3LO42BwAUFq44AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmPh/Zpz5EQ8PeoAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 1...\n",
      "Prefix length: 1\n",
      "Preparing data...\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Prefix length: 2\n",
      "Preparing data...\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Prefix length: 3\n",
      "Preparing data...\n",
      "28/28 [==============================] - 0s 2ms/step\n",
      "Prefix length: 4\n",
      "Preparing data...\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "Prefix length: 5\n",
      "Preparing data...\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Prefix length: 6\n",
      "Preparing data...\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Prefix length: 7\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Prefix length: 8\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Prefix length: 9\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Prefix length: 10\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Prefix length: 11\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Prefix length: 12\n",
      "Preparing data...\n",
      "Prefix length: 13\n",
      "Preparing data...\n",
      "Prefix length: 14\n",
      "Preparing data...\n",
      "Results for ('concept_name', 'next')\n",
      "                k    weight  accuracy    fscore  precision    recall\n",
      "0               1  0.274745  0.853712  0.809355   0.830355  0.853712\n",
      "1               2  0.274445  0.676503  0.573286   0.504813  0.676503\n",
      "2               3  0.262448  0.805714  0.753495   0.789519  0.805714\n",
      "3               4  0.111878  0.820375  0.772228   0.764764  0.820375\n",
      "4               5  0.046191  0.818182  0.753346   0.698402  0.818182\n",
      "5               6  0.017996  0.766667  0.697373   0.639580  0.766667\n",
      "6               7  0.007199  0.708333  0.628489   0.565104  0.708333\n",
      "7               8  0.003299  0.727273  0.675325   0.651515  0.727273\n",
      "8               9    0.0012  0.750000  0.750000   0.750000  0.750000\n",
      "9              10    0.0003  1.000000  1.000000   1.000000  1.000000\n",
      "10             11    0.0003  1.000000  1.000000   1.000000  1.000000\n",
      "11  Weighted Mean            0.784043  0.719449   0.710934  0.784043\n",
      "Results for ('time_timestamp', 'next')\n",
      "                k    weight        mae         mse       rmse        r2\n",
      "0               1  0.274745   3.940291   46.322269   6.806046  0.021498\n",
      "1               2  0.274445   4.637068   78.215157   8.843933  0.149980\n",
      "2               3  0.262448   7.856273  130.604477  11.428231  0.538951\n",
      "3               4  0.111878   6.428979   95.401871   9.767388  0.533781\n",
      "4               5  0.046191   6.928314   94.272438   9.709399  0.479799\n",
      "5               6  0.017996   8.074788  127.907623  11.309626  0.266473\n",
      "6               7  0.007199   5.457251   47.246731   6.873626  0.597805\n",
      "7               8  0.003299   7.244454  107.107864  10.349293  0.353678\n",
      "8               9    0.0012  12.737377  194.635742  13.951192 -3.783674\n",
      "9              10    0.0003   4.781806   22.865664   4.781806       NaN\n",
      "10             11    0.0003  12.006304  144.151337  12.006304       NaN\n",
      "11  Weighted Mean             5.685159   86.776234   9.146581       NaN\n",
      "_____________________________________________\n",
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 2ms/step\n",
      "Evaluating model 2...\n",
      "Prefix length: 1\n",
      "Preparing data...\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Prefix length: 2\n",
      "Preparing data...\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Prefix length: 3\n",
      "Preparing data...\n",
      "28/28 [==============================] - 0s 2ms/step\n",
      "Prefix length: 4\n",
      "Preparing data...\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "Prefix length: 5\n",
      "Preparing data...\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Prefix length: 6\n",
      "Preparing data...\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Prefix length: 7\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Prefix length: 8\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Prefix length: 9\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Prefix length: 10\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Prefix length: 11\n",
      "Preparing data...\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Prefix length: 12\n",
      "Preparing data...\n",
      "Prefix length: 13\n",
      "Preparing data...\n",
      "Prefix length: 14\n",
      "Preparing data...\n",
      "Results for ('concept_name', 'next')\n",
      "                k    weight  accuracy    fscore  precision    recall\n",
      "0               1  0.274745  0.832969  0.766087   0.719251  0.832969\n",
      "1               2  0.274445  0.696175  0.592615   0.517610  0.696175\n",
      "2               3  0.262448  0.806857  0.770117   0.746688  0.806857\n",
      "3               4  0.111878  0.820375  0.783530   0.750097  0.820375\n",
      "4               5  0.046191  0.792208  0.748959   0.740710  0.792208\n",
      "5               6  0.017996  0.766667  0.720992   0.699450  0.766667\n",
      "6               7  0.007199  0.750000  0.701978   0.707961  0.750000\n",
      "7               8  0.003299  0.727273  0.696970   0.672727  0.727273\n",
      "8               9    0.0012  0.750000  0.750000   0.750000  0.750000\n",
      "9              10    0.0003  1.000000  1.000000   1.000000  1.000000\n",
      "10             11    0.0003  1.000000  1.000000   1.000000  1.000000\n",
      "11  Weighted Mean            0.783143  0.719316   0.675169  0.783143\n",
      "Results for ('time_timestamp', 'next')\n",
      "                k    weight        mae         mse       rmse        r2\n",
      "0               1  0.274745   3.512128   49.890926   7.063351 -0.053885\n",
      "1               2  0.274445   4.487971   78.413773   8.855155  0.147821\n",
      "2               3  0.262448   7.796214  131.432632  11.464407  0.536027\n",
      "3               4  0.111878   5.890086   85.265297   9.233921  0.583317\n",
      "4               5  0.046191   6.890668   94.244156   9.707943  0.479955\n",
      "5               6  0.017996   7.255130  115.147827  10.730696  0.339648\n",
      "6               7  0.007199   4.943476   50.830898   7.129579  0.567294\n",
      "7               8  0.003299   7.874295  145.151672  12.047891  0.124110\n",
      "8               9    0.0012  14.186365  238.027100  15.428127 -4.850128\n",
      "9              10    0.0003   2.538241    6.442668   2.538241       NaN\n",
      "10             11    0.0003  12.758801  162.786987  12.758801       NaN\n",
      "11  Weighted Mean             5.433733   86.867612   9.168451       NaN\n",
      "_____________________________________________\n",
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/home/vzimmer/miniconda3/envs/master_thesis_env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 2ms/step\n",
      "Histories and results saved to datasets/helpdesk/results/20240923-220552\n",
      "\n",
      "======================================\n",
      "======================================\n",
      "\n",
      "  _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
      " |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
      " |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
      " |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
      " |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
      " |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
      " |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
      "                                                                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"model_epochs\": 1,\n",
    "        \"warmup_epochs\": 0,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"Activity\", \"next\"): Target.NEXT_FEATURE, (\"Complete Timestamp\", \"next\"): Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"Activity\", \"Complete Timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2012 = {\n",
    "        \"dataset_name\": \"bpi_2012\",\n",
    "        \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2020 = {\n",
    "        \"dataset_name\": \"bpi_2020\",\n",
    "        \"filepath\": \"InternationalDeclarations.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "# [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "\n",
    "processing_queue = [args_helpdesk]\n",
    "for dataset in processing_queue:\n",
    "    dataset_name = dataset[\"dataset_name\"]\n",
    "    run(f\"5_cross_val_{dataset_name}\", dataset)\n",
    "    print(\n",
    "    \"\"\"\n",
    "  _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
    " |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
    " |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
    " |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
    " |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
    " |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
    " |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
    "                                                                        \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Activity\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\", \"Complete Timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_sepsis = {\n",
    "#         \"dataset_name\": \"sepsis\",\n",
    "#         \"filepath\": \"sepsis.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2012 = {\n",
    "#         \"dataset_name\": \"bpi_2012\",\n",
    "#         \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2013 = {\n",
    "#         \"dataset_name\": \"bpi_2013\",\n",
    "#         \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2015_1 = {\n",
    "#         \"dataset_name\": \"bpi_2015_1\",\n",
    "#         \"filepath\": \"BPIC15_1.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2020 = {\n",
    "#         \"dataset_name\": \"bpi_2020\",\n",
    "#         \"filepath\": \"InternationalDeclarations.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# # [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "\n",
    "# processing_queue = [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "# for dataset in processing_queue:\n",
    "#     dataset_name = dataset[\"dataset_name\"]\n",
    "#     run(f\"3_holdout_{dataset_name}\", dataset)\n",
    "#     print(\n",
    "#     \"\"\"\n",
    "#   _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
    "#  |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
    "#  |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
    "#  |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
    "#  |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
    "#  |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
    "#  |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
    "                                                                        \n",
    "# \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_Val Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Activity\": Target.NEXT_FEATURE, \"Activity\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\", \"Complete Timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_sepsis = {\n",
    "#         \"dataset_name\": \"sepsis\",\n",
    "#         \"filepath\": \"sepsis.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2012 = {\n",
    "#         \"dataset_name\": \"bpi_2012\",\n",
    "#         \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2013 = {\n",
    "#         \"dataset_name\": \"bpi_2013\",\n",
    "#         \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2015_1 = {\n",
    "#         \"dataset_name\": \"bpi_2015_1\",\n",
    "#         \"filepath\": \"BPIC15_1.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# args_bpi_2020 = {\n",
    "#         \"dataset_name\": \"bpi_2020\",\n",
    "#         \"filepath\": \"InternationalDeclarations.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": False,\n",
    "#         \"cross_val\": True\n",
    "#         }\n",
    "\n",
    "# # [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "\n",
    "# processing_queue = [args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "# for dataset in processing_queue:\n",
    "#     dataset_name = dataset[\"dataset_name\"]\n",
    "#     run(f\"5_cross_val_{dataset_name}\", dataset)\n",
    "#     print(\n",
    "#     \"\"\"\n",
    "#   _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
    "#  |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
    "#  |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
    "#  |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
    "#  |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
    "#  |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
    "#  |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
    "                                                                        \n",
    "# \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# args_helpdesk = {\n",
    "#         \"dataset_name\": \"helpdesk\",\n",
    "#         \"filepath\": \"helpdesk.csv\",\n",
    "#         \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"Activity\": Target.NEXT_FEATURE, \"Activity\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"Activity\", \"Complete Timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_sepsis = {\n",
    "#         \"dataset_name\": \"sepsis\",\n",
    "#         \"filepath\": \"sepsis.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2012 = {\n",
    "#         \"dataset_name\": \"bpi_2012\",\n",
    "#         \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2013 = {\n",
    "#         \"dataset_name\": \"bpi_2013\",\n",
    "#         \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2015_1 = {\n",
    "#         \"dataset_name\": \"bpi_2015_1\",\n",
    "#         \"filepath\": \"BPIC15_1.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# args_bpi_2020 = {\n",
    "#         \"dataset_name\": \"bpi_2020\",\n",
    "#         \"filepath\": \"InternationalDeclarations.xes\",\n",
    "#         \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "#         \"additional_columns\": {},\n",
    "#         \"datetime_format\": None,\n",
    "#         \"model_epochs\": 100,\n",
    "#         \"warmup_epochs\": 10,\n",
    "#         \"model_num_layers\": 1,\n",
    "#         \"target_columns\": {\"concept:nam\": Target.NEXT_FEATURE, \"concept:name\": Target.NEXT_FEATURE},\n",
    "#         \"input_columns\": [\"concept:name\", \"time:timestamp\"],\n",
    "#         \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "#         \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "#         \"sorting\": True,\n",
    "#         \"cross_val\": False\n",
    "#         }\n",
    "\n",
    "# # [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "\n",
    "# processing_queue = [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "# for dataset in processing_queue:\n",
    "#     dataset_name = dataset[\"dataset_name\"]\n",
    "#     run(f\"5_holdout_{dataset_name}\", dataset)\n",
    "#     print(\n",
    "#     \"\"\"\n",
    "#   _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
    "#  |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
    "#  |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
    "#  |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
    "#  |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
    "#  |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
    "#  |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
    "                                                                        \n",
    "# \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
      " |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
      " |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
      " |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
      " |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
      " |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
      " |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
      "                                                                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "  _______  __    _  _______  _______  _______  __   __  _______  _______ \n",
    " |       ||  |  | ||       ||       ||       ||  | |  ||       ||       |\n",
    " |    ___||   |_| ||  _____||  _____||   _   ||  |_|  ||    ___||   _   |\n",
    " |   |___ |       || |_____ | |_____ |  | |  ||       ||   |___ |  | |  |\n",
    " |    ___||  _    ||_____  ||_____  ||  |_|  ||       ||    ___||  |_|  |\n",
    " |   |___ | | |   | _____| | _____| ||       ||   _   ||   |___ |       |\n",
    " |_______||_|  |__||_______||_______||_______||__| |__||_______||_______|\n",
    "                                                                        \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_env",
   "language": "python",
   "name": "master_thesis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
