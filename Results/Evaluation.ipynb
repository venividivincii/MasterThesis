{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crossval_histories(dataset):\n",
    "    \"\"\"\n",
    "    Plots the cross-validation histories and includes the averaged test set metrics in the plots.\n",
    "    Handles cases where there is only one target feature without task names in the column names.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: Path object representing the dataset directory containing the history CSV files.\n",
    "    \"\"\"\n",
    "    # Define known metrics\n",
    "    known_metrics = ['loss', 'mean_absolute_error', 'sparse_categorical_accuracy']\n",
    "    \n",
    "    # Mapping of history metric names to averaged results metric names\n",
    "    metric_name_mapping = {\n",
    "        'mean_absolute_error': 'mae',\n",
    "        'sparse_categorical_accuracy': 'accuracy',\n",
    "        # Add other mappings if necessary\n",
    "    }\n",
    "    \n",
    "    # Get histories and determine the minimum length\n",
    "    histories = []\n",
    "    min_length = float('inf')\n",
    "    for idx in range(5):\n",
    "        history_path = dataset / f\"history_{idx+1}.csv\"\n",
    "        history = pd.read_csv(history_path)\n",
    "        histories.append(history)\n",
    "        if len(history) < min_length:\n",
    "            min_length = len(history)\n",
    "    \n",
    "    # Assuming all histories have the same structure\n",
    "    columns = histories[0].columns\n",
    "    \n",
    "    # Create a mapping from task names to their corresponding averaged results files\n",
    "    result_files = list(dataset.glob(\"averaged_results_*.csv\"))\n",
    "    task_to_result_file = {}\n",
    "    task_names_in_results = []\n",
    "\n",
    "    for result_file in result_files:\n",
    "        base_name = result_file.name\n",
    "        parts = base_name.split(\"averaged_results_\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        task_name_with_ext = parts[1]\n",
    "        task_name = task_name_with_ext.replace(\".csv\", \"\")\n",
    "        # Map the task name to the result file\n",
    "        task_to_result_file[task_name] = result_file\n",
    "        task_names_in_results.append(task_name)\n",
    "\n",
    "    if len(task_names_in_results) == 1:\n",
    "        single_task_name = task_names_in_results[0]\n",
    "    else:\n",
    "        single_task_name = 'main'  # Default task name\n",
    "\n",
    "    # Initialize data structures\n",
    "    metrics_data = {}  # {task_name: {'metrics': {metric_name: [data]}, 'loss': [data], 'val_metrics': {}, 'val_loss': [], 'test_metrics': {}}}\n",
    "\n",
    "    # For each column, parse and collect data\n",
    "    for col in columns:\n",
    "        is_val = col.startswith('val_')\n",
    "        col_base = col[4:] if is_val else col\n",
    "\n",
    "        if col_base in ['loss', 'lr']:\n",
    "            # Handle overall loss and learning rate separately\n",
    "            if col_base == 'loss':\n",
    "                task_name = 'overall'\n",
    "                metric_name = 'loss'\n",
    "                if task_name not in metrics_data:\n",
    "                    metrics_data[task_name] = {'metrics': {}, 'loss': [], 'val_metrics': {}, 'val_loss': [], 'test_metrics': {}}\n",
    "                # Collect data\n",
    "                for history in histories:\n",
    "                    data = history[col][:min_length].values\n",
    "                    if is_val:\n",
    "                        metrics_data[task_name]['val_loss'].append(data)\n",
    "                    else:\n",
    "                        metrics_data[task_name]['loss'].append(data)\n",
    "            # Skip 'lr' for now\n",
    "            continue\n",
    "        else:\n",
    "            # Extract task and metric\n",
    "            # Handle the case when there is only one target feature and no task name in the column name\n",
    "            for metric in known_metrics:\n",
    "                if col_base == metric:\n",
    "                    task_name = single_task_name  # Use the single task name\n",
    "                    metric_name = metric\n",
    "                    break\n",
    "                elif col_base.endswith(metric):\n",
    "                    task_name = col_base[:-len(metric)].rstrip('_')\n",
    "                    metric_name = metric\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Could not parse column '{col}'\")\n",
    "                continue\n",
    "\n",
    "            # Remove 'output_' prefix from task name\n",
    "            if task_name.startswith('output_'):\n",
    "                task_name = task_name[len('output_'):]\n",
    "            # Initialize data structures if needed\n",
    "            if task_name not in metrics_data:\n",
    "                metrics_data[task_name] = {'metrics': {}, 'loss': [], 'val_metrics': {}, 'val_loss': [], 'test_metrics': {}}\n",
    "\n",
    "            # Collect data\n",
    "            for history in histories:\n",
    "                data = history[col][:min_length].values\n",
    "                if is_val:\n",
    "                    if metric_name == 'loss':\n",
    "                        metrics_data[task_name]['val_loss'].append(data)\n",
    "                    else:\n",
    "                        if metric_name not in metrics_data[task_name]['val_metrics']:\n",
    "                            metrics_data[task_name]['val_metrics'][metric_name] = []\n",
    "                        metrics_data[task_name]['val_metrics'][metric_name].append(data)\n",
    "                else:\n",
    "                    if metric_name == 'loss':\n",
    "                        metrics_data[task_name]['loss'].append(data)\n",
    "                    else:\n",
    "                        if metric_name not in metrics_data[task_name]['metrics']:\n",
    "                            metrics_data[task_name]['metrics'][metric_name] = []\n",
    "                        metrics_data[task_name]['metrics'][metric_name].append(data)\n",
    "\n",
    "    # Read the averaged results for each task, excluding 'overall'\n",
    "    for task in metrics_data.keys():\n",
    "        if task == 'overall':\n",
    "            continue  # Skip the 'overall' task as there is no averaged results file\n",
    "        # Find the corresponding averaged results file\n",
    "        if task in task_to_result_file:\n",
    "            averaged_results_path = task_to_result_file[task]\n",
    "            df_averaged = pd.read_csv(averaged_results_path)\n",
    "            # Get the 'Weighted Mean' row\n",
    "            weighted_mean_row = df_averaged[df_averaged['k'] == 'Weighted Mean']\n",
    "            if not weighted_mean_row.empty:\n",
    "                # Identify metric columns dynamically\n",
    "                metric_columns = [col for col in df_averaged.columns if col not in ['k', 'weight']]\n",
    "                test_metrics = {}\n",
    "                for metric in metric_columns:\n",
    "                    value = weighted_mean_row.iloc[0][metric]\n",
    "                    if pd.notna(value):\n",
    "                        test_metrics[metric] = float(value)\n",
    "                # Store the test metrics\n",
    "                metrics_data[task]['test_metrics'] = test_metrics\n",
    "            else:\n",
    "                print(f\"'Weighted Mean' row not found in averaged results for task '{task}'.\")\n",
    "        else:\n",
    "            print(f\"Averaged results file for task '{task}' not found in '{dataset}'.\")\n",
    "\n",
    "    # Now plot per task\n",
    "    tasks = list(metrics_data.keys())\n",
    "    num_tasks = len(tasks)\n",
    "\n",
    "    epochs = np.arange(min_length)\n",
    "\n",
    "    # Define colors\n",
    "    training_color = 'blue'\n",
    "    validation_color = 'orange'\n",
    "    fold_color = 'gray'\n",
    "    test_color = 'red'\n",
    "\n",
    "    # Map internal task names to friendly names\n",
    "    task_name_mapping = {\n",
    "        'concept_name_next': 'Next Activity',\n",
    "        'time_timestamp_next': 'Next Time',\n",
    "        'time_timestamp_last': 'Remaining Time',\n",
    "        'overall': 'Overall'\n",
    "    }\n",
    "    # Add the single task name to the mapping if not present\n",
    "    if single_task_name not in task_name_mapping:\n",
    "        task_name_mapping[single_task_name] = 'Main Task'\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(num_tasks, 2, figsize=(12, 5 * num_tasks))\n",
    "    if num_tasks == 1:\n",
    "        axes = np.array([axes])  # Ensure axes is always 2D array\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        task_data = metrics_data[task]\n",
    "        friendly_task_name = task_name_mapping.get(task, task)\n",
    "\n",
    "        # Plot loss\n",
    "        ax_loss = axes[i, 0]\n",
    "        plotted = False  # Flag to check if any data is plotted\n",
    "        if task_data['loss']:\n",
    "            # Plot individual folds\n",
    "            for data in task_data['loss']:\n",
    "                ax_loss.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "            # Compute mean and std\n",
    "            data_array = np.array(task_data['loss'])\n",
    "            mean = np.mean(data_array, axis=0)\n",
    "            std = np.std(data_array, axis=0)\n",
    "            ax_loss.plot(epochs, mean, label='Training Loss', color=training_color, linewidth=2)\n",
    "            ax_loss.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=training_color)\n",
    "            plotted = True\n",
    "\n",
    "        if task_data['val_loss']:\n",
    "            # Plot individual folds\n",
    "            for data in task_data['val_loss']:\n",
    "                ax_loss.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "            # Compute mean and std\n",
    "            data_array = np.array(task_data['val_loss'])\n",
    "            mean = np.mean(data_array, axis=0)\n",
    "            std = np.std(data_array, axis=0)\n",
    "            ax_loss.plot(epochs, mean, label='Validation Loss', color=validation_color, linewidth=2)\n",
    "            ax_loss.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=validation_color)\n",
    "            plotted = True\n",
    "\n",
    "        if plotted:\n",
    "            # Plot test metric as a horizontal line\n",
    "            if 'loss' in task_data['test_metrics']:\n",
    "                test_loss = task_data['test_metrics']['loss']\n",
    "                ax_loss.axhline(y=test_loss, color=test_color, linestyle='-', linewidth=2, label='Test Loss')\n",
    "            ax_loss.set_title(f'{friendly_task_name} - Training and Validation Loss')\n",
    "            ax_loss.set_xlabel('Epochs')\n",
    "            ax_loss.set_ylabel('Loss')\n",
    "            ax_loss.legend(loc='upper right', fontsize='small')\n",
    "        else:\n",
    "            ax_loss.axis('off')  # Hide the axis if no data\n",
    "\n",
    "        # Plot metrics\n",
    "        ax_metric = axes[i, 1]\n",
    "        plotted = False\n",
    "        # Combine training and validation metrics to ensure consistent legends\n",
    "        all_metrics = set(task_data['metrics'].keys()).union(task_data['val_metrics'].keys())\n",
    "        for metric_name in all_metrics:\n",
    "            # Plot training metrics\n",
    "            if metric_name in task_data['metrics']:\n",
    "                if task_data['metrics'][metric_name]:\n",
    "                    # Plot individual folds\n",
    "                    for data in task_data['metrics'][metric_name]:\n",
    "                        ax_metric.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "                    # Compute mean and std\n",
    "                    data_array = np.array(task_data['metrics'][metric_name])\n",
    "                    mean = np.mean(data_array, axis=0)\n",
    "                    std = np.std(data_array, axis=0)\n",
    "                    ax_metric.plot(epochs, mean, label=f'Training {metric_name}', color=training_color, linewidth=2)\n",
    "                    ax_metric.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=training_color)\n",
    "                    plotted = True\n",
    "\n",
    "            # Plot validation metrics\n",
    "            if metric_name in task_data['val_metrics']:\n",
    "                if task_data['val_metrics'][metric_name]:\n",
    "                    # Plot individual folds\n",
    "                    for data in task_data['val_metrics'][metric_name]:\n",
    "                        ax_metric.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "                    # Compute mean and std\n",
    "                    data_array = np.array(task_data['val_metrics'][metric_name])\n",
    "                    mean = np.mean(data_array, axis=0)\n",
    "                    std = np.std(data_array, axis=0)\n",
    "                    ax_metric.plot(epochs, mean, label=f'Validation {metric_name}', color=validation_color, linewidth=2)\n",
    "                    ax_metric.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=validation_color)\n",
    "                    plotted = True\n",
    "\n",
    "            # Map the metric name to the test metric name\n",
    "            test_metric_name = metric_name_mapping.get(metric_name, metric_name)\n",
    "\n",
    "            # Plot test metric as a horizontal line\n",
    "            if test_metric_name in task_data['test_metrics']:\n",
    "                test_metric_value = task_data['test_metrics'][test_metric_name]\n",
    "                ax_metric.axhline(y=test_metric_value, color=test_color, linestyle='-', linewidth=2, label=f'Test {metric_name}')\n",
    "                plotted = True\n",
    "            else:\n",
    "                print(f\"Test metric '{test_metric_name}' not found for task '{task}'.\")\n",
    "\n",
    "        if plotted:\n",
    "            ax_metric.set_title(f'{friendly_task_name} - Training and Validation Metrics')\n",
    "            ax_metric.set_xlabel('Epochs')\n",
    "            ax_metric.set_ylabel('Metric')\n",
    "            ax_metric.legend(loc='lower right', fontsize='small')\n",
    "        else:\n",
    "            ax_metric.axis('off')  # Hide the axis if no data\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(dataset / 'train_val_history.png')\n",
    "    plt.show()\n",
    "    plt.clf()  # Clear figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_holdout_history(dataset):\n",
    "    \"\"\"\n",
    "    Plots the training and validation metrics for the holdout approach.\n",
    "    Includes test metrics from the results files in the plots.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: Path object representing the dataset directory containing the history CSV file.\n",
    "    \"\"\"\n",
    "    # Define known metrics\n",
    "    known_metrics = ['loss', 'mean_absolute_error', 'sparse_categorical_accuracy']\n",
    "    \n",
    "    # Mapping of history metric names to results metric names\n",
    "    metric_name_mapping = {\n",
    "        'mean_absolute_error': 'mae',\n",
    "        'sparse_categorical_accuracy': 'accuracy',\n",
    "        # Add other mappings if necessary\n",
    "    }\n",
    "    \n",
    "    # Read the history CSV file\n",
    "    history_path = dataset / \"history_1.csv\"\n",
    "    history = pd.read_csv(history_path)\n",
    "    \n",
    "    # Assuming the history has the same structure as in cross-validation\n",
    "    columns = history.columns\n",
    "\n",
    "    # Create a mapping from task names to their corresponding result files\n",
    "    result_files = list(dataset.glob(\"results_*__*.csv\"))\n",
    "    task_to_result_file = {}\n",
    "    task_names_in_results = []\n",
    "\n",
    "    for result_file in result_files:\n",
    "        base_name = result_file.name\n",
    "        parts = base_name.split(\"__\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        task_name_with_ext = parts[1]\n",
    "        task_name = task_name_with_ext.replace(\".csv\", \"\")\n",
    "        # Remove parentheses and quotes from task name\n",
    "        task_name = task_name.strip(\"()'\\\"\").replace(\"', '\", \"_\").replace(\" \", \"_\")\n",
    "        # Map the task name to the result file\n",
    "        task_to_result_file[task_name] = result_file\n",
    "        task_names_in_results.append(task_name)\n",
    "\n",
    "    if len(task_names_in_results) == 1:\n",
    "        single_task_name = task_names_in_results[0]\n",
    "    else:\n",
    "        single_task_name = 'main'  # Default task name\n",
    "\n",
    "    # Initialize data structures\n",
    "    metrics_data = {}  # {task_name: {'metrics': {metric_name: data}, 'loss': data, 'val_metrics': {}, 'val_loss': data, 'test_metrics': {}}}\n",
    "\n",
    "    # For each column, parse and collect data\n",
    "    for col in columns:\n",
    "        is_val = col.startswith('val_')\n",
    "        col_base = col[4:] if is_val else col\n",
    "\n",
    "        if col_base in ['loss', 'lr']:\n",
    "            # Handle overall loss and learning rate separately\n",
    "            if col_base == 'loss':\n",
    "                task_name = 'overall'\n",
    "                metric_name = 'loss'\n",
    "                if task_name not in metrics_data:\n",
    "                    metrics_data[task_name] = {'metrics': {}, 'loss': None, 'val_metrics': {}, 'val_loss': None, 'test_metrics': {}}\n",
    "                # Collect data\n",
    "                data = history[col].values\n",
    "                if is_val:\n",
    "                    metrics_data[task_name]['val_loss'] = data\n",
    "                else:\n",
    "                    metrics_data[task_name]['loss'] = data\n",
    "            # Skip 'lr' for now\n",
    "            continue\n",
    "        else:\n",
    "            # Extract task and metric\n",
    "            # Handle the case when there is only one target feature and no task name in the column name\n",
    "            for metric in known_metrics:\n",
    "                if col_base == metric:\n",
    "                    task_name = single_task_name  # Use the single task name\n",
    "                    metric_name = metric\n",
    "                    break\n",
    "                elif col_base.endswith(metric):\n",
    "                    task_name = col_base[:-len(metric)].rstrip('_')\n",
    "                    metric_name = metric\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Could not parse column '{col}'\")\n",
    "                continue\n",
    "\n",
    "            # Remove 'output_' prefix from task name\n",
    "            if task_name.startswith('output_'):\n",
    "                task_name = task_name[len('output_'):]\n",
    "            # Initialize data structures if needed\n",
    "            if task_name not in metrics_data:\n",
    "                metrics_data[task_name] = {'metrics': {}, 'loss': None, 'val_metrics': {}, 'val_loss': None, 'test_metrics': {}}\n",
    "\n",
    "            # Collect data\n",
    "            data = history[col].values\n",
    "            if is_val:\n",
    "                if metric_name == 'loss':\n",
    "                    metrics_data[task_name]['val_loss'] = data\n",
    "                else:\n",
    "                    metrics_data[task_name]['val_metrics'][metric_name] = data\n",
    "            else:\n",
    "                if metric_name == 'loss':\n",
    "                    metrics_data[task_name]['loss'] = data\n",
    "                else:\n",
    "                    metrics_data[task_name]['metrics'][metric_name] = data\n",
    "\n",
    "    # Read the results for each task, excluding 'overall'\n",
    "    for task in metrics_data.keys():\n",
    "        if task == 'overall':\n",
    "            continue  # Skip the 'overall' task as there is no separate results file\n",
    "        # Find the corresponding result file\n",
    "        if task in task_to_result_file:\n",
    "            results_path = task_to_result_file[task]\n",
    "            df_results = pd.read_csv(results_path)\n",
    "            # Exclude 'Weighted Mean' row if present\n",
    "            if 'k' in df_results.columns and 'Weighted Mean' in df_results['k'].values:\n",
    "                df_results = df_results[df_results['k'] != 'Weighted Mean']\n",
    "\n",
    "            # Identify metric columns dynamically\n",
    "            metric_columns = [col for col in df_results.columns if col not in ['k', 'weight']]\n",
    "\n",
    "            # Compute the weighted mean metrics\n",
    "            if 'weight' in df_results.columns:\n",
    "                total_weight = df_results['weight'].sum()\n",
    "                test_metrics = {}\n",
    "                for metric in metric_columns:\n",
    "                    valid = df_results[metric].notna()\n",
    "                    if valid.any():\n",
    "                        weighted_metric = (df_results.loc[valid, metric] * df_results.loc[valid, 'weight']).sum() / total_weight\n",
    "                        test_metrics[metric] = weighted_metric\n",
    "                # Store the test metrics\n",
    "                metrics_data[task]['test_metrics'] = test_metrics\n",
    "            else:\n",
    "                print(f\"No 'weight' column found in results file '{results_path.name}'. Cannot compute weighted mean.\")\n",
    "        else:\n",
    "            print(f\"Results file for task '{task}' not found in '{dataset}'.\")\n",
    "\n",
    "    # Now plot per task\n",
    "    tasks = list(metrics_data.keys())\n",
    "    num_tasks = len(tasks)\n",
    "\n",
    "    epochs = np.arange(len(history))\n",
    "\n",
    "    # Define colors\n",
    "    training_color = 'blue'\n",
    "    validation_color = 'orange'\n",
    "    test_color = 'red'  # Use red color for test metrics\n",
    "\n",
    "    # Map internal task names to friendly names\n",
    "    task_name_mapping = {\n",
    "        'concept_name_next': 'Next Activity',\n",
    "        'time_timestamp_next': 'Next Time',\n",
    "        'time_timestamp_last': 'Remaining Time',\n",
    "        'overall': 'Overall'\n",
    "    }\n",
    "    # Add the single task name to the mapping if not present\n",
    "    if single_task_name not in task_name_mapping:\n",
    "        task_name_mapping[single_task_name] = 'Main Task'\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(num_tasks, 2, figsize=(12, 5 * num_tasks))\n",
    "    if num_tasks == 1:\n",
    "        axes = np.array([axes])  # Ensure axes is always 2D array\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        task_data = metrics_data[task]\n",
    "        friendly_task_name = task_name_mapping.get(task, task)\n",
    "\n",
    "        # Plot loss\n",
    "        ax_loss = axes[i, 0]\n",
    "        plotted = False  # Flag to check if any data is plotted\n",
    "        if task_data['loss'] is not None:\n",
    "            ax_loss.plot(epochs, task_data['loss'], label='Training Loss', color=training_color, linewidth=2)\n",
    "            plotted = True\n",
    "\n",
    "        if task_data['val_loss'] is not None:\n",
    "            ax_loss.plot(epochs, task_data['val_loss'], label='Validation Loss', color=validation_color, linewidth=2)\n",
    "            plotted = True\n",
    "\n",
    "        if plotted:\n",
    "            # Plot test loss as a horizontal line\n",
    "            if 'loss' in task_data['test_metrics']:\n",
    "                test_loss = task_data['test_metrics']['loss']\n",
    "                ax_loss.axhline(y=test_loss, color=test_color, linestyle='-', linewidth=2, label='Test Loss')\n",
    "            ax_loss.set_title(f'{friendly_task_name} - Training and Validation Loss')\n",
    "            ax_loss.set_xlabel('Epochs')\n",
    "            ax_loss.set_ylabel('Loss')\n",
    "            ax_loss.legend(loc='upper right', fontsize='small')\n",
    "        else:\n",
    "            ax_loss.axis('off')  # Hide the axis if no data\n",
    "\n",
    "        # Plot metrics\n",
    "        ax_metric = axes[i, 1]\n",
    "        plotted = False\n",
    "        # Combine training and validation metrics to ensure consistent legends\n",
    "        all_metrics = set(task_data['metrics'].keys()).union(task_data['val_metrics'].keys())\n",
    "        for metric_name in all_metrics:\n",
    "            # Plot training metrics\n",
    "            if metric_name in task_data['metrics']:\n",
    "                if task_data['metrics'][metric_name] is not None:\n",
    "                    ax_metric.plot(epochs, task_data['metrics'][metric_name], label=f'Training {metric_name}', color=training_color, linewidth=2)\n",
    "                    plotted = True\n",
    "\n",
    "            # Plot validation metrics\n",
    "            if metric_name in task_data['val_metrics']:\n",
    "                if task_data['val_metrics'][metric_name] is not None:\n",
    "                    ax_metric.plot(epochs, task_data['val_metrics'][metric_name], label=f'Validation {metric_name}', color=validation_color, linewidth=2)\n",
    "                    plotted = True\n",
    "\n",
    "            # Map the metric name to the test metric name\n",
    "            test_metric_name = metric_name_mapping.get(metric_name, metric_name)\n",
    "\n",
    "            # Plot test metric as a horizontal line\n",
    "            if test_metric_name in task_data['test_metrics']:\n",
    "                test_metric_value = task_data['test_metrics'][test_metric_name]\n",
    "                ax_metric.axhline(y=test_metric_value, color=test_color, linestyle='-', linewidth=2, label=f'Test {metric_name}')\n",
    "                plotted = True\n",
    "            else:\n",
    "                print(f\"Test metric '{test_metric_name}' not found for task '{task}'.\")\n",
    "\n",
    "        if plotted:\n",
    "            ax_metric.set_title(f'{friendly_task_name} - Training and Validation Metrics')\n",
    "            ax_metric.set_xlabel('Epochs')\n",
    "            ax_metric.set_ylabel('Metric')\n",
    "            ax_metric.legend(loc='lower right', fontsize='small')\n",
    "        else:\n",
    "            ax_metric.axis('off')  # Hide the axis if no data\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.clf()  # Clear figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_results_over_folds(dataset):\n",
    "    \"\"\"\n",
    "    Averages the results over all folds and stores the result in a CSV file for each task.\n",
    "    Additionally, prints the weighted mean accuracy for classification tasks and \n",
    "    the weighted mean MAE for regression tasks.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: Path object representing the dataset directory containing the results CSV files.\n",
    "    \"\"\"\n",
    "    # Get all result files in the dataset directory\n",
    "    result_files = list(dataset.glob(\"results_*__*.csv\"))\n",
    "\n",
    "    # Dictionary to store dataframes for each task\n",
    "    task_data = {}\n",
    "\n",
    "    # Process each result file\n",
    "    for file_path in result_files:\n",
    "        # Extract task name from the filename\n",
    "        base_name = file_path.name\n",
    "        parts = base_name.split(\"__\")\n",
    "        if len(parts) < 2:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        task_name_with_ext = parts[1]  # This includes the .csv extension\n",
    "        task_name = task_name_with_ext.replace(\".csv\", \"\")\n",
    "        # Remove parentheses and quotes from task name\n",
    "        task_name = task_name.strip(\"()'\\\"\").replace(\"', '\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, index_col=False)\n",
    "\n",
    "        # Print the columns in df for debugging\n",
    "        # print(f\"Processing file: {file_path}\")\n",
    "        # print(\"Columns in df:\", df.columns.tolist())\n",
    "        # print(df.head())\n",
    "\n",
    "        # Handle 'Weighted Mean' row separately\n",
    "        weighted_mean_row = df[df['k'] == 'Weighted Mean']\n",
    "        df = df[df['k'] != 'Weighted Mean']\n",
    "\n",
    "        # Convert 'k' to numeric, handling errors\n",
    "        df['k'] = pd.to_numeric(df['k'], errors='coerce')\n",
    "        df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "\n",
    "        # Identify metric columns dynamically\n",
    "        metric_columns = [col for col in df.columns if col not in ['k', 'weight', 'fold']]\n",
    "\n",
    "        # Convert metric columns to numeric\n",
    "        df[metric_columns] = df[metric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Drop rows with NaN values in 'k' or 'weight'\n",
    "        df = df.dropna(subset=['k', 'weight'])\n",
    "\n",
    "        # Add a column for fold number\n",
    "        fold_part = parts[0]\n",
    "        fold_number = fold_part.split('_')[1]\n",
    "        df['fold'] = int(fold_number)\n",
    "\n",
    "        # Store the dataframe in the task_data dictionary\n",
    "        if task_name not in task_data:\n",
    "            task_data[task_name] = {'dfs': [], 'metric_columns': metric_columns}\n",
    "        else:\n",
    "            # Ensure metric_columns are consistent across folds\n",
    "            if set(metric_columns) != set(task_data[task_name]['metric_columns']):\n",
    "                print(f\"Warning: Metric columns differ for task {task_name} in file {file_path}\")\n",
    "        task_data[task_name]['dfs'].append(df)\n",
    "\n",
    "    # Process each task\n",
    "    for task_name, data in task_data.items():\n",
    "        dfs = data['dfs']\n",
    "        metric_columns = data['metric_columns']\n",
    "\n",
    "        # Concatenate all folds data for this task\n",
    "        df_all_folds = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Group by 'k' and compute the mean of metrics over folds\n",
    "        agg_dict = {'weight': 'first'}\n",
    "        for metric in metric_columns:\n",
    "            agg_dict[metric] = 'mean'\n",
    "\n",
    "        df_mean = df_all_folds.groupby('k').agg(agg_dict).reset_index()\n",
    "\n",
    "        # Compute the overall weighted mean for each metric\n",
    "        weighted_metrics = {}\n",
    "        for metric in metric_columns:\n",
    "            valid = df_mean[metric].notna()\n",
    "            if valid.any():\n",
    "                total_weight_metric = df_mean.loc[valid, 'weight'].sum()\n",
    "                weighted_metric = (df_mean.loc[valid, metric] * df_mean.loc[valid, 'weight']).sum() / total_weight_metric\n",
    "                weighted_metrics[metric] = weighted_metric\n",
    "            else:\n",
    "                weighted_metrics[metric] = float('nan')  # Assign NaN if all values are NaN\n",
    "\n",
    "        # Create the 'Weighted Mean' row\n",
    "        weighted_mean_row = {'k': 'Weighted Mean', 'weight': ''}\n",
    "        for metric in metric_columns:\n",
    "            weighted_mean_row[metric] = weighted_metrics[metric]\n",
    "\n",
    "        # Append the 'Weighted Mean' row to the averaged dataframe using pd.concat\n",
    "        df_mean = pd.concat([df_mean, pd.DataFrame([weighted_mean_row])], ignore_index=True)\n",
    "\n",
    "        # Reorder columns to match the original file\n",
    "        columns_order = ['k', 'weight'] + metric_columns\n",
    "        df_mean = df_mean[columns_order]\n",
    "\n",
    "        # Save the averaged results to a new CSV file in the dataset directory\n",
    "        output_file = dataset / f\"averaged_results_{task_name}.csv\"\n",
    "        df_mean.to_csv(output_file, index=False)\n",
    "\n",
    "        # Print the weighted mean accuracy or MAE\n",
    "        if 'accuracy' in metric_columns:\n",
    "            weighted_accuracy = weighted_metrics['accuracy']\n",
    "            print(f\"Accuracy '{task_name}': {weighted_accuracy:.4f}\")\n",
    "        elif 'mae' in metric_columns:\n",
    "            weighted_mae = weighted_metrics['mae']\n",
    "            print(f\"MAE '{task_name}': {weighted_mae:.4f}\")\n",
    "\n",
    "        # print(f\"Averaged results for task '{task_name}' saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_holdout_weighted_mean(dataset):\n",
    "    \"\"\"\n",
    "    Reads the results CSV files for the holdout dataset, computes the weighted mean values,\n",
    "    and prints the weighted mean accuracy (for classification tasks) or weighted mean MAE\n",
    "    (for regression tasks).\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: Path object representing the dataset directory containing the results CSV files.\n",
    "    \"\"\"\n",
    "    # Get all result files in the dataset directory\n",
    "    result_files = list(dataset.glob(\"results_*.csv\"))\n",
    "    \n",
    "    # Process each result file\n",
    "    for file_path in result_files:\n",
    "        # Extract task name from the filename\n",
    "        base_name = file_path.name\n",
    "        parts = base_name.split(\"__\")\n",
    "        if len(parts) < 2:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        task_name_with_ext = parts[1]  # This includes the .csv extension\n",
    "        task_name = task_name_with_ext.replace(\".csv\", \"\")\n",
    "        # Remove parentheses and quotes from task name\n",
    "        task_name = task_name.strip(\"()'\\\"\").replace(\"', '\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, index_col=False)\n",
    "\n",
    "        # Handle 'Weighted Mean' row separately if it exists\n",
    "        if 'k' in df.columns and 'Weighted Mean' in df['k'].values:\n",
    "            df = df[df['k'] != 'Weighted Mean']\n",
    "\n",
    "        # Convert 'k' to numeric, handling errors\n",
    "        if 'k' in df.columns:\n",
    "            df['k'] = pd.to_numeric(df['k'], errors='coerce')\n",
    "        if 'weight' in df.columns:\n",
    "            df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "\n",
    "        # Identify metric columns dynamically\n",
    "        metric_columns = [col for col in df.columns if col not in ['k', 'weight']]\n",
    "\n",
    "        # Convert metric columns to numeric\n",
    "        df[metric_columns] = df[metric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Drop rows with NaN values in 'k' or 'weight'\n",
    "        if 'k' in df.columns and 'weight' in df.columns:\n",
    "            df = df.dropna(subset=['k', 'weight'])\n",
    "\n",
    "        # Compute the overall weighted mean for each metric\n",
    "        weighted_metrics = {}\n",
    "        if 'weight' in df.columns:\n",
    "            total_weight = df['weight'].sum()\n",
    "            for metric in metric_columns:\n",
    "                valid = df[metric].notna()\n",
    "                if valid.any():\n",
    "                    weighted_metric = (df.loc[valid, metric] * df.loc[valid, 'weight']).sum() / total_weight\n",
    "                    weighted_metrics[metric] = weighted_metric\n",
    "                else:\n",
    "                    weighted_metrics[metric] = float('nan')  # Assign NaN if all values are NaN\n",
    "\n",
    "            # Print the weighted mean accuracy or MAE\n",
    "            if 'accuracy' in metric_columns:\n",
    "                weighted_accuracy = weighted_metrics['accuracy']\n",
    "                print(f\"Accuracy {task_name}: {weighted_accuracy:.4f}\")\n",
    "            elif 'mae' in metric_columns:\n",
    "                weighted_mae = weighted_metrics['mae']\n",
    "                print(f\"MAE {task_name}: {weighted_mae:.4f}\")\n",
    "            else:\n",
    "                # If other metrics are present, you can adjust this section to print them\n",
    "                for metric in metric_columns:\n",
    "                    weighted_value = weighted_metrics[metric]\n",
    "                    print(f\"{metric} {task_name}: {weighted_value:.4f}\")\n",
    "        else:\n",
    "            print(f\"No 'weight' column found in file {file_path}. Cannot compute weighted mean.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rename_file(dataset):\n",
    "#     concept_name_target = \"next\"\n",
    "#     time_timestamp_target = \"next\"\n",
    "    \n",
    "    \n",
    "#     for file in dataset.iterdir():\n",
    "#         if \"download\" in file.name:\n",
    "#             file.unlink()\n",
    "#         # print(file.name)\n",
    "#         elif \"results_\" in file.name or \"predictions_\" in file.name:\n",
    "#             if \"averaged\" in file.name:\n",
    "#                 file.unlink()\n",
    "#             else:\n",
    "#                 # print(file.name)\n",
    "#                 name_parts = file.name.split(\"__\")\n",
    "#                 if \"concept_name\" in name_parts[1]:\n",
    "#                     name_parts[1] = f\"('concept_name', '{concept_name_target}').csv\"\n",
    "#                 elif \"time_timestamp\" in name_parts[1]:\n",
    "#                     name_parts[1] = f\"('time_timestamp', '{time_timestamp_target}').csv\"\n",
    "#                 new_file = Path(name_parts[0]+ \"__\" + name_parts[1])\n",
    "#                 new_file_path = dataset / new_file\n",
    "                \n",
    "#                 if not (new_file_path.exists() and new_file_path.is_file()):\n",
    "#                     file.rename(new_file_path)\n",
    "            \n",
    "#             # print(new_filename)\n",
    "#             # rename file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path.cwd()  # Get the current directory\n",
    "\n",
    "for job_idx, job in enumerate(current_dir.iterdir()):\n",
    "    if job.is_dir() and job_idx != 0:\n",
    "        if job == current_dir / \"15\":  # TODO: for testing\n",
    "            print(f\"########################## {job.name} ##########################\")\n",
    "            # cross_val and holdout\n",
    "            for app_idx, approach in enumerate(job.iterdir()):\n",
    "                # cross_val\n",
    "                if app_idx == 0:\n",
    "                    print(f\"############# cross_val #############\")\n",
    "                    # loop over datasets in approach\n",
    "                    for dataset in approach.iterdir():\n",
    "                        print(f\"----- {dataset.name} -----\")\n",
    "                        # Average results over folds\n",
    "                        average_results_over_folds(dataset)\n",
    "                        # Plot histories\n",
    "                        plot_crossval_histories(dataset)\n",
    "                        break\n",
    "                        \n",
    "                # holdout\n",
    "                elif app_idx == 1:\n",
    "                    print(f\"############# holdout #############\")\n",
    "                    # loop over datasets in approach\n",
    "                    for dataset in approach.iterdir():\n",
    "                        print(f\"----- {dataset.name} -----\")\n",
    "                        # Print weighted mean values for holdout\n",
    "                        print_holdout_weighted_mean(dataset)\n",
    "                        # Plot history for holdout\n",
    "                        plot_holdout_history(dataset)\n",
    "                        break\n",
    "                # if there are more dirs\n",
    "                else:\n",
    "                    raise ValueError(f\"Too many approach folders in {job}. Expected 2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
