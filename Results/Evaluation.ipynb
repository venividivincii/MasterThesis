{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crossval_histories(dataset):\n",
    "\n",
    "    # Define known metrics\n",
    "    known_metrics = ['loss', 'mean_absolute_error', 'sparse_categorical_accuracy']\n",
    "\n",
    "    # Get histories and determine the minimum length\n",
    "    histories = []\n",
    "    min_length = float('inf')\n",
    "    for idx in range(5):\n",
    "        history_path = dataset / f\"history_{idx+1}.csv\"\n",
    "        history = pd.read_csv(history_path)\n",
    "        histories.append(history)\n",
    "        if len(history) < min_length:\n",
    "            min_length = len(history)\n",
    "\n",
    "    # Assuming all histories have the same structure\n",
    "    columns = histories[0].columns\n",
    "\n",
    "    # Initialize data structures\n",
    "    metrics_data = {}  # {task_name: {'metrics': {metric_name: [data]}, 'loss': [data], 'val_metrics': {}, 'val_loss': []}}\n",
    "\n",
    "    # For each column, parse and collect data\n",
    "    for col in columns:\n",
    "        is_val = col.startswith('val_')\n",
    "        col_base = col[4:] if is_val else col\n",
    "\n",
    "        if col_base in ['loss', 'lr']:\n",
    "            # Handle overall loss and learning rate separately\n",
    "            if col_base == 'loss':\n",
    "                task_name = 'overall'\n",
    "                metric_name = 'loss'\n",
    "                if task_name not in metrics_data:\n",
    "                    metrics_data[task_name] = {'metrics': {}, 'loss': [], 'val_metrics': {}, 'val_loss': []}\n",
    "                # Collect data\n",
    "                for history in histories:\n",
    "                    data = history[col][:min_length].values\n",
    "                    if is_val:\n",
    "                        metrics_data[task_name]['val_loss'].append(data)\n",
    "                    else:\n",
    "                        metrics_data[task_name]['loss'].append(data)\n",
    "            # Skip 'lr' for now\n",
    "            continue\n",
    "        else:\n",
    "            # Extract task and metric\n",
    "            for metric in known_metrics:\n",
    "                if col_base.endswith(metric):\n",
    "                    task_name = col_base[:-len(metric)].rstrip('_')\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Could not parse column '{col}'\")\n",
    "                continue\n",
    "\n",
    "            # Remove 'output_' prefix from task name\n",
    "            if task_name.startswith('output_'):\n",
    "                task_name = task_name[len('output_'):]\n",
    "            metric_name = metric  # The metric found in the loop\n",
    "\n",
    "            # Initialize data structures if needed\n",
    "            if task_name not in metrics_data:\n",
    "                metrics_data[task_name] = {'metrics': {}, 'loss': [], 'val_metrics': {}, 'val_loss': []}\n",
    "\n",
    "            # Collect data\n",
    "            for history in histories:\n",
    "                data = history[col][:min_length].values\n",
    "                if is_val:\n",
    "                    if metric_name == 'loss':\n",
    "                        metrics_data[task_name]['val_loss'].append(data)\n",
    "                    else:\n",
    "                        if metric_name not in metrics_data[task_name]['val_metrics']:\n",
    "                            metrics_data[task_name]['val_metrics'][metric_name] = []\n",
    "                        metrics_data[task_name]['val_metrics'][metric_name].append(data)\n",
    "                else:\n",
    "                    if metric_name == 'loss':\n",
    "                        metrics_data[task_name]['loss'].append(data)\n",
    "                    else:\n",
    "                        if metric_name not in metrics_data[task_name]['metrics']:\n",
    "                            metrics_data[task_name]['metrics'][metric_name] = []\n",
    "                        metrics_data[task_name]['metrics'][metric_name].append(data)\n",
    "\n",
    "    # Now plot per task\n",
    "    tasks = list(metrics_data.keys())\n",
    "    num_tasks = len(tasks)\n",
    "\n",
    "    epochs = np.arange(min_length)\n",
    "\n",
    "    # Define colors\n",
    "    training_color = 'blue'\n",
    "    validation_color = 'orange'\n",
    "    fold_color = 'gray'\n",
    "\n",
    "    # Map internal task names to friendly names\n",
    "    task_name_mapping = {\n",
    "        'concept_name_next': 'Next Activity',\n",
    "        'time_timestamp_next': 'Next Time',\n",
    "        'time_timestamp_last': 'Remaining Time',\n",
    "        'overall': 'Overall'\n",
    "    }\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(num_tasks, 2, figsize=(12, 5 * num_tasks))\n",
    "    if num_tasks == 1:\n",
    "        axes = np.array([axes])  # Ensure axes is always 2D array\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        task_data = metrics_data[task]\n",
    "        friendly_task_name = task_name_mapping.get(task, task)\n",
    "\n",
    "        # Plot loss\n",
    "        ax_loss = axes[i, 0]\n",
    "        plotted = False  # Flag to check if any data is plotted\n",
    "        if task_data['loss']:\n",
    "            # Plot individual folds\n",
    "            for data in task_data['loss']:\n",
    "                ax_loss.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "            # Compute mean and std\n",
    "            data_array = np.array(task_data['loss'])\n",
    "            mean = np.mean(data_array, axis=0)\n",
    "            std = np.std(data_array, axis=0)\n",
    "            ax_loss.plot(epochs, mean, label='Training Loss', color=training_color, linewidth=2)\n",
    "            ax_loss.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=training_color)\n",
    "            plotted = True\n",
    "\n",
    "        if task_data['val_loss']:\n",
    "            # Plot individual folds\n",
    "            for data in task_data['val_loss']:\n",
    "                ax_loss.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "            # Compute mean and std\n",
    "            data_array = np.array(task_data['val_loss'])\n",
    "            mean = np.mean(data_array, axis=0)\n",
    "            std = np.std(data_array, axis=0)\n",
    "            ax_loss.plot(epochs, mean, label='Validation Loss', color=validation_color, linewidth=2)\n",
    "            ax_loss.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=validation_color)\n",
    "            plotted = True\n",
    "\n",
    "        if plotted:\n",
    "            ax_loss.set_title(f'{friendly_task_name} - Training and Validation Loss')\n",
    "            ax_loss.set_xlabel('Epochs')\n",
    "            ax_loss.set_ylabel('Loss')\n",
    "            ax_loss.legend(loc='upper right', fontsize='small')\n",
    "        else:\n",
    "            ax_loss.axis('off')  # Hide the axis if no data\n",
    "\n",
    "        # Plot metrics\n",
    "        ax_metric = axes[i, 1]\n",
    "        plotted = False\n",
    "        # Combine training and validation metrics to ensure consistent legends\n",
    "        all_metrics = set(task_data['metrics'].keys()).union(task_data['val_metrics'].keys())\n",
    "        for metric_name in all_metrics:\n",
    "            # Plot training metrics\n",
    "            if metric_name in task_data['metrics']:\n",
    "                if task_data['metrics'][metric_name]:\n",
    "                    # Plot individual folds\n",
    "                    for data in task_data['metrics'][metric_name]:\n",
    "                        ax_metric.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "                    # Compute mean and std\n",
    "                    data_array = np.array(task_data['metrics'][metric_name])\n",
    "                    mean = np.mean(data_array, axis=0)\n",
    "                    std = np.std(data_array, axis=0)\n",
    "                    ax_metric.plot(epochs, mean, label=f'Training {metric_name}', color=training_color, linewidth=2)\n",
    "                    ax_metric.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=training_color)\n",
    "                    plotted = True\n",
    "\n",
    "            # Plot validation metrics\n",
    "            if metric_name in task_data['val_metrics']:\n",
    "                if task_data['val_metrics'][metric_name]:\n",
    "                    # Plot individual folds\n",
    "                    for data in task_data['val_metrics'][metric_name]:\n",
    "                        ax_metric.plot(epochs, data, color=fold_color, alpha=0.3)\n",
    "                    # Compute mean and std\n",
    "                    data_array = np.array(task_data['val_metrics'][metric_name])\n",
    "                    mean = np.mean(data_array, axis=0)\n",
    "                    std = np.std(data_array, axis=0)\n",
    "                    ax_metric.plot(epochs, mean, label=f'Validation {metric_name}', color=validation_color, linewidth=2)\n",
    "                    ax_metric.fill_between(epochs, mean - std, mean + std, alpha=0.2, color=validation_color)\n",
    "                    plotted = True\n",
    "\n",
    "        if plotted:\n",
    "            ax_metric.set_title(f'{friendly_task_name} - Training and Validation Metrics')\n",
    "            ax_metric.set_xlabel('Epochs')\n",
    "            ax_metric.set_ylabel('Metric')\n",
    "            ax_metric.legend(loc='lower right', fontsize='small')\n",
    "        else:\n",
    "            ax_metric.axis('off')  # Hide the axis if no data\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_holdout_history(dataset):\n",
    "    \"\"\"\n",
    "    Plots the training and validation metrics for the holdout approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: Path object representing the dataset directory containing the history CSV file.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Define known metrics\n",
    "    known_metrics = ['loss', 'mean_absolute_error', 'sparse_categorical_accuracy']\n",
    "\n",
    "    # Read the history CSV file\n",
    "    history_path = dataset / \"history_1.csv\"\n",
    "    history = pd.read_csv(history_path)\n",
    "    \n",
    "    # Assuming the history has the same structure as in cross-validation\n",
    "    columns = history.columns\n",
    "\n",
    "    # Initialize data structures\n",
    "    metrics_data = {}  # {task_name: {'metrics': {metric_name: data}, 'loss': data, 'val_metrics': {}, 'val_loss': data}}\n",
    "\n",
    "    # For each column, parse and collect data\n",
    "    for col in columns:\n",
    "        is_val = col.startswith('val_')\n",
    "        col_base = col[4:] if is_val else col\n",
    "\n",
    "        if col_base in ['loss', 'lr']:\n",
    "            # Handle overall loss and learning rate separately\n",
    "            if col_base == 'loss':\n",
    "                task_name = 'overall'\n",
    "                metric_name = 'loss'\n",
    "                if task_name not in metrics_data:\n",
    "                    metrics_data[task_name] = {'metrics': {}, 'loss': None, 'val_metrics': {}, 'val_loss': None}\n",
    "                # Collect data\n",
    "                data = history[col].values\n",
    "                if is_val:\n",
    "                    metrics_data[task_name]['val_loss'] = data\n",
    "                else:\n",
    "                    metrics_data[task_name]['loss'] = data\n",
    "            # Skip 'lr' for now\n",
    "            continue\n",
    "        else:\n",
    "            # Extract task and metric\n",
    "            for metric in known_metrics:\n",
    "                if col_base.endswith(metric):\n",
    "                    task_name = col_base[:-len(metric)].rstrip('_')\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Could not parse column '{col}'\")\n",
    "                continue\n",
    "\n",
    "            # Remove 'output_' prefix from task name\n",
    "            if task_name.startswith('output_'):\n",
    "                task_name = task_name[len('output_'):]\n",
    "            metric_name = metric  # The metric found in the loop\n",
    "\n",
    "            # Initialize data structures if needed\n",
    "            if task_name not in metrics_data:\n",
    "                metrics_data[task_name] = {'metrics': {}, 'loss': None, 'val_metrics': {}, 'val_loss': None}\n",
    "\n",
    "            # Collect data\n",
    "            data = history[col].values\n",
    "            if is_val:\n",
    "                if metric_name == 'loss':\n",
    "                    metrics_data[task_name]['val_loss'] = data\n",
    "                else:\n",
    "                    metrics_data[task_name]['val_metrics'][metric_name] = data\n",
    "            else:\n",
    "                if metric_name == 'loss':\n",
    "                    metrics_data[task_name]['loss'] = data\n",
    "                else:\n",
    "                    metrics_data[task_name]['metrics'][metric_name] = data\n",
    "\n",
    "    # Now plot per task\n",
    "    tasks = list(metrics_data.keys())\n",
    "    num_tasks = len(tasks)\n",
    "\n",
    "    epochs = np.arange(len(history))\n",
    "\n",
    "    # Define colors\n",
    "    training_color = 'blue'\n",
    "    validation_color = 'orange'\n",
    "\n",
    "    # Map internal task names to friendly names\n",
    "    task_name_mapping = {\n",
    "        'concept_name_next': 'Next Activity',\n",
    "        'time_timestamp_next': 'Next Time',\n",
    "        'time_timestamp_last': 'Remaining Time',\n",
    "        'overall': 'Overall'\n",
    "    }\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(num_tasks, 2, figsize=(12, 5 * num_tasks))\n",
    "    if num_tasks == 1:\n",
    "        axes = np.array([axes])  # Ensure axes is always 2D array\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        task_data = metrics_data[task]\n",
    "        friendly_task_name = task_name_mapping.get(task, task)\n",
    "\n",
    "        # Plot loss\n",
    "        ax_loss = axes[i, 0]\n",
    "        plotted = False  # Flag to check if any data is plotted\n",
    "        if task_data['loss'] is not None:\n",
    "            ax_loss.plot(epochs, task_data['loss'], label='Training Loss', color=training_color, linewidth=2)\n",
    "            plotted = True\n",
    "\n",
    "        if task_data['val_loss'] is not None:\n",
    "            ax_loss.plot(epochs, task_data['val_loss'], label='Validation Loss', color=validation_color, linewidth=2)\n",
    "            plotted = True\n",
    "\n",
    "        if plotted:\n",
    "            ax_loss.set_title(f'{friendly_task_name} - Training and Validation Loss')\n",
    "            ax_loss.set_xlabel('Epochs')\n",
    "            ax_loss.set_ylabel('Loss')\n",
    "            ax_loss.legend(loc='upper right', fontsize='small')\n",
    "        else:\n",
    "            ax_loss.axis('off')  # Hide the axis if no data\n",
    "\n",
    "        # Plot metrics\n",
    "        ax_metric = axes[i, 1]\n",
    "        plotted = False\n",
    "        # Combine training and validation metrics to ensure consistent legends\n",
    "        all_metrics = set(task_data['metrics'].keys()).union(task_data['val_metrics'].keys())\n",
    "        for metric_name in all_metrics:\n",
    "            # Plot training metrics\n",
    "            if metric_name in task_data['metrics']:\n",
    "                if task_data['metrics'][metric_name] is not None:\n",
    "                    ax_metric.plot(epochs, task_data['metrics'][metric_name], label=f'Training {metric_name}', color=training_color, linewidth=2)\n",
    "                    plotted = True\n",
    "\n",
    "            # Plot validation metrics\n",
    "            if metric_name in task_data['val_metrics']:\n",
    "                if task_data['val_metrics'][metric_name] is not None:\n",
    "                    ax_metric.plot(epochs, task_data['val_metrics'][metric_name], label=f'Validation {metric_name}', color=validation_color, linewidth=2)\n",
    "                    plotted = True\n",
    "\n",
    "        if plotted:\n",
    "            ax_metric.set_title(f'{friendly_task_name} - Training and Validation Metrics')\n",
    "            ax_metric.set_xlabel('Epochs')\n",
    "            ax_metric.set_ylabel('Metric')\n",
    "            ax_metric.legend(loc='lower right', fontsize='small')\n",
    "        else:\n",
    "            ax_metric.axis('off')  # Hide the axis if no data\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_results_over_folds(dataset):\n",
    "    \"\"\"\n",
    "    Averages the results over all folds and stores the result in a CSV file for each task.\n",
    "    Additionally, prints the weighted mean accuracy for classification tasks and \n",
    "    the weighted mean MAE for regression tasks.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: Path object representing the dataset directory containing the results CSV files.\n",
    "    \"\"\"\n",
    "    # Get all result files in the dataset directory\n",
    "    result_files = list(dataset.glob(\"results_*__*.csv\"))\n",
    "\n",
    "    # Dictionary to store dataframes for each task\n",
    "    task_data = {}\n",
    "\n",
    "    # Process each result file\n",
    "    for file_path in result_files:\n",
    "        # Extract task name from the filename\n",
    "        base_name = file_path.name\n",
    "        parts = base_name.split(\"__\")\n",
    "        if len(parts) < 2:\n",
    "            continue  # Skip files that don't match the pattern\n",
    "        task_name_with_ext = parts[1]  # This includes the .csv extension\n",
    "        task_name = task_name_with_ext.replace(\".csv\", \"\")\n",
    "        # Remove parentheses and quotes from task name\n",
    "        task_name = task_name.strip(\"()'\\\"\").replace(\"', '\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, index_col=False)\n",
    "\n",
    "        # Print the columns in df for debugging\n",
    "        # print(f\"Processing file: {file_path}\")\n",
    "        # print(\"Columns in df:\", df.columns.tolist())\n",
    "        # print(df.head())\n",
    "\n",
    "        # Handle 'Weighted Mean' row separately\n",
    "        weighted_mean_row = df[df['k'] == 'Weighted Mean']\n",
    "        df = df[df['k'] != 'Weighted Mean']\n",
    "\n",
    "        # Convert 'k' to numeric, handling errors\n",
    "        df['k'] = pd.to_numeric(df['k'], errors='coerce')\n",
    "        df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "\n",
    "        # Identify metric columns dynamically\n",
    "        metric_columns = [col for col in df.columns if col not in ['k', 'weight', 'fold']]\n",
    "\n",
    "        # Convert metric columns to numeric\n",
    "        df[metric_columns] = df[metric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Drop rows with NaN values in 'k' or 'weight'\n",
    "        df = df.dropna(subset=['k', 'weight'])\n",
    "\n",
    "        # Add a column for fold number\n",
    "        fold_part = parts[0]\n",
    "        fold_number = fold_part.split('_')[1]\n",
    "        df['fold'] = int(fold_number)\n",
    "\n",
    "        # Store the dataframe in the task_data dictionary\n",
    "        if task_name not in task_data:\n",
    "            task_data[task_name] = {'dfs': [], 'metric_columns': metric_columns}\n",
    "        else:\n",
    "            # Ensure metric_columns are consistent across folds\n",
    "            if set(metric_columns) != set(task_data[task_name]['metric_columns']):\n",
    "                print(f\"Warning: Metric columns differ for task {task_name} in file {file_path}\")\n",
    "        task_data[task_name]['dfs'].append(df)\n",
    "\n",
    "    # Process each task\n",
    "    for task_name, data in task_data.items():\n",
    "        dfs = data['dfs']\n",
    "        metric_columns = data['metric_columns']\n",
    "\n",
    "        # Concatenate all folds data for this task\n",
    "        df_all_folds = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Group by 'k' and compute the mean of metrics over folds\n",
    "        agg_dict = {'weight': 'first'}\n",
    "        for metric in metric_columns:\n",
    "            agg_dict[metric] = 'mean'\n",
    "\n",
    "        df_mean = df_all_folds.groupby('k').agg(agg_dict).reset_index()\n",
    "\n",
    "        # Compute the overall weighted mean for each metric\n",
    "        weighted_metrics = {}\n",
    "        for metric in metric_columns:\n",
    "            valid = df_mean[metric].notna()\n",
    "            if valid.any():\n",
    "                total_weight_metric = df_mean.loc[valid, 'weight'].sum()\n",
    "                weighted_metric = (df_mean.loc[valid, metric] * df_mean.loc[valid, 'weight']).sum() / total_weight_metric\n",
    "                weighted_metrics[metric] = weighted_metric\n",
    "            else:\n",
    "                weighted_metrics[metric] = float('nan')  # Assign NaN if all values are NaN\n",
    "\n",
    "        # Create the 'Weighted Mean' row\n",
    "        weighted_mean_row = {'k': 'Weighted Mean', 'weight': ''}\n",
    "        for metric in metric_columns:\n",
    "            weighted_mean_row[metric] = weighted_metrics[metric]\n",
    "\n",
    "        # Append the 'Weighted Mean' row to the averaged dataframe using pd.concat\n",
    "        df_mean = pd.concat([df_mean, pd.DataFrame([weighted_mean_row])], ignore_index=True)\n",
    "\n",
    "        # Reorder columns to match the original file\n",
    "        columns_order = ['k', 'weight'] + metric_columns\n",
    "        df_mean = df_mean[columns_order]\n",
    "\n",
    "        # Save the averaged results to a new CSV file in the dataset directory\n",
    "        output_file = dataset / f\"averaged_results_{task_name}.csv\"\n",
    "        df_mean.to_csv(output_file, index=False)\n",
    "\n",
    "        # Print the weighted mean accuracy or MAE\n",
    "        if 'accuracy' in metric_columns:\n",
    "            weighted_accuracy = weighted_metrics['accuracy']\n",
    "            print(f\"Accuracy '{task_name}': {weighted_accuracy:.4f}\")\n",
    "        elif 'mae' in metric_columns:\n",
    "            weighted_mae = weighted_metrics['mae']\n",
    "            print(f\"MAE '{task_name}': {weighted_mae:.4f}\")\n",
    "\n",
    "        # print(f\"Averaged results for task '{task_name}' saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path.cwd()  # Get the current directory\n",
    "\n",
    "for job_idx, job in enumerate(current_dir.iterdir()):\n",
    "    if job.is_dir() and job_idx != 0:\n",
    "        if job == current_dir / \"15 Activity__Resource__timestamp----Activity_next__timestamp_next__timestamp_last\":  # TODO: for testing\n",
    "            print(f\"########################## {job.name} ##########################\")\n",
    "            # cross_val and holdout\n",
    "            for app_idx, approach in enumerate(job.iterdir()):\n",
    "                # cross_val\n",
    "                if app_idx == 0:\n",
    "                    # loop over datasets in approach\n",
    "                    for dataset in approach.iterdir():\n",
    "                        # print(f\"----- {dataset.name} -----\")\n",
    "                        # Plot histories\n",
    "                        # plot_crossval_histories(dataset)\n",
    "                        # Average results over folds\n",
    "                        # average_results_over_folds(dataset)\n",
    "                        break\n",
    "                        \n",
    "                # holdout\n",
    "                elif app_idx == 1:\n",
    "                    # TODO\n",
    "                    # Plot history for holdout\n",
    "                    plot_holdout_history(dataset)\n",
    "                # if there are more dirs\n",
    "                else:\n",
    "                    raise ValueError(f\"Too many approach folders in {job}. Expected 2.\")\n",
    "\n",
    "\n",
    "# for job_dir in job_dirs:\n",
    "#     for item in job_dir.iterdir():\n",
    "#         print(item)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
