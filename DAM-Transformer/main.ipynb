{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get CUDA device information\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "device = cuda.Device(0)\n",
    "print(\"Device Name:\", device.name())\n",
    "print(\"Total Memory:\", device.total_memory() / (1024 ** 2), \"MB\")\n",
    "print(\"Compute Capability:\", device.compute_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs\n",
    "# pip install numpy pandas tqdm matplotlib scikit-learn tensorflow pm4py\n",
    "# !pip install numpy pandas tqdm matplotlib scikit-learn tensorflow pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from typing import List, Optional\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import pm4py\n",
    "from package import transformer\n",
    "from package.loader import LogsDataLoader\n",
    "from package.processor import LogsDataProcessor, masked_standard_scaler, masked_min_max_scaler\n",
    "from package.constants import Feature_Type, Target, Temporal_Feature, Model_Architecture\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize data dir, if not exists\n",
    "if not os.path.exists(\"datasets\"): \n",
    "    os.mkdir(\"datasets\")\n",
    "\n",
    "# Set global random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    \n",
    "    def __init__(self, job_id: str, dataset_name: str, filepath: str, sorting: bool, columns: List[str],\n",
    "                 additional_columns: Optional[Dict[Feature_Type, List[str]]],\n",
    "                 datetime_format: str, model_epochs: int, warmup_epochs: int, model_num_layers: int,\n",
    "                 input_columns: List[str], target_columns: Dict[str, Target], temporal_features: Dict[Temporal_Feature, bool],\n",
    "                 cross_val: bool, model_architecture: Model_Architecture):\n",
    "        self.job_id: str = job_id\n",
    "        self.dataset_name: str = dataset_name\n",
    "        self.filepath: str = filepath\n",
    "        self.sorting: bool = sorting\n",
    "        self.columns: List[str] = columns\n",
    "        self.additional_columns: Optional[Dict[Feature_Type, List[str]]] = additional_columns\n",
    "        self.datetime_format: str = datetime_format\n",
    "        self.model_epochs: int = model_epochs\n",
    "        self.warmup_epochs: int = warmup_epochs\n",
    "        self.model_num_layers: int = model_num_layers\n",
    "        \n",
    "        self.target_columns: Dict[tuple, Target] = target_columns\n",
    "        for target_col, suffix in target_columns.keys():\n",
    "            if target_col == columns[1]:\n",
    "                self.target_columns[(\"concept_name\", suffix)] = self.target_columns.pop((target_col, suffix))\n",
    "                break\n",
    "                \n",
    "        self.input_columns: List[str] = input_columns\n",
    "        for idx, input_col in enumerate(input_columns):\n",
    "            if input_col == columns[1]:\n",
    "                self.input_columns[idx] = \"concept_name\"\n",
    "                break\n",
    "        self.temporal_features: Dict[Temporal_Feature, bool] = temporal_features\n",
    "        self.cross_val = cross_val\n",
    "        self.model_architecture = model_architecture\n",
    "        self.start_timestamp = None\n",
    "        self.end_timestamp = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"dataset_name: '{self.dataset_name}'\\n\"\n",
    "            f\"filepath: '{self.filepath}'\\n\"\n",
    "            f\"columns: '{self.columns}'\\n\"\n",
    "            f\"additional_columns: '{self.additional_columns}'\\n\"\n",
    "            f\"datetime_format: '{self.datetime_format}'\\n\"\n",
    "            f\"Model Epochs: '{self.model_epochs}'\\n\"\n",
    "            f\"Number of Transformer Layers in Model: '{self.model_num_layers}'\\n\"\n",
    "            f\"Target columns: '{self.target_columns}'\\n\"\n",
    "            f\"Input columns: '{self.input_columns}'\\n\")\n",
    "        \n",
    "    \n",
    "    def save_as_csv(self):\n",
    "        dir_path = os.path.join( \"datasets\", self.dataset_name )\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join( dir_path, self.filepath )\n",
    "        \n",
    "        \n",
    "        if file_path.endswith('.xes'):\n",
    "            print(\"Converting xes to csv file\")\n",
    "            df = pm4py.convert_to_dataframe(pm4py.read_xes(file_path)).astype(str)\n",
    "            df.to_csv(file_path.replace(\".xes\", \".csv\"), index=False)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Input file already has csv format\")\n",
    "            \n",
    "    \n",
    "    # preprocess the event log and save the train-test split as csv files\n",
    "    def preprocess_log(self) -> List[int]:\n",
    "        data_processor = LogsDataProcessor(\n",
    "            name=self.dataset_name,\n",
    "            filepath=self.filepath,\n",
    "            sorting=self.sorting,\n",
    "            columns=self.columns,\n",
    "            additional_columns=self.additional_columns,\n",
    "            input_columns=self.input_columns,\n",
    "            target_columns=self.target_columns,\n",
    "            datetime_format=self.datetime_format,\n",
    "            temporal_features=self.temporal_features,\n",
    "            pool=4\n",
    "        )\n",
    "        \n",
    "        self.target_columns = {(data_processor.sanitize_filename(feature, self.columns), suffix): target for (feature, suffix), target in self.target_columns.items()}\n",
    "        self.input_columns = [data_processor.sanitize_filename(col, self.columns) for col in self.input_columns]\n",
    "        self.columns = [data_processor.sanitize_filename(col, self.columns) for col in self.columns]\n",
    "        \n",
    "        # Preprocess the event log and make train-test split\n",
    "        data_processor.process_logs()\n",
    "        # flatten self.additional_columns to get all used features\n",
    "        self.additional_columns = data_processor.additional_columns\n",
    "        self.used_features = [item for sublist in self.additional_columns.values() for item in sublist]\n",
    "    \n",
    "    \n",
    "    # load the preprocessed train-test split from the csv files\n",
    "    def load_data(self) -> Tuple [ LogsDataLoader, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, Dict[str, int]], Dict[Feature_Type, List[str]] ]:\n",
    "        data_loader = LogsDataLoader(name=self.dataset_name, sorting=self.sorting, input_columns=self.input_columns,\n",
    "                                     target_columns=self.target_columns, temporal_features=self.temporal_features)\n",
    "        train_dfs, test_dfs, word_dicts, feature_type_dict, mask = data_loader.load_data()\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "        return data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask\n",
    "    \n",
    "    \n",
    "    def prepare_data( self, data_loader, dfs: Dict[str, pd.DataFrame], x_scaler=None, y_scaler=None,\n",
    "                     train: bool = True) -> Tuple[ Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], Dict[str, NDArray[np.float32]], int ]:\n",
    "        print(\"Preparing data...\")\n",
    "        # initialize max_case_length\n",
    "        max_case_length = False\n",
    "        # initialize token dicts\n",
    "        x_token_dict, y_token_dict, x_token_dict_numerical, y_token_dict_numerical = {}, {}, {}, {}\n",
    "        \n",
    "        # initialize case_id_df\n",
    "        case_ids = next(iter(dfs.values()))[\"case_id\"]\n",
    "        \n",
    "        # loop over all feature dfs\n",
    "        for idx, (feature, feature_df) in enumerate(dfs.items()):\n",
    "\n",
    "            feature_type = None\n",
    "            # get current feature_type\n",
    "            for _feature_type, feature_lst in self.additional_columns.items():\n",
    "                if feature in feature_lst:\n",
    "                    feature_type = _feature_type\n",
    "                    break\n",
    "            \n",
    "            if idx == 0 and train:\n",
    "                (x_tokens, y_tokens, max_case_length\n",
    "                ) = data_loader.prepare_data(feature=feature, df=feature_df, max_case_length=True)\n",
    "            else:\n",
    "                x_tokens, y_tokens = data_loader.prepare_data(feature=feature, df=feature_df)\n",
    "            \n",
    "            if feature_type is Feature_Type.TIMESTAMP or feature_type is Feature_Type.NUMERICAL:\n",
    "                x_token_dict_numerical.update(x_tokens)\n",
    "                y_token_dict_numerical.update(y_tokens)\n",
    "            else:\n",
    "                # update x_token_dict\n",
    "                x_token_dict.update(x_tokens)\n",
    "                y_token_dict.update(y_tokens)\n",
    "        if len(x_token_dict_numerical) > 0  and len(list(x_token_dict_numerical.values())[0]) > 0:\n",
    "            # Concatenate all the feature arrays along the rows (axis=0)\n",
    "            combined_data = np.vstack(list(x_token_dict_numerical.values()))\n",
    "            if x_scaler is None:\n",
    "                # Initialize  x_Scaler\n",
    "                x_scaler = FunctionTransformer(masked_min_max_scaler, kw_args={'padding_value': -1})\n",
    "                # Fit the scaler on the combined data\n",
    "                x_scaler.fit(combined_data)\n",
    "            # Transform the combined data\n",
    "            scaled_combined_data = x_scaler.transform(combined_data)\n",
    "            # split the scaled combined data back into the original feature dict\n",
    "            split_indices = np.cumsum([value.shape[0] for value in x_token_dict_numerical.values()])[:-1]\n",
    "            scaled_data_parts = np.vsplit(scaled_combined_data, split_indices)\n",
    "            # Reconstruct the dictionary with scaled data\n",
    "            scaled_dict = {key: scaled_data_parts[i] for i, key in enumerate(x_token_dict_numerical.keys())}\n",
    "            # update x_token_dict\n",
    "            x_token_dict.update(scaled_dict)\n",
    "        if len(y_token_dict_numerical) > 0:\n",
    "            # Prepare list to store valid arrays (non-empty)\n",
    "            valid_arrays = []\n",
    "            valid_keys = []\n",
    "\n",
    "            # Check for empty arrays and prepare data for scaling\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size > 0:  # Only consider non-empty arrays\n",
    "                    valid_arrays.append(value.reshape(-1, 1))  # Reshape to 2D\n",
    "                    valid_keys.append(key)\n",
    "\n",
    "            # If there are valid arrays to scale\n",
    "            if valid_arrays:\n",
    "                combined_data = np.hstack(valid_arrays)  # Horizontal stacking for features\n",
    "\n",
    "                if y_scaler is None:\n",
    "                    # Initialize y_Scaler\n",
    "                    # y_scaler = StandardScaler()\n",
    "                    y_scaler = MinMaxScaler(feature_range=(0, 30))\n",
    "                    # Fit the scaler on the combined data\n",
    "                    y_scaler.fit(combined_data)\n",
    "\n",
    "                # Transform the combined data\n",
    "                scaled_combined_data = y_scaler.transform(combined_data)\n",
    "\n",
    "                # Split the scaled combined data back into individual features\n",
    "                scaled_data_parts = np.hsplit(scaled_combined_data, scaled_combined_data.shape[1])\n",
    "\n",
    "                # Reconstruct the dictionary with scaled data\n",
    "                scaled_dict = {key: scaled_data_parts[i].flatten() for i, key in enumerate(valid_keys)}\n",
    "\n",
    "                # Update y_token_dict with the scaled data\n",
    "                y_token_dict.update(scaled_dict)\n",
    "\n",
    "            # Handle any empty arrays (if necessary)\n",
    "            for key, value in y_token_dict_numerical.items():\n",
    "                if value.size == 0:\n",
    "                    y_token_dict[key] = value\n",
    "            \n",
    "            \n",
    "        # sort dicts\n",
    "        x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "        y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "        return case_ids, x_token_dict, y_token_dict, x_scaler, y_scaler, max_case_length\n",
    "    \n",
    "    \n",
    "    # Prepare data and train the model\n",
    "    def train(self,\n",
    "            case_ids: pd.DataFrame,\n",
    "            feature_type_dict: Dict[Feature_Type, List[str]],\n",
    "            train_token_dict_x: Dict[str, NDArray[np.float32]],\n",
    "            train_token_dict_y: Dict[str, NDArray[np.float32]],\n",
    "            word_dicts: Dict[str, Dict[str, int]],\n",
    "            max_case_length: int,\n",
    "            y_scaler,\n",
    "            mask # Fraction of the training data to be used for validation\n",
    "            ) -> tf.keras.Model:\n",
    "\n",
    "        # Ensure that input columns and dictionaries are sorted\n",
    "        self.input_columns.sort()\n",
    "        self.target_columns = dict(sorted(self.target_columns.items()))\n",
    "        train_token_dict_x = dict(sorted(train_token_dict_x.items()))\n",
    "        train_token_dict_y = dict(sorted(train_token_dict_y.items()))\n",
    "        word_dicts = dict(sorted(word_dicts.items()))\n",
    "\n",
    "        # initialize model_wrapper with data for model\n",
    "        model_wrapper = transformer.ModelWrapper(\n",
    "                                                job_id = self.job_id,\n",
    "                                                dataset_name = self.dataset_name,\n",
    "                                                case_ids = case_ids,\n",
    "                                                input_columns=self.input_columns,\n",
    "                                                target_columns=self.target_columns,\n",
    "                                                additional_columns=self.additional_columns,\n",
    "                                                word_dicts=word_dicts,\n",
    "                                                max_case_length=max_case_length,\n",
    "                                                feature_type_dict=feature_type_dict,\n",
    "                                                temporal_features=self.temporal_features,\n",
    "                                                model_architecture=self.model_architecture,\n",
    "                                                sorting=self.sorting,\n",
    "                                                masking=True\n",
    "                                                )\n",
    "\n",
    "        # train the model\n",
    "        models, histories = model_wrapper.train_model(\n",
    "                                                    train_token_dict_x = train_token_dict_x,\n",
    "                                                    train_token_dict_y = train_token_dict_y,\n",
    "                                                    cross_val = self.cross_val,\n",
    "                                                    y_scaler = y_scaler,\n",
    "                                                    model_epochs = self.model_epochs,\n",
    "                                                    batch_size = 12,\n",
    "                                                    warmup_epochs = self.warmup_epochs,\n",
    "                                                    initial_lr = 1e-5,\n",
    "                                                    target_lr = 1e-3\n",
    "                                                    )\n",
    "        # Plot training loss\n",
    "        self._plot_training_loss(histories)\n",
    "        return models, histories\n",
    "            \n",
    "            \n",
    "    # helper function for plotting the training loss\n",
    "    def _plot_training_loss(self, histories):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # If there are multiple histories (cross-validation), plot for each fold\n",
    "        if isinstance(histories, list):\n",
    "            for i, history in enumerate(histories):\n",
    "                # Extract the loss and validation loss from the custom history structure\n",
    "                training_loss = [epoch['loss'] for epoch in history]\n",
    "                validation_loss = [epoch['val_loss'] for epoch in history if 'val_loss' in epoch]\n",
    "                \n",
    "                plt.plot(training_loss, label=f'Training Loss Fold {i+1}')\n",
    "                if validation_loss:\n",
    "                    plt.plot(validation_loss, label=f'Validation Loss Fold {i+1}')\n",
    "        else:\n",
    "            # Single history (no cross-validation)\n",
    "            training_loss = [epoch['loss'] for epoch in histories]\n",
    "            validation_loss = [epoch['val_loss'] for epoch in histories if 'val_loss' in epoch]\n",
    "            \n",
    "            plt.plot(training_loss, label='Training Loss')\n",
    "            if validation_loss:\n",
    "                plt.plot(validation_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    # inverse transform single feature with scaler fitted on multiple features\n",
    "    def _scaler_inverse_transform(self, scaler, feature_data):\n",
    "        # flatten feature_data\n",
    "        feature_data = feature_data.flatten()\n",
    "        # Get the number of features the scaler was fitted with\n",
    "        scaler_features = scaler.n_features_in_\n",
    "        if scaler_features == 1:\n",
    "            reshaped_data = feature_data.reshape(-1, 1)\n",
    "        else:\n",
    "            # Reshape the 1D array to (batch_size, scaler_features) with dummy values for other features\n",
    "            batch_size = feature_data.shape[0]\n",
    "            reshaped_data = np.zeros((batch_size, scaler_features))\n",
    "            # Fill the first column with feature values\n",
    "            reshaped_data[:, 0] = feature_data\n",
    "        # use inverse_transform on reshaped_data\n",
    "        inverse_transformed_data = scaler.inverse_transform(reshaped_data)\n",
    "        # Extract inverse-transformed (first) column\n",
    "        inverse_transformed_feature = inverse_transformed_data[:, 0]\n",
    "        return inverse_transformed_feature\n",
    "    \n",
    "\n",
    "    def evaluate(self, models, data_loader: LogsDataLoader, test_dfs: Dict[str, pd.DataFrame],\n",
    "                 max_case_length: int, x_scaler=None, y_scaler=None):\n",
    "        \n",
    "        results, preds = [], []\n",
    "        for idx, model in enumerate(models):\n",
    "            print(f\"Evaluating model {idx+1}...\")\n",
    "\n",
    "            # Prepare lists to store evaluation metrics\n",
    "            k, accuracies, fscores, precisions, recalls, weights = {}, {}, {}, {}, {}, {}\n",
    "            mae, mse, rmse, r2 = {}, {}, {}, {}\n",
    "            \n",
    "            for (target_col, suffix) in self.target_columns.keys():\n",
    "                target_key = (target_col, suffix)\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        k.update({target_key: []})\n",
    "                        weights.update({target_key: []})\n",
    "                        \n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            accuracies.update({target_key: []})\n",
    "                            fscores.update({target_key: []})\n",
    "                            precisions.update({target_key: []})\n",
    "                            recalls.update({target_key: []})\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            mae.update({target_key: []})\n",
    "                            mse.update({target_key: []})\n",
    "                            rmse.update({target_key: []})\n",
    "                            r2.update({target_key: []})\n",
    "\n",
    "            # Calculate total number of samples\n",
    "            total_samples = len(list(test_dfs.values())[0])\n",
    "\n",
    "            # Iterate over all prefixes (k)\n",
    "            for i in range(1, max_case_length + 1):\n",
    "                print(\"Prefix length: \" + str(i))\n",
    "                test_data_subsets = {}\n",
    "\n",
    "                for key, df in test_dfs.items():\n",
    "                    if (Feature_Type.TIMESTAMP in self.additional_columns\n",
    "                            and key in self.additional_columns[Feature_Type.TIMESTAMP]):\n",
    "                        prefix_str = f\"{key}##Prefix Length\"\n",
    "                    else:\n",
    "                        prefix_str = \"Prefix Length\"\n",
    "                    filtered_df = df[df[prefix_str] == i]\n",
    "                    test_data_subsets.update({key: filtered_df})\n",
    "\n",
    "\n",
    "                _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_data_subsets,\n",
    "                                                                x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "\n",
    "                # sort dicts\n",
    "                x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "                y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "\n",
    "                if len(test_data_subsets[self.input_columns[0]]) > 0:\n",
    "\n",
    "                    # Make predictions\n",
    "                    predictions = model.predict(x_token_dict)\n",
    "                    \n",
    "                    # Handle multiple outputs for multitask learning\n",
    "                    if len(self.target_columns) > 1:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "                    else:\n",
    "                        result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "\n",
    "                    # Compute metrics\n",
    "                    for (feature, suffix), result in result_dict.items():\n",
    "                        target_key = (feature, suffix)\n",
    "                        for feature_type, feature_lst in self.additional_columns.items():\n",
    "                            if feature in feature_lst:\n",
    "                                if feature_type is Feature_Type.CATEGORICAL:\n",
    "                                    result = np.argmax(result, axis=1)\n",
    "                                    accuracy = metrics.accuracy_score(y_token_dict[f\"output_{feature}_{suffix}\"], result)\n",
    "                                    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                                        y_token_dict[f\"output_{feature}_{suffix}\"], result, average=\"weighted\", zero_division=0)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[target_key].append(i)\n",
    "                                    accuracies[target_key].append(accuracy)\n",
    "                                    fscores[target_key].append(fscore)\n",
    "                                    precisions[target_key].append(precision)\n",
    "                                    recalls[target_key].append(recall)\n",
    "                                    weights[target_key].append(weight)\n",
    "                                \n",
    "                                elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                                    # inverse transform y_true and y_pred\n",
    "                                    y_true = self._scaler_inverse_transform( y_scaler, y_token_dict[f\"output_{feature}_{suffix}\"] )\n",
    "                                    y_pred = self._scaler_inverse_transform(y_scaler, result)\n",
    "                                    \n",
    "                                    mae_value = metrics.mean_absolute_error(y_true, y_pred)\n",
    "                                    mse_value = metrics.mean_squared_error(y_true, y_pred)\n",
    "                                    rmse_value = np.sqrt(mse_value)\n",
    "                                    r2_value = metrics.r2_score(y_true, y_pred)\n",
    "                                    weight = len(test_data_subsets[feature]) / total_samples\n",
    "\n",
    "                                    k[target_key].append(i)\n",
    "                                    mae[target_key].append(mae_value)\n",
    "                                    mse[target_key].append(mse_value)\n",
    "                                    rmse[target_key].append(rmse_value)\n",
    "                                    r2[target_key].append(r2_value)\n",
    "                                    weights[target_key].append(weight)\n",
    "            feature_results = []\n",
    "            for (target_col, suffix) in self.target_columns.keys():\n",
    "                target_key = (target_col, suffix)\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if target_col in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_accuracy = np.average(accuracies[target_key], weights=weights[target_key])\n",
    "                            weighted_fscore = np.average(fscores[target_key], weights=weights[target_key])\n",
    "                            weighted_precision = np.average(precisions[target_key], weights=weights[target_key])\n",
    "                            weighted_recall = np.average(recalls[target_key], weights=weights[target_key])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_key].append(\"\")\n",
    "                            k[target_key].append(\"Weighted Mean\")\n",
    "                            accuracies[target_key].append(weighted_accuracy)\n",
    "                            fscores[target_key].append(weighted_fscore)\n",
    "                            precisions[target_key].append(weighted_precision)\n",
    "                            recalls[target_key].append(weighted_recall)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_key}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_key],\n",
    "                                'weight': weights[target_key],\n",
    "                                'accuracy': accuracies[target_key],\n",
    "                                'fscore': fscores[target_key],\n",
    "                                'precision': precisions[target_key],\n",
    "                                'recall': recalls[target_key]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "                        \n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            # Compute weighted mean metrics over all k\n",
    "                            weighted_mae = np.average(mae[target_key], weights=weights[target_key])\n",
    "                            weighted_mse = np.average(mse[target_key], weights=weights[target_key])\n",
    "                            weighted_rmse = np.average(rmse[target_key], weights=weights[target_key])\n",
    "                            weighted_r2 = np.average(r2[target_key], weights=weights[target_key])\n",
    "                            # Append weighted mean metrics to the lists\n",
    "                            weights[target_key].append(\"\")\n",
    "                            k[target_key].append(\"Weighted Mean\")\n",
    "                            mae[target_key].append(weighted_mae)\n",
    "                            mse[target_key].append(weighted_mse)\n",
    "                            rmse[target_key].append(weighted_rmse)\n",
    "                            r2[target_key].append(weighted_r2)\n",
    "                            # Create a DataFrame to display the results\n",
    "                            print(f\"Results for {target_key}\")\n",
    "                            results_df = pd.DataFrame({\n",
    "                                'k': k[target_key],\n",
    "                                'weight': weights[target_key],\n",
    "                                'mae': mae[target_key],\n",
    "                                'mse': mse[target_key],\n",
    "                                'rmse': rmse[target_key],\n",
    "                                'r2': r2[target_key]\n",
    "                            })\n",
    "                            feature_results.append(results_df)\n",
    "                            # Display the results\n",
    "                            print(results_df)\n",
    "            results.append(feature_results)\n",
    "            print(\"_____________________________________________\")\n",
    "            \n",
    "          \n",
    "            \n",
    "            # calculate predictions for all test data\n",
    "            _, x_token_dict, y_token_dict, _, _, _ = self.prepare_data(data_loader=data_loader, dfs=test_dfs,\n",
    "                                                    x_scaler=x_scaler, y_scaler=y_scaler, train=False)\n",
    "            # sort dicts\n",
    "            x_token_dict = dict(sorted(x_token_dict.items()))\n",
    "            y_token_dict = dict(sorted(y_token_dict.items()))\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(x_token_dict)\n",
    "            \n",
    "            # Handle multiple outputs for multitask learning\n",
    "            if len(self.target_columns) > 1:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), predictions))\n",
    "            else:\n",
    "                result_dict = dict(zip(self.target_columns.keys(), [predictions]))\n",
    "                \n",
    "            feature_preds = []\n",
    "            for (feature, suffix), result in result_dict.items():\n",
    "                for feature_type, feature_lst in self.additional_columns.items():\n",
    "                    if feature in feature_lst:\n",
    "                        if feature_type is Feature_Type.CATEGORICAL:\n",
    "                            y_true = y_token_dict[f\"output_{feature}_{suffix}\"]\n",
    "                            y_pred = np.argmax(result, axis=1)\n",
    "                        elif feature_type is Feature_Type.TIMESTAMP:\n",
    "                            # inverse transform y_true and y_pred\n",
    "                            y_true = self._scaler_inverse_transform( y_scaler, y_token_dict[f\"output_{feature}_{suffix}\"] )\n",
    "                            y_pred = self._scaler_inverse_transform(y_scaler, result)\n",
    "                            \n",
    "                        preds_df = pd.DataFrame({\n",
    "                                        'y_true': y_true.reshape(-1),\n",
    "                                        'y_pred': y_pred.reshape(-1)\n",
    "                                    })\n",
    "                        feature_preds.append(preds_df)\n",
    "            preds.append(feature_preds)\n",
    "                    \n",
    "        \n",
    "        return results, preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            \n",
    "    def safe_results(self, y_scaler, histories: list, results: list, preds: list):\n",
    "        \n",
    "        elapsed_time = self.end_timestamp - self.start_timestamp\n",
    "        \n",
    "        # Directory for saving results\n",
    "        dir_path = os.path.join(\"datasets\", self.dataset_name, \"results\", self.job_id)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Save parameters in JSON\n",
    "        parameters = {\n",
    "            \"Input Columns\": self.input_columns,\n",
    "            \"Target Columns\": {f\"{col}_{suff}\": value.value for (col, suff), value in self.target_columns.items()},\n",
    "            \"Model Epochs\": self.model_epochs,\n",
    "            \"Transformer Layers\": self.model_num_layers,\n",
    "            \"Sorting\": self.sorting,\n",
    "            \"Cross Validation\": self.cross_val,\n",
    "            \"Elapsed Time\": elapsed_time\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(dir_path, \"parameters.json\"), \"w\") as metadata_file:\n",
    "            json.dump(parameters, metadata_file)\n",
    "        \n",
    "        # Save histories and results\n",
    "        for model_idx, (history, result, pred) in enumerate(zip(histories, results, preds)):\n",
    "            # Create DataFrame from custom history structure\n",
    "            history_df = pd.DataFrame(history)\n",
    "            \n",
    "            # Reverse transform MAE values if y_scaler is provided\n",
    "            for col in history_df.columns:\n",
    "                if \"mean_absolute_error\" in col:\n",
    "                    history_df[col] = self._scaler_inverse_transform(y_scaler, history_df[col].to_numpy())\n",
    "            \n",
    "            # Save history as CSV\n",
    "            history_path = os.path.join(dir_path, f\"history_{model_idx+1}.csv\")\n",
    "            history_df.to_csv(history_path, index=False)\n",
    "            \n",
    "            for output_idx, (output_result_df, output_pred_df) in enumerate(zip(result, pred)):\n",
    "                feature = list(self.target_columns.keys())[output_idx]\n",
    "                \n",
    "                # Save results DataFrame as CSV\n",
    "                results_path = os.path.join(dir_path, f\"results_{model_idx+1}__{feature}.csv\")\n",
    "                output_result_df.to_csv(results_path, index=False)\n",
    "                \n",
    "                # Save predictions DataFrame as CSV\n",
    "                predictions_path = os.path.join(dir_path, f\"predictions_{model_idx+1}__{feature}.csv\")\n",
    "                output_pred_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "        print(f\"Histories and results saved to {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "# helper function to save xes file as csv\n",
    "def save_csv(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    pipe.save_as_csv()\n",
    "    \n",
    "\n",
    "# helper function: do only preprocessing on data\n",
    "def preprocess(args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(**args)  # Examples: \"concept:name\", \"Resource\"\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "\n",
    "# helper function\n",
    "def run(job_id, args):\n",
    "    # initialize pipeline with parameters\n",
    "    pipe = pipeline(job_id, **args)  # Examples: \"concept:name\", \"Resource\"\n",
    "\n",
    "    pipe.start_timestamp = time.time()\n",
    "\n",
    "    # print parameters\n",
    "    print(pipe)\n",
    "\n",
    "    # preprocess data\n",
    "    pipe.preprocess_log()\n",
    "\n",
    "    # load data\n",
    "    data_loader, train_dfs, test_dfs, word_dicts, feature_type_dict, mask = pipe.load_data()\n",
    "\n",
    "    # prepare data\n",
    "    case_ids, train_token_dict_x, train_token_dict_y, x_scaler, y_scaler, max_case_length = pipe.prepare_data(data_loader, train_dfs)\n",
    "\n",
    "    case_ids = case_ids.astype(str)\n",
    "    # Check for NaN or None values using pd.Series.isna()\n",
    "    assert not case_ids.isna().any(), \"case_ids contains NaN or None values!\"\n",
    "\n",
    "    # train the model\n",
    "    models, histories = pipe.train(\n",
    "                case_ids = case_ids,\n",
    "                feature_type_dict = feature_type_dict,\n",
    "                train_token_dict_x = train_token_dict_x,\n",
    "                train_token_dict_y = train_token_dict_y,\n",
    "                word_dicts = word_dicts,\n",
    "                max_case_length = max_case_length,\n",
    "                y_scaler = y_scaler,\n",
    "                mask = mask\n",
    "                )\n",
    "\n",
    "    # evaluate the model\n",
    "    results, preds = pipe.evaluate(models=models, data_loader=data_loader, test_dfs=test_dfs, x_scaler=x_scaler,\n",
    "                                y_scaler=y_scaler, max_case_length=max_case_length)\n",
    "    \n",
    "    pipe.end_timestamp = time.time()\n",
    "    \n",
    "    # safe the training histories and results\n",
    "    pipe.safe_results(y_scaler=y_scaler, histories=histories, results=results, preds=preds)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"======================================\")\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args & Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross_Val Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"Activity\", \"next\"): Target.NEXT_FEATURE, (\"Complete Timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"Activity\", \"Resource\", \"Complete Timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:group\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:group\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2012 = {\n",
    "        \"dataset_name\": \"bpi_2012\",\n",
    "        \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "args_bpi_2020 = {\n",
    "        \"dataset_name\": \"bpi_2020\",\n",
    "        \"filepath\": \"InternationalDeclarations.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:role\"]},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:role\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": False,\n",
    "        \"cross_val\": True\n",
    "        }\n",
    "\n",
    "processing_queue = [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "for dataset in processing_queue:\n",
    "    dataset_name = dataset[\"dataset_name\"]\n",
    "    run(f\"999_cross_val_{dataset_name}\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "args_helpdesk = {\n",
    "        \"dataset_name\": \"helpdesk\",\n",
    "        \"filepath\": \"helpdesk.csv\",\n",
    "        \"columns\": [\"Case ID\", \"Activity\", \"Complete Timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"Resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"Activity\", \"next\"): Target.NEXT_FEATURE, (\"Complete Timestamp\", \"next\"): Target.NEXT_FEATURE, (\"Complete Timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"Activity\", \"Resource\", \"Complete Timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_sepsis = {\n",
    "        \"dataset_name\": \"sepsis\",\n",
    "        \"filepath\": \"sepsis.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:group\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:group\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2012 = {\n",
    "        \"dataset_name\": \"bpi_2012\",\n",
    "        \"filepath\": \"BPI_Challenge_2012.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2013 = {\n",
    "        \"dataset_name\": \"bpi_2013\",\n",
    "        \"filepath\": \"BPI_Challenge_2013_incidents.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2015_1 = {\n",
    "        \"dataset_name\": \"bpi_2015_1\",\n",
    "        \"filepath\": \"BPIC15_1.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:resource\"]},\n",
    "        \"datetime_format\": \"%Y-%m-%d %H:%M:%S%z\",\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:resource\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "args_bpi_2020 = {\n",
    "        \"dataset_name\": \"bpi_2020\",\n",
    "        \"filepath\": \"InternationalDeclarations.xes\",\n",
    "        \"columns\": [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],\n",
    "        \"additional_columns\": {Feature_Type.CATEGORICAL: [\"org:role\"]},\n",
    "        \"datetime_format\": None,\n",
    "        \"model_epochs\": 100,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"model_num_layers\": 1,\n",
    "        \"target_columns\": {(\"concept:name\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"next\"): Target.NEXT_FEATURE, (\"time:timestamp\", \"last\"): Target.LAST_FEATURE},\n",
    "        \"input_columns\": [\"concept:name\", \"org:role\", \"time:timestamp\"],\n",
    "        \"temporal_features\": {Temporal_Feature.DAY_OF_WEEK: False, Temporal_Feature.HOUR_OF_DAY: False},\n",
    "        \"model_architecture\": Model_Architecture.COMMON_POSEMBS_TRANSF,\n",
    "        \"sorting\": True,\n",
    "        \"cross_val\": False\n",
    "        }\n",
    "\n",
    "processing_queue = [args_helpdesk, args_sepsis, args_bpi_2012, args_bpi_2013, args_bpi_2015_1, args_bpi_2020]\n",
    "for dataset in processing_queue:\n",
    "    dataset_name = dataset[\"dataset_name\"]\n",
    "    run(f\"999_holdout_{dataset_name}\", dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
